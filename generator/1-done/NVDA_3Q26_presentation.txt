NVIDIA Corporation (NVDA) Q3 2026 Earnings Call November 19, 2025 5:00 PM EST

Company Participants

Toshiya Hari - Vice President of Investor Relations & Strategic Finance
Colette Kress - Executive VP & CFO
Jen-Hsun Huang - Co-Founder, CEO, President & Director

Conference Call Participants

Joseph Moore - Morgan Stanley, Research Division
Christopher Muse - Cantor Fitzgerald & Co., Research Division
Vivek Arya - BofA Securities, Research Division
Benjamin Reitzes - Melius Research LLC
James Schneider - Goldman Sachs Group, Inc., Research Division
Timothy Arcuri - UBS Investment Bank, Research Division
Stacy Rasgon - Sanford C. Bernstein & Co., LLC., Research Division
Aaron Rakers - Wells Fargo Securities, LLC, Research Division

Presentation

Operator

Good afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Third Quarter Earnings Call. [Operator Instructions].

Toshiya Hari, you may begin your conference.

Toshiya Hari
Vice President of Investor Relations & Strategic Finance

Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the third quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer.

I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the fourth quarter of fiscal 2026.

The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.

For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q. And the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, November 19, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP measures to GAAP financial measures in our CFO Commentary which is posted on our website. With that, let me turn the call over to Colette.

Colette Kress
Executive VP & CFO

Thank you, Toshiya. We delivered another outstanding quarter with revenue of $57 billion, up 62% and a record sequential revenue growth of $10 billion or 22%. Our customers continued to lean into 3 platform shifts, fueling exponential growth for accelerated computing, powerful AI models and agentic applications yet we are still in the early innings of these transitions that will impact our work across every industry.

We currently have visibility to $0.5 trillion in Blackwell and Rubin revenue from the start of this year through the end of calender year 2026. By executing our annual product cadence and extending our performance leadership through full stack design we believe NVIDIA will be the superior choice for the $3 trillion to $4 trillion in annual AI infrastructure build we estimate by the end of the decade.

Demand for AI infrastructure continues to exceed our expectations. The clouds are sold out and our GPU installed base, both new and previous generations, including Blackwell, Hopper and Ampere is fully utilized. Record Q3 data center revenue of $51 billion (sic) [ $51.2 billion ] increased 66% year-over-year, a significant feat at our scale. Compute grew 56% year-over-year, driven primarily by the GB300 ramp, while networking more than doubled, given the onset of NVLink scale up and robust double-digit growth across Spectrum-X Ethernet and Quantum-X InfiniBand.

The world hyperscalers, a trillion-dollar industry are transforming search recommendations and content understanding from classical machine learning to generative AI. NVIDIA CUDA excels at both and is the ideal platform for this transition. Driving infrastructure investment measured in hundreds of billions of dollars.

At Meta, AI recommendation systems are delivering higher quality and more relevant content, leading to more time spent on apps such as Facebook and Threads, any expectations for the top CSPs and hyperscalers in 2026, aggregate CapEx have continued to increase and now sit roughly at $600 billion, more than $200 billion higher relative to the start of the year.

We see the transition to accelerate computing in generative AI across current hyper workloads contributing toward roughly half of our long-term opportunity. Another growth pillar is the ongoing increase in compute spend driven by foundation model builders such as Anthropic, Mistral, OpenAI, Reflection, Safe Superintelligence, Thinking Machines Lab and xAI, all scaling, compute aggressively to scale intelligence.

The 3 scaling laws, pretraining post training and inference remain intact, in fact, we see a positive virtuous cycle emerging whereby the 3 scaling laws and access to compute are generating better intelligence and in turn, increasing adoption and profits.

OpenAI recently shared that their weekly user base has grown to 800 million. Enterprise customers has increased to 1 million and that their gross margins were healthy. Well, Anthropic recently reported that its annualized run rate revenue has reached $7 billion as of last month, up from $1 billion at the start of the year.

We are also witnessing a proliferation of agentic AI across various industries and tasks, companies such as Cursor, Anthropic, OpenEvidence, Epic and Abridge are experiencing a surge in user growth as they supercharge the existing workforce, delivering unquestionable ROI for coders and health care professionals.

The world's most important enterprise software platforms like ServiceNow, CrowdStrike, and SAP are integrating NVIDIA's accelerated computing and AI stack. Our new partner, Palantir, is supercharging the incredibly popular oncology platform with NVIDIA CUDA-X libraries and AI models for the first time.

Previously, like most enterprise software platforms, Anthology runs only on CPUs. Lowe's is leveraging the platform to build supply chain agility, reducing costs and improving customer satisfaction. Enterprises broadly are leveraging AI to boost productivity, increase efficiency and reduce cost. RBC is leveraging agent AI to drive significant analyst productivity slashing, report generation, time from hours to minutes. AI and digital twins are helping Unilever accelerate content creation by 2x and cut costs by 50%.

Salesforce's engineering team has seen at least 30% productivity increase in new co-development after adopting Cursor. This past quarter, we announced AI factory and infrastructure projects amounting to an aggregate of 5 million GPUs. This demand spans every market, CSPs, sovereigns, modern builders, enterprises and supercomputing centers and includes multiple landmark build-outs. xAI's Colossus 2, the world's first gigawatt scale data center, Lilly's AI Factory for drug discovery, the pharmaceutical industry's most powerful data center.

And just today, AWS and HUMAIN expanded their partnership, including the deployment of up to 150,000 AI accelerators, including our GB300, xAI and HUMAIN also announced a partnership in which the 2 will jointly develop a network of world-class GPU data centers anchored by the flagship 500-megawatt facility.

Blackwell gained further momentum in Q3, as GB300 crossed over GB200 and contributed roughly 2/3 of the total Blackwell revenue. The transition to GB300 has been seamless, with production shipments to the majority -- to the major cloud service providers, hyperscalers and GP clouds and is already driving their growth.

The Hopper platform in its 13th quarter since inception, recorded approximately $2 billion in revenue in Q3. H20 sales were approximately $50 million, sizable purchase orders never materialized in the quarter due to geopolitical issues and the increasingly competitive market in China. While we were disappointed in the current state that prevents us from shipping more competitive data center compute products to China, we are committed to continued engagement with the U.S. and China governments and will continue to advocate for America's ability to compete around the world.

To establish a sustainable leadership position in AI computing, America must win the support of every developer and be the platform of choice for every commercial business, including those in China. The Rubin platform is on track to ramp in the second half of 2026. Powered by 7 chips, the Vera Rubin platform will once again deliver an x-factor improvement in performance relative to Blackwell. We have received silicon back from our supply chain partners and are happy to report that NVIDIA teams across the world are executing to bring up beautifully.

Rubin is our third-generation rack-scale system substantially redefined the manufacturability while remaining compatible with Grace Blackwell. Our supply chain data center ecosystem and cloud partners have now mastered the build to installation process of NVIDIA's rack architecture. Our ecosystem will be ready for a fast Rubin ramp.

Our annual X factor performance leap increases performance per dollar while driving down computing costs for our customers. The long useful life of NVIDIA's CUDA GPUs is a significant TCO advantage over accelerators. CUDA's compatibility in our massive installed base, extend the life NVIDIA Systems well beyond their original estimated useful life. For more than 2 decades, we have optimized the CUDA ecosystem, improving existing workloads, accelerating new ones and increasing throughput with every software release.

Most accelerators without CUDA and NVIDIA's time-tested and versatile architecture became obsolete within a few years as model technologies evolve. Thanks to CUDA, the A100 GPUs we shipped 6 years ago are still running at full utilization today, powered by vastly improved software stack.

We have evolved over the past 25 years from a gaming GPU company to now an AI data center infrastructure company. Our ability to innovate across the CPU, the GPU, networking and software and ultimately drive down cost per token is unmatched across the industry. Our networking business purpose built for AI and now the largest in the world, generated revenue of $8.2 billion, up 162% year-over-year with NVLink, InfiniBand and Spectrum-X Ethernet, all contributing to growth.

We are winning in data center networking, as the majority of AI deployments now include our switches with Ethernet GPU attach rates roughly on par with InfiniBand. Meta, Microsoft, Oracle and xAI are building gigawatt AI factories with Spectrum-X Ethernet switches and each will run its operating system of choice, highlighting the flexibility and openness of our platform.

We recently introduced Spectrum-XGS, a scale across technology that enables gigascale AI factories And NVIDIA is the only company with AI scale up, scale out and scale across platforms, reinforcing our unique position in the market as the AI infrastructure provider.

Customer interest in NVLink Fusion continues to grow. We announced a strategic collaboration with Fujitsu in October, where we will integrate Fujitsu's CPUs and NVIDIA GPUs, via and NVLink Fusion, connecting our large ecosystems. We also announced a collaboration with Intel to develop multiple generations of custom data center and PC products, connecting NVIDIA and Intel's ecosystems using NVLink.

This week at Supercomputing '25, Arm announced that it will be integrating NVLink IP for customers to build CPU SoCs that connect with NVIDIA currently on its fifth generation. NVLink is the only proven scale up technology available on the market today. In the latest MLPerf training results, Blackwell Ultra delivered 5x faster time to train than Hopper, NVIDIA swept every benchmark. Notably, NVIDIA is the only training platform to leverage FP4 while meeting the MLPerf's strict accuracy standards. In semi-analysis, InferenceMAX benchmark, Blackwell achieved the highest performance and lowest total cost of ownership across every model and use case. Particularly important is Blackwell's NVLinks performance on a mixture of experts. The architecture for the world's most popular reasoning models.

On DeepSeek-R1 Blackwell delivered 10x higher performance per watt and 10x lower cost per token versus H200, a huge generational leap fueled by our extreme co-design approach. NVIDIA Dynamo an open source, low latency modular inference framework has now been adopted by every major cloud service provider, leveraging Dynamo's enablement, and disaggregated inference, the resulting increase in performance of complex AI models, such as MoE models, AWS, Google Cloud, Microsoft Azure and OCI have boosted AI inference performance for enterprise cloud customers.

We are working on a strategic partnership with OpenAI focused on helping them build and deploy at least 10 gigawatts of AI data centers. In addition, we have the opportunity to invest in the company. We serve OpenAI through their cloud partners, Microsoft Azure, OCI and CoreWeave. We will continue to do so for the foreseeable future. As they continue to scale, we are delighted to support the company to add self-build infrastructure and we are working towards a definitive agreement and are excited to support OpenAI's growth.

Yesterday, we celebrated an announcement with Anthropic. For the first time, Anthropic is adopting NVIDIA and we are establishing a deep technology partnership to support Anthropic's fast growth. We will collaborate to optimize Anthropic models for CUDA and deliver the best possible performance, efficiency and TCO. We will also optimize future NVIDIA architectures for Anthropic workloads. Anthropic's compute commitment is initially including up to 1 gigawatt of compute capacity with Grace Blackwell and Vera Rubin Systems.

Our strategic investments in Anthropic, Mistral, OpenAI, Reflection, Thinking Machines and other represent partnerships that grow the NVIDIA CUDA AI ecosystem and enable every model to run optimally on NVIDIAs everywhere. We will continue to invest strategically while preserving our disciplined approach to cash flow management. Physical AI is already a multibillion-dollar business addressing a multitrillion dollar opportunity on the next leg of growth for NVIDIA. Leading U.S. manufacturers and robotics innovators are leveraging NVIDIA's 3 computer architecture to train on NVIDIA, test on Omniverse's computer and deploy real-world AI and just in robotic computers. PTC and Siemens introduced new services that bring Omniverse powered digital twin workflows to their extensive installed base of customers. companies, including Belden, Caterpillar, Foxconn, Lucid Motors, Toyota, TSMC and Wistron, are building Omniverse digital twin factories to accelerate AI-driven manufacturing and automation.

Agility Robotics, Amazon Robotics, Figure and Skild at AI are building our platform, tapping offerings such as NVIDIA, Cosmos, World Foundation Models for development, Omniverse for simulation and validation and Jetson to power next-generation intelligent robots.

We remain focused on building resiliency and redundancy in our global supply chain. Last month, in partnership with TSMC, we celebrated the first Blackwell wafer produced on U.S. soil. We will continue to work with Foxconn, Wistron, Amkor, SPIL and others to grow our presence in the U.S. over the next 4 years. Gaming revenue was $4.3 billion, up 30% year-on-year, driven by strong demand as Blackwell momentum continued. End market sell-through remains robust and channel inventories are at normal levels heading into the holiday season. Steam recently broke its concurrent user record with 42 million gamers while thousands of fans pack the GeForce Gamer Festival in South Korea to celebrate 25 years of GeForce.

NVIDIA pro visualization has evolved into computers for engineers and developers, whether for graphics or for AI. Professional Visualization revenue was $760 million, up 56% year-over-year, was another record. Growth was driven by DGX Spark, the world's smallest AI supercomputer built on a small configuration of Grace Blackwell. Automotive revenue was $592 million, up 32% year-over-year, primarily driven by self-driving solutions. We are partnering with Uber to scale the world's largest Level 4 ready autonomous fleet built on the new NVIDIA Hyperion L4 robotaxi reference architecture.

Moving to the rest of the P&L. GAAP gross margins were 73.4% and non-GAAP gross margins was 73.6%, exceeding our outlook. Gross margins increased sequentially due to our data center mix, improved cycle time and cost structure. GAAP operating expenses were up 8% sequentially and up 11% on non-GAAP basis. The growth was driven by infrastructure compute as well as higher compensation and benefits and engineering development costs.

Non-GAAP effective tax rate for the third quarter was just over 17% higher than our guidance of 16.5% due to the strong U.S. revenue. On our balance sheet, inventory grew 32% quarter-over-quarter, while supply commitments increased 63% sequentially. The we are preparing for significant growth ahead and feel good about our ability to execute against our opportunity set.

Okay. Let me turn to the outlook for the fourth quarter. Total revenue is expected to be $65 billion, plus or minus 2%. At the midpoint, our outlook implies 14% sequential growth driven by continued momentum in the Blackwell architecture. Consistent with last quarter, we are not assuming any data center compute revenue from China. GAAP and non-GAAP gross margins are expected to be 74.8% and 75%, respectively, plus or minus 50 basis points.

Looking ahead to fiscal year 2027, and input costs are on the rise, but we are working to hold gross margins in the mid-70s. GAAP and non-GAAP operating expenses are expected to be approximately $6.7 billion and $5 billion, respectively. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $500 million, excluding gains and losses from nonmarketable and publicly held equity securities. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items.

At this time, let me turn the call over to Jensen for him to say a few words.

Jen-Hsun Huang
Co-Founder, CEO, President & Director

Thanks, Colette. There's been a lot of talk about an AI bubble. From our vantage point, we see something very different. As a reminder, NVIDIA is unlike any other accelerator. We excel at every phase of AI from pre-training and post training to inference. And with our 2-decade investment in CUDA-X acceleration libraries, we are also exceptional at science and engineering simulations, computer graphics, structured data processing to classical machine learning.

The world is going -- is undergoing 3 massive platform shifts at once. The first time since the dawn of Moore's Law, NVIDIA is uniquely addressing each of the 3 transformations. The first transition is from CPU general purpose computing to GPU accelerated computing and Moore's Law slows. The world has a massive investment in non-AI software. From data processing to science and engineering simulations, representing hundreds of billions of dollars in compute -- cloud computing spend each year.

Many of these applications which ran once exclusively on CPUs are now rapidly shifting to CUDA GPUs. Accelerated computing has reached a tipping point. Secondly, AI has also reached a tipping point and is transforming existing applications while enabling entirely new ones. For existing applications, generative AI is replacing classical machine learning in search ranking, recommender systems, ad targeting, click-through prediction to content moderation. The very foundations of hyperscale infrastructure.

Meta's GEM, a foundation model for ad recommendations trained on large-scale GPU clusters exemplifies this shift. In Q2, Meta reported over a 5% increase in ad conversions on Instagram and 3% gain on Facebook feed driven by generative AI-based GEM. Transitioning to generative AI represents substantial revenue gains for hyperscalers. Now a new wave is rising, agenetic AI systems capable of reasoning, planning and using tools from coding assistance like Cursor and Claude Code to radiology tools like Aidoc, legal assistants like Harvey and AI chauffeurs like Tesla FSD and Waymo. These systems mark the next frontier of computing, the fastest-growing companies in the world today, OpenAI, Anthropic, xAI, Google, Cursor, Lovable, Replit, Cognition AI, OpenEvidence, Abridge, Tesla are pioneering agentic AI.

So there are 3 massive platform shifts. The transition to accelerated computing is foundational and necessary essential in a post-Moore's Law era. The transition to generative AI is transformational and necessary, supercharging existing applications and business models. And the transition to agentic and physical AI will be revolutionary, giving rise to new applications, companies, products and services.

As you consider infrastructure investments, consider these 3 fundamental dynamics, each will contribute to infrastructure growth in the coming years. NVIDIA is chosen because our singular architecture enables all 3 transitions. And thus so, for any form and modality of AI across all industries, across every phase of AI, across all of the diverse computing needs in the cloud, and also from cloud to enterprise to robots, one architecture.

Toshiya, back to you.

Toshiya Hari
Vice President of Investor Relations & Strategic Finance

We will now open the call for questions. Operator, would you please poll for questions?