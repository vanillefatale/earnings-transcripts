Question-and-Answer Session

Operator

[Operator Instructions] And the first question comes from the line of Vivek Arya with Bank of America Securities.

Vivek Arya
BofA Securities, Research Division

I had a near-term and a medium-term question. For the near term, Lisa, I was hoping if you could give us some sense of the CPU, GPU mix in Q3 and Q4. And just tactically, how are you managing this transition from your MI355 towards MI400 in the second half of next year? Can you continue to grow in the first half of next year from these Q4 levels? Or should we expect some kind of pause or digestion before customers get onboard the MI400 Series?

Lisa Su
Chair, President & CEO

Sure, Vivek. Thanks for the question. So couple comments, we had a very strong Q3 for the data center business. I think we saw strong outperformance in both the server as well as the data center AI business. And a reminder that, that was without any MI308 sales. The MI355 has ramped really nicely. We expected a sharp ramp into the third quarter and that proceeded well. And as I mentioned, we've also seen some strengthening of the server CPU sales and not just, let's call it, near term, but we're seeing our customers are giving us some visibility in the next few quarters that they see elevated demand, which is positive.

Going into the fourth quarter, again, strong data center performance, up double digits sequentially and up in both server and data center AI, again, on the strength of those businesses. And to your question, I mean, we're not guiding into 2026 yet obviously, but given what we see today, we see a very good demand environment into 2026. So we would expect that MI355 continue to ramp in the first half of '26 and then, as we mentioned, MI450 Series comes online in the second half of 2026, and we would expect a sharper ramp as we go into the second half of 2026 of our data center AI business.

Vivek Arya
BofA Securities, Research Division

And for my follow-up, there is some industry debate, Lisa, about OpenAI's ability to kind of simultaneously engage with all 3 merchants and the ASIC suppliers, just given the constraints around power and CapEx and their existing kind of CSP partners and so forth. So how are you thinking about that? Like what is your level of visibility in the initial engagement and then more importantly how it kind of broadens out into '27? Is there a way that one can model what the allocation would be? Or just how should we think about the level of visibility in this very important customer?

Lisa Su
Chair, President & CEO

Yes, absolutely, Vivek. Look, we're very -- obviously, very excited about our relationship with OpenAI. It's a very significant relationship. Think about it as it's a pretty unique time for AI right now. There's just so much compute demand across all of the workloads. I think in our work with OpenAI we are planning multiple quarters out, ensuring that the power is available, that the supply chain is available.

The key point is the first gigawatt, we will start deploying in the second half of '26 and that work is well underway. And we continue -- just given where lead times are and things like that, we are planning very closely with OpenAI as well as the CSP partners to ensure that we're all prepared with Helios so that we can deploy the technology as we stated. So I think, overall, we're working very closely together. I think we have good visibility into the MI450 ramp, and things are progressing very well.

Operator

And the next question comes from the line of Thomas O'Malley with Barclays.

Thomas O'Malley
Barclays Bank PLC, Research Division

Congrats on the good results. I had a first question on Helios. Obviously, with the announcement at OCP, customer interaction has to be growing. Could you talk about into next year what your view is on discrete sales versus system sales? When do you see that crossover kind of happening? And just what initial responses have been from customers after getting a better look at it at the shelf?

Lisa Su
Chair, President & CEO

Yes, sure. Tom, thanks for the question. There's a lot of excitement around MI450 and Helios. I think the OCP reception was phenomenal. We had numerous customers and frankly bringing their engineering teams to understand more about the system, more about how it's built. There's always been some discussion about just how complex these rack-scale systems are, and they certainly are, and we are very proud of the Helios design. I think it has all the features, functions, reliability, performance, power performance that you would expect.

I think the interest in MI450 and Helios has just expanded over the last number of weeks, certainly with some of the announcements that we've made with OpenAI and OCI as well as the OCP show with Meta. I think, overall, from our perspective, I think things are going really well in both the development as well as the customer engagements there. So in terms of rack-scale solutions, we would expect that the early customers for MI450 will really be around the rack-scale solutions. We will have other form factors as well for the MI450 Series, but there's a lot of interest in the full rack-scale solution.

Thomas O'Malley
Barclays Bank PLC, Research Division

Super helpful. And then as my follow-up, it's a broader question as well and similar to kind of what Vivek asked. But if you look at the power requirements that are out there for some of the early announcements into next year, they're pretty substantial. And then you also have component issues that you're seeing across interconnected memory. Just from your perspective as an industry leader, where do you think that the constraint will be? Will it come first with components not being available? Or do you think that both data center footprint in terms of infrastructure and/or power is the gating factor to some of these deployments into next year just as we really see some larger number starts get deployed.

Lisa Su
Chair, President & CEO

Yes. Sure, Tom. I think what you're pointing out is what we, as an industry, have to do together. The entire ecosystem has to plan together and that is exactly what we're doing. So we're working with our customers on their power plans over the next, actually, I would say, 2 years from a silicon and a memory and a packaging and a component supply chain. We're working with our supply chain partners to make sure all of that capacity is available.

I can tell you from our visibility, we feel very good that we have a strong supply chain that is prepared to deliver sort of these very significant growth rates and large amount of compute that is out there. And I think all of this is going to be tight. I think there is a -- you can see from some of the CapEx spending that there's a desire to put on more compute, and we're working closely together.

I will say that the ecosystem is very -- I would say, works very hard when there are these types of, let's call it, tightness out there. And so we also see things open up as we're working, getting more power, getting more supply, all of those things. So the net-net is, I think, we are well positioned to grow significantly as we transition into the second half of '26 into '27 with the MI450 and Helios.

Operator

And the next question comes from the line of Joshua Buchalter with TD Cowen.

Joshua Buchalter
TD Cowen, Research Division

Actually, wanted to start on the CPU side. So you and your largest competitor in that space have talked about near-term strength, supporting AI workloads on general purpose servers from agentic. Maybe you could speak to the sustainability of these trends. And they called out supply constraints. Are you seeing any of those in your supply chain? And like are we in a period where we should think about the CPU business on the data center side as being a seasonal or should we expect normal seasonality in the first half of next year?

Lisa Su
Chair, President & CEO

Sure, Josh. A couple comments on the CPU server side. I think we've been watching this trend for the last couple of quarters and we started seeing, let's call it, some positive signs in CPU demand actually a couple quarters ago. And what's happened as we've gone through 2025 is now we see sort of a broadening of that CPU demand. So we have -- a number of our large hyperscale clients are now forecasting significant CPU build into 2026. And so from that standpoint, I think it's a positive demand environment, and it is because AI is requiring quite a bit of general-purpose compute. And that's great. It catches our cycle as we're ramping Turin. So the Turin ramp has gone extremely fast, and we see good pull for that product as well as consistent strong demand for our Genoa product line as well.

So back to seasonality as we go into 2026, I think we expect that the CPU demand environment into 2026 is going to be, let's call it, positive. And so we'll guide more as we get into the end of the year, but I would expect a positive demand environment for CPUs as we see this demand. I do feel like it's durable. It is not a short-term thing. I think it is a multi-quarter phenomenon as we're seeing just much more demand as these AI workloads really turn into -- you have to do real work.

Jean Hu
Executive VP, CFO, Treasurer & Interim Chief Accounting Officer

So Josh, on the supply side, we have supplies to support our growth and especially in 2026, we're prepared for the ramp.

Joshua Buchalter
TD Cowen, Research Division

Got it. And for my follow-up, Lisa, in your prepared remarks, you highlighted progress you guys have made on ROCm 7. I know this has been an area of focus. And can you maybe spend a minute or 2 talking about where you feel you're at competitively with ROCm? How wide is the breadth of support you're able to offer to the developer community? And what areas do you still have work to do to close any potential competitive gap?

Lisa Su
Chair, President & CEO

Yes, Josh, thanks for the question. Look, we've made great progress with ROCm. ROCm 7 is a significant step forward in terms of performance and sort of all the frameworks that we support. It's been really, really important for us to get sort of day zero support of all the newest models and native support for all the newest frameworks. I would say most customers who are starting with AMD now have a very smooth experience as they're bringing on their workload to AMD. There's obviously always more work to do.

We're continuing to augment the libraries and the overall environment that we have, especially as we go to some of the newer workloads where you see training and inference really coming together with reinforcement learning. But overall, I think very strong progress with ROCm. And by the way, we're going to continue to invest in this area because it's so important to really make our customer development experience as smooth as we can.

Operator

And the next question comes from the line of C.J. Muse with Cantor Fitzgerald.

Christopher Muse
Cantor Fitzgerald & Co., Research Division

I guess first question, as you think about the 355 to 400 transition and moving to full rack scale, is there a framework that we should be thinking about for gross margins throughout calendar '26?

Jean Hu
Executive VP, CFO, Treasurer & Interim Chief Accounting Officer

Yes, C.J., thanks for the question. I think in general, as we said in the past for our data center GPU business, the gross margin continue to improve when we ramp a new generation of product. Typically at the beginning of the ramp you go through a transition period, then you will normalize the gross margin. We're not guiding 2026, but our priority in data center GPU business is to really expand the top line revenue growth and the gross margin dollars, and of course, at the same time, we'll continue to drive the gross margin percentage up, too.

Christopher Muse
Cantor Fitzgerald & Co., Research Division

Very helpful. And I guess maybe, Lisa, to kind of probe kind of your growth expectations through '26 and beyond, and you talked about tens of billions of dollars in '27, can you kind of speak at a high level how you're thinking about OpenAI and other large customers and how we should be thinking about the breadth of your customer kind of penetration throughout calendar '26, '27? Any help on that would be super.

Lisa Su
Chair, President & CEO

Sure, C.J. And we'll certainly address this topic in more detail at our Analyst Day next week. But let me give you some maybe higher-level points. Look, I think we're really excited about our road map. I think we have seen great traction amongst the largest customers. The OpenAI relationship is extremely important to us, and it's great to be able to talk at the multi-gigawatt scale because I think that really is what we believe we can deliver to the marketplace.

But there are numerous other customers that we are in deep engagements with. We talked about OCI. We also announced a couple of systems with the Department of Energy that are significant systems, and we have many other engagements. So the way you should think about it is there are multiple customers that we would expect to have, let's call it, very significant scale in the MI450 generation. And that's sort of the breadth of the customer engagements that we've built, and it's also how we're dimensioning the supply chain to ensure that we can supply certainly our OpenAI partnership as well as the numerous other partnerships that are well underway.

Operator

And the next question comes from the line of Stacy Rasgon with Bernstein Research.

Stacy Rasgon
Sanford C. Bernstein & Co., LLC., Research Division

My first one, for data center in the quarter, what grew more year-over-year on a dollar to percentage basis, the servers or the GPUs?

Lisa Su
Chair, President & CEO

Yes. Stacy, I think our commentary was data center grew nicely year-over-year in both of the areas, both for servers as well as data center AI.

Stacy Rasgon
Sanford C. Bernstein & Co., LLC., Research Division

Yes. But could you -- I mean, just directionally, did one -- which one grew more than the other? I'm not even asking for numbers, just directionally.

Jean Hu
Executive VP, CFO, Treasurer & Interim Chief Accounting Officer

Directionally, they are similar, but server is a little bit better.

Stacy Rasgon
Sanford C. Bernstein & Co., LLC., Research Division

Server is a little bit better. Okay. And then on the guidance, so you said that servers -- I mean, data center overall up double digits. You said server is up strong double digits. What does that mean? Is that like more than 20%? Or like how do I think about what you mean by strong double digits? Because, again, I'm trying to -- like I mean, for the GPUs for the year, like do you think you're -- you were saying roughly like $6.5 billion or something last quarter for the year, do you think it's still in that range? It kind of feels like you're still there.

Jean Hu
Executive VP, CFO, Treasurer & Interim Chief Accounting Officer

Stacy, here is what we guided. We guided sequentially data center will be up double digits, and we said server will go up strongly. And at the same time, we also said that MI350 also going to ramp. So we did not -- I don't think what you just mentioned was what we guided.

Stacy Rasgon
Sanford C. Bernstein & Co., LLC., Research Division

Okay. So I mean if you say servers are up strongly, does that mean they're up more than the Instinct because you didn't really make that commentary on Instinct?

Lisa Su
Chair, President & CEO

No. Look, Stacy, let me say it. So data center [ up ] sequentially double-digit percentage, both server and data center AI are going to be up as well. And from the standpoint of where they are, I think we're pleased with how both of them are performing. The strong double-digit percentage comment perhaps was applying to the year-over-year commentary.

Operator

And the next question comes from the line of Timothy Arcuri with UBS.

Timothy Arcuri
UBS Investment Bank, Research Division

Lisa, I know it's only been a month since you announced this idea with OpenAI, but can you give us maybe some anecdotes of how this has influenced your position in the market with other customers? Like are you engaged with customers that you wouldn't have been engaged with if you hadn't done this deal? That's the first part of the question. And then the second part relates to a prior question, which is that it looks like they could be something like half of your data center GPU revenue in the 2027, 2028 time frame. So how much risk, in your mind, is there around that single customer for you?

Lisa Su
Chair, President & CEO

Sure, Tim. So let me say a couple things. First of all, the OpenAI deal has been in the works for quite some time. We're happy to be able to talk about it broadly and also talk about the scale of the deployments and the scale of the engagement being multiyear, multi-gigawatt. I think all those things were very positive. We've had a number of other engagements as well. I think over the last -- if you were to ask specifically over the last month, I would say that it's been a number of factors. I think the OpenAI deal was one of them.

I think having -- being able to show the Helios rack in full force at Open Compute was also a very important milestone because people could see the engineering and sort of the capabilities of the Helios rack. And if you're asking whether we've seen an increase of interest or an acceleration of interest, I think the answer is yes. I think customers are broadly engaged and perhaps broadly engaged at higher scale, which is a good thing.

And then from the standpoint of customer concentration, I think a very key foundation for us in this business is to have a broad set of customers. We've always been engaged with a number of customers. I think we're dimensioning the supply chain in such a way that we would have ample supply to have multiple customers at similar scale as we go into the '27, '28 time frame, and that's certainly the goal.

Operator

And the next question comes from the line of Aaron Rakers with Wells Fargo.

Aaron Rakers
Wells Fargo Securities, LLC, Research Division

I'm curious on the server strength that you're seeing, if there's a way to unpack how we think about unit growth versus ASP expansion as we move through the Turin product cycle. And how do you guys just kind of think about that going forward?

Lisa Su
Chair, President & CEO

Yes. So Aaron, on the server CPU side, Turin certainly is more content, so we see ASPs grow as Turin ramps. But I also mentioned in the prepared remarks that we're actually seeing a very good mix of Genoa still there. So Turin is ramping very quickly, but we are also seeing Genoa demand continue well as the hyperscalers are not able to move everything to the latest generation immediately.

So from our standpoint, I think it's broad-based CPU demand across a number of different workloads. This is -- a little bit of this is, let's call it, server refresh, but it seems like from our customer conversations, the workloads are broadly due to the fact that AI workloads are spawning more traditional compute, so more build-out is necessary.

I think going forward, one of the things that we see is there is more of a desire for the latest generation. And so as much as we're happy with how Turin is ramping, we're seeing actually a strong pull on Venice and a lot of early engagement in Venice, which kind of says a lot about kind of the importance of general-purpose compute at this point in time.

Aaron Rakers
Wells Fargo Securities, LLC, Research Division

As a quick follow-up, I'm curious and not to steal maybe the discussion from next week, but Lisa, you've been very consistent, like $500 billion of total AI silicon TAM opportunity and obviously progressing above that. I'm curious, as we think about these large megawatt kind of deployments, how you think about the updated views on that AI silicon TAM as we look forward.

Lisa Su
Chair, President & CEO

Well, Aaron, as you said, not to take too much away from what we're going to talk about next week, look, we're going to give you a full picture of how we see the market next week. But suffice it to say, from everything that we see, we see the AI compute TAM just going up. So we'll have some updated numbers for you, but the view is, whereas $500 billion sounded like a lot when we first talked about it, we think there is a larger opportunity for us over the next few years, and that's pretty exciting.

Operator

The next question comes from Antoine Chkaiban with New Street Research.

Antoine Chkaiban
New Street Research LLP

So I'd like to ask about whether the developing relationship with OpenAI could be a tailwind to the development of your software stack. Can you maybe tell us about how the collaboration works in practice and whether the partnership contributed in making ROCm more robust?

Lisa Su
Chair, President & CEO

Yes. Antoine, thanks for the question. I think the answer is yes. I think all of our large customers contribute to, let's call it, a broadening and deepening of our software stack overall. I think the relationship with OpenAI is certainly one where our plans are to work deeply together on hardware as well as software as well as systems and future road map. And from that standpoint, the work that we're doing together with them on Triton is certainly very valuable.

But I will say beyond OpenAI, the work that we do with all of our largest customers are super helpful to strengthen the software stack. And we have put significant new resources into not just the largest customers, but we are working with a broad set of AI-native companies who are actively developing on the ROCm stack. We get lots of feedback. I think we've made significant progress in the training and inference stack, and we're going to continue to double down and triple down in this area. So more customers that use AMD, I think all of that goes to enhancing the ROCm stack. And we're actually -- we'll talk a little bit more about this next week, but we're also using AI to help us accelerate the rate and pace of some of the ROCm kernel development and just the overall ecosystem.

Antoine Chkaiban
New Street Research LLP

Maybe as a quick follow-up, could you tell us about the useful lives of GPUs? I know that most CSPs depreciate them over 5, 6 years. But in your conversations with them, I'm just wondering if you see or hear any early indication that, in practice, they may be planning to sweat those GPUs for longer than that.

Lisa Su
Chair, President & CEO

I think we have seen some early indications of that, Antoine. I think the key point being, clearly, there's a desire to get on the latest and greatest GPUs when you're building new data center infrastructure. And certainly, when we're looking at MI355s, they're often going into new liquid-cooled facilities, MI450 Series as well. But then we're also seeing the other trend, which is there's just a need for more AI compute. And from that standpoint, some of the older generation -- MI300X is still doing quite well in terms of just where we see people deploying and using especially for inference. And from that standpoint, I think you see a little bit of both.

Operator

And the next question comes from the line of Joe Moore with Morgan Stanley.

Joseph Moore
Morgan Stanley, Research Division

You mentioned MI308, I guess what's your posture there to the extent that if there is some relief that you're able to ship, do you have readiness to do that? Can you give us a sense for how much of a swing factor that could be?

Lisa Su
Chair, President & CEO

Sure, Joe. So look, it's still a pretty dynamic situation with MI308. So that's the reason that we did not include any MI308 revenue in the Q4 guide. We have received some licenses for MI308, so we're appreciative of the administration supporting some licenses for MI308. We're still working with our customers on the demand environment and sort of what the overall opportunity is. And so we'll be able to update that more in the next couple of months.

Joseph Moore
Morgan Stanley, Research Division

Okay. But you do have product to support that market if it does open up? Or does -- are you going to have to start to kind of rebuild inventory for that?

Lisa Su
Chair, President & CEO

We've had some work in process. I think we continue to have that work in process, but we'll have to see sort of how the demand environment shapes up.

Operator

And the final question comes from the line of Ross Seymore with Deutsche Bank.

Ross Seymore
Deutsche Bank AG, Research Division

Lisa, this might take longer than the amount of time we have left before the top of the hour, but there's been so many of these multi-gigawatt announcements from OpenAI. How does AMD truly differentiate in there? When you see that big customer signing deals with other GPU vendors and ASIC vendors, et cetera, how do you attack that market differently than those competitors to not only get the 6 gigawatt initially but hopefully more after that?

Lisa Su
Chair, President & CEO

Sure, Ross. Well, look, what I see is actually this environment where the world needs more AI compute. And from that standpoint, I think OpenAI has kind of led in the quest for more AI compute, but they're not alone. I think when you look across the large customers, there is really a demand for more AI compute as you go forward over the next couple of years. I think we each have our advantages in terms of how we are positioning our products. I think MI450 Series, in particular, I think, is an extremely strong product, rack-scale solution. Overall, when we look at compute performance, when we look at memory performance, we think it's extremely well positioned for both inference as well as training.

I think the key here is time to market, it's total cost of ownership, it's deep partnership and thinking about not just MI450 Series, but what happens after that. So we're deep in conversations on MI500 and beyond. And we certainly think we're well positioned to not only participate but participate in a very meaningful way across the sort of the demand environment here. And I think we have certainly learned a ton over the last couple of years with our AI road map. We've made significant inroads in terms of just what the largest customer needs from a workload standpoint. So I'm pretty optimistic about our ability to capture a significant piece of this market going forward.

Ross Seymore
Deutsche Bank AG, Research Division

Great. And I guess as my follow-up, it'll be a direct follow-on to that. You did a unique structure by granting some warrants with this deal, and I know they're -- they vest according to a price that would be very accretive and make everybody happy. Do you think that was a relatively unique agreement or given that the world needs more processing power that AMD is open to somewhat similar, conceptually similar creative ways to address that demand over time with other equity vehicles, et cetera?

Lisa Su
Chair, President & CEO

Sure, Ross. So I would say it was a unique agreement from the standpoint that unique time in AI, what we wanted, what we prioritized was really deep partnership and multiyear, multigeneration, significant scale. And I think we got that. We got a structure that has extremely aligned incentives. Everybody wins, right? We win. OpenAI wins, and our shareholder win -- sort of benefits from this. And all of that accrues to the overall road map.

I think as we look forward, I think we have a lot of very interesting partnerships that are developing, whether they're with the largest AI users or you think about sovereign AI opportunities. And we look at each one of these as a unique opportunity where we're bringing sort of the whole of AMD, both technically as well as all the rest of our capabilities to the party. So I would say OpenAI was pretty unique, but I would imagine that there are lots of other opportunities for us to bring our capabilities into the ecosystem and participate in a significant way.

Operator

Ladies and gentlemen, that does conclude the question-and-answer session and that also concludes today's teleconference. We thank you for your participation. You may disconnect your lines at this time.