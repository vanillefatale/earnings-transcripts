Question-and-Answer Session

Jonathan Neilson
Vice President of Investor Relations

Thanks, Amy. We'll now move over to Q&A. Out of respects for others on the call, we request that participants please only ask 1 question. Operator, can you please repeat your instructions?

Operator

[Operator Instructions] And our first question comes from the line of Keith Weiss with Morgan Stanley.

Keith Weiss
Morgan Stanley, Research Division

I'm looking at Microsoft print where earnings is growing 24% year-on-year, which is a spectacular result. Great execution on your part, top line growing well, margins expanding. But I'm looking at after-hours trading and the stock is still down. And I think one of the core issues that is weighing on investors is CapEx is growing faster than we expected and maybe Azure is growing a little bit slower than we expected. And I think that fundamentally comes down to a concern on the ROI on this CapEx spend over time. So I was hoping you guys could help us fill in some of the blanks a little bit in terms of how should we think about capacity expansion and what that can yield in terms of Azure growth going forward. More to the point, how should we think about the ROI on this investment as it comes to fruition?

Amy Hood
Executive VP & CFO

Thanks, Keith. And let me start and Satya can add some broader comments, I'm sure. I think the first thing, I think you really asked a very direct correlation that I do think many investors are doing, which is between the CapEx spend and seeing an Azure revenue number. And we tried last quarter, and I think, again, this quarter to talk more specifically about all the places that the CapEx spend, especially the short-lived CapEx spend across CPU and GPU and where that will show up.

Sometimes, I think it's probably better to think about the Azure guidance that we give as an allocated capacity guide about what we can deliver in Azure revenue. Because as we spend the capital and put GPUs specifically, it applies to CPUs, the GPUs more specifically, we're really making long-term decisions. And the first thing we're doing is solving for the increased usage in sales and the accelerating pace of M365 Copilot as well as GitHub Copilot, our first-party apps. Then we make sure we're investing in the long-term nature of R&D and product innovation. And much of the acceleration that I think you've seen from us and products over the past a bit is coming because we are allocating GPUs and capacity to many of the talented AI people we've been hiring over the past years.

Then, when you end up, is that, you end up with the remainder going towards serving the Azure capacity that continues to grow in terms of demand. And a way to think about it, because I think, I get asked this question sometimes, is if I had taken the GPUs that just came online in Q1 and Q2 in terms of GPUs and allocated them all to Azure, the KPI would have been over 40. And I think the most important thing to realize is that this is about investing in all the layers of the stack that benefit customers. And I think that's hopefully helpful in terms of thinking about capital growth, it shows in every piece, it shows in revenue growth across the business and shows as OpEx growth as we invest in our people.

Satya Nadella
Chairman & CEO

Yes, I think you -- Amy covered it. But basically, as an investor, I think when you think about our capital and you think about the GM profile of our portfolio, you should obviously think about Azure. But you should think about M365 Copilot and you should think about GitHub pilot, you should think about Dragon Copilot, Security Copilot. All of those have a GM profile and lifetime value. I mean if you think about it, acquiring an Azure customer is super important to us, but so is acquiring an M365 or a GitHub or a Dragon Copilot, which are all by the way incremental businesses and TAMs for us. And so we don't want to maximize just 1 business of ours, we want to be able to allocate capacity while we're sort of supply constrained in a way that allow us to essentially build the best LTV portfolio. That's on one side. And the other one that Amy mentioned is also R&D. I mean you got to think about compute is also R&D, and that's sort of the second element of it. And so we are using all of that, obviously, to optimize for the long term.

Operator

The next question comes from the line of Mark Moerdler with Bernstein Research.

Mark Moerdler
Bernstein Institutional Services LLC, Research Division

Congrats on the quarter. One of the other questions we believe investors want to understand is how to think about your line of sight from hardware CapEx investment to revenue and margins. You capitalized servers over 6 years, but the average duration of your RPO is 2.5 years, up from 2 years last quarter. How do investors get comfortable that since this is a lot of this CapEx is AI-centric that you'll be able to capture sufficient revenue over the 6-year useful life of the hardware to deliver solid revenue and gross profit dollars growth, hopefully, one similar to the CPU revenue.

Amy Hood
Executive VP & CFO

Thanks, Mark. Let me start with at a high level and Satya can add as well. I think when you think about average duration, I think what you're getting to is -- and we need to remember, is it, average duration is a combination of a broad set of contract arrangements that we have. A lot of them around things like M365 or our BizApps portfolio, are shorter dated, right, 3-year contracts. And so they have, quite frankly, a short duration. The majority then that's remaining are Azure contracts are longer duration. And you saw that this quarter when we saw the extension of that duration from around 2 years to 2.5 years. And the way to think about that is the majority of the capital that we're spending today, and a lot of the GPUs that we're buying are already contracted for most of their useful life. And so a way to think about that is much of that risk that I think you're pointing to isn't there, because they're already sold for the entirety of their useful life. And so part of it exists because you have this shorter-dated RPO because of some of the M365 stuff. If you look at the Azure only, RPO is a little bit more extended. A lot of that is CPU basis. It's not just GPU. And on the GPU contracts that we've talked about, including for some of our largest customers, those are sold for the entire useful life of the GPU. And so there's not the risk to which I think you may be referring. Hopefully, that's helpful.

Satya Nadella
Chairman & CEO

Yes. And just to -- one other thing I would add to it is, in addition to sort of what Amy mentioned, which is it's already contracted for the useful life is we do use software to continuously around even the latest models on the fleet that is aging, if you will. So that's sort of what gives us that duration. And so at the end of the day, we want to have -- that's why we even think about aging the fleet constantly, right? So it's not about buying a whole lot of gear 1 year. It's about each year, you write the Moore's Law, you add, you use software, and then you optimize across all of it.

Amy Hood
Executive VP & CFO

And Mark, maybe to state this in case it's not obvious, is that as you go through the useful life, actually, you get more and more and more efficient at delivery. So where you've sold the entirety of its life, the margins actually improved with time. And so I think that may be a good reminder to people as we see that, obviously, in the CPU fleet all the time.

Operator

The next question comes from the line of Brent Thill with Jefferies.

Brent Thill
Jefferies LLC, Research Division

Amy, on 45% of the backlog being related to OpenAI. I'm just curious if you can comment, there's obviously concern about the durability. And I know maybe there's not much you can say on this, but I think everyone is concerned about the exposure. And if you could maybe talk through your perspective and what both you and Satya are seeing.

Amy Hood
Executive VP & CFO

I think maybe I would have thought about the question quite differently, Brent. The first thing to focus on is the reason we talked about that number is because 55% or roughly $350 billion is related to the breadth of our portfolio, a breadth of customers across solutions, across Azure, across industries, across geographies. That is a significant RPO balance, larger than most peers, more diversified than most peers. And frankly, I think we have super high confidence in it.

And when you think about that portion alone growing 28%, it's really impressive work on the breadth as well as the adoption curve that we're seeing, which is I think what I get asked most frequently, it's grown by customer segment, by industry and by geo. And so it's very consistent. And so then if you're asking about how do I feel about OpenAI and the contract and the health, listen, it's a great partnership. We continue to be their provider of scale. We're excited to do that. We sit under one of the most successful businesses built, and we continue to feel quite good about that. It's allowed us to remain a leader in terms of what we're building and being on the cutting edge of app innovation.

Operator

The next question comes from the line of Karl Keirstead with UBS.

Karl Keirstead
UBS Investment Bank, Research Division

Amy, regardless of how you allocate the capacity between first party and third party, can you comment qualitatively on the amount of capacity that's coming on. I think the 1 gigawatt added in the December quarter was extraordinary and hence that the capacity adds are accelerating. But I think a lot of investors have their eyes on Fairwater Atlanta, Fairwater Wisconsin, and would love some comments about the magnitude of the capacity adds regardless of how they're allocated in the coming quarters.

Amy Hood
Executive VP & CFO

Yes, Karl, I think we've said a couple of things. We're working as hard as we can to add capacity as quickly as we can. You've mentioned specific sites like Atlanta or Wisconsin, those are multiyear deliveries. So I wouldn't focus necessarily on specific locations. The real thing we've got to do, and we're working incredibly hard doing it, is adding capacity globally. A lot of that will be added in the United States, the 2 locations you've mentioned, but it also needs to be added across the globe to meet the customer demand that we're seeing and the increased usage. We'll continue to add both long-lived infrastructure. The way to think about that is we need to make sure we've got power and land and facilities available and we'll continue to put GPUs and CPUs in them when they're done as quickly as we can.

And then finally, we'll try to make sure we can get as efficient as we possibly can on the pace at which we do that and how we operate them so that they can have the highest possible utility. And so I think it's not really about 2 places, Karl, I would definitely abstract away from that. Those are multiyear delivery time lines. But really, we just need to get it done every location where we're currently in a build or starting to do that. We're working as quickly as we can.

Operator

The next question comes from the line of Mark Murphy with JPMorgan.

Mark Murphy
JPMorgan Chase & Co, Research Division

Satya, the performance achievements of the Maia 200 accelerator for inference, look quite remarkable, especially in comparison to TPUs and Trinium and Blackwell, which have just been around a lot longer. Could you put that accomplishment in perspective in terms of how much of a core competency you think silicon might become for Microsoft. And Amy, are there any ramifications worth mentioning there in terms of supporting your gross margin profile for inference costs going forward?

Satya Nadella
Chairman & CEO

Yes, thanks for the question. So a couple of things. One is we've been at this in a variety of different forms for a long, long time in terms of building our own silicon. And so we're very, very thrilled about the progress with Maia 200, and -- especially when we think about running a GPT-5.2 and the performance we were able to get in the GEMS at FB4, just proof point that when you have a new workload, a new shape of a workload, you can start innovating end-to-end between the model and the silicon and the entire system. It's just not even about just the silicon, the way the networking works at rack scale that's optimized with memory for this particular workload.

And the other thing is we're obviously round-tripping and working very closely with own super intelligence team with all of our models, as you can imagine, whatever we build will be all optimized for Maia. So we feel great about it. And I think the way to think about all up is we're in such early innings. I mean, even just look at the amount of silicon innovation and systems innovation. Even since December, I think the new thing is everybody is talking about low latency inference, right? And so one of the things we want to make sure is we are not locked into any one thing. If anything, we have great partnership with NVIDIA, with AMD, they are innovating, we're innovating. We want a fleet at any given point in time to have access to the best TCO. And it's not a one-generation game. I think a lot of folks just talk about who's ahead. It's just remember, you have to be ahead for all time to come. And that means you really want to think about having a lot of innovation that happens out there to be in your fleet, so that your fleet is fundamentally advantaged at the TCO level. So that's kind of how I look at it, which is we are excited about Maia. We're excited about Cobalt. We're excited about our DPU, our NIC. So we have a lot of systems capability. That means we can vertically integrate. And because we can vertically integrate doesn't mean we just only vertically integrate. And so we want to be able to have the flexibility here, and that's what you see us do.

Operator

The next question comes from the line of Brad Zelnick with Deutsche Bank.

Brad Zelnick
Deutsche Bank AG, Research Division

Satya, we heard a lot about Frontier transformations from Judson at Ignite, and we've seen customers realize breakthrough benefits when they adopt the Microsoft AI stack. Can you help frame for us the momentum in enterprises embarking on these journeys? And any expectation for how much their spend with Microsoft can expand in becoming frontier firms?

Satya Nadella
Chairman & CEO

Yes. Thank you for that. So I think one of the things that we are seeing is the adoption across the 3 major suites of ours, right? So if you take M365, you take what's happening with security and you take GitHub. In fact, it's fascinating. I mean these 3 things had effectively compounding effects for our customers in the past, like something like Entra as an identity system, or Defender as the protection system, across all 3 was sort of super helpful.

But so what now you're seeing is something like Work IQ, right? So I mean just to give you a favor for it, the most important database underneath for any company that uses Microsoft today is the data underneath Microsoft 365. And the reason is because it has all those tacit information, right, who are your people, what are their relationships, what are their projects they're working on, what are their artifacts, their communications. So that's a super important asset for any business process, business workflow context.

In fact, the scenario I even had in my transcript around you can now take Work IQ as an MCP server and GitHub Repo and say, "Hey, please look at my design meetings for the last month in Teams and tell me if my repo reflects it." I mean that's a pretty high-level way to think about how, what is happening previously perhaps with our tools business and our GitHub business are suddenly now being transformative, right? That agent black plane is really transforming companies in some sense, right? That's, I think, the most magical thing, which is you deploy these things. And suddenly, the agents are helping you coordinate, bring more leverage to your enterprise.

Then on top of it, of course, there is the transformation, which is what businesses are doing. How should we think about customer service. How should we think about marketing. How should we think about finance. How should we think about that and build our own agents. That's where all the services in Fabric and Foundry. And of course, GitHub tooling is helping them or even the low-code/no-code tools. I had some stats on how much that's being used. But one of the more exciting things for me is the new agents systems, M365 Copilot, GitHub Copilot, Security Copilot, all coming together to compound the benefits of all the data and all the deployment, I think, is probably the most transformative effect right now.

Jonathan Neilson
Vice President of Investor Relations

Thanks, Brad. Operator, we have time for 1 last question.

Operator

And the last question will come from the line of Raimo Lenschow with Barclays.

Raimo Lenschow
Barclays Bank PLC, Research Division

The last few quarters we talked -- besides the GPU side, you talked about CPU as well on the Azure side and you had some operational changes at the beginning of January last year. Can you speak what you saw there? And maybe put it more on a bigger picture in terms of clients realizing that their move to the cloud is important if you want to deliver proper AI. So what are we seeing in terms of cloud transition?

Satya Nadella
Chairman & CEO

I didn't quite...

Jonathan Neilson
Vice President of Investor Relations

Sorry, Raimo, you were asking about the SMC CPU side? Or can you just repeat the question, please?

Raimo Lenschow
Barclays Bank PLC, Research Division

Yes. Sorry. So I was wondering about the CPU side of Azure because we had some operational changes there. And we also hear from the [ field a lot ] that people are realizing they need to be in the Cloud if you want to do proper AI and if that's kind of driving momentum.

Satya Nadella
Chairman & CEO

Yes. I think I get it. So first of all, I had mentioned in my remarks that when you think about AI workloads, you should think of AI workloads as just AI accelerator compute, right? Because in some sense, you take any agent, the agent will then spawn through tools used maybe a container, which runs obviously on compute. In fact, we have -- whenever we think about even building out of the fleet, we think of in ratios or even for a training job, by the way. An AI training job requires a bunch of compute and a bunch of storage very close to compute. So therefore -- and same thing in inferencing as well.

So in inferencing with agent mode would require you to essentially provision a computer or computing resources to the agent. So not -- they don't need GPUs. They're running on GPUs, but they need computers, which are compute and storage. So that's what's happening even in the new world.

The other thing you mentioned is the Cloud migrations are still going on. In fact, 1 of the stats I had was SQL -- latest SQL server growing as an IaaS service in Azure. And so -- that's one of the reasons why we have to think about our commercial cloud and keep it balanced with the rest of our AI Cloud because when clients bring their workloads and build new workloads, they need all of these infrastructure elements in the region in which they are deploying.

Jonathan Neilson
Vice President of Investor Relations

That wraps up the Q&A portion of today's earnings call. Thank you for joining us today, and we look forward to speaking with you all soon.

Satya Nadella
Chairman & CEO

Thank you all.

Amy Hood
Executive VP & CFO

Thank you.

Operator

Thank you. This concludes today's conference. You may disconnect your lines at this time, and we thank you for your participation. Have a great night.