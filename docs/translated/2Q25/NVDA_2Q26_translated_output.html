<!DOCTYPE html>
<html><head><meta charset="UTF-8">
<title>Earnings Call 번역</title>
<style>
    body { font-family: Arial; margin: 40px; background-color: #fdfdfd; }
    h1 { text-align: center; }
    h2 { margin-top: 50px; color: #003366; }
    h3 { color: #333; }
    table { border: 1px solid #ddd; width: 100%; border-collapse: collapse; }
    th { background: #f0f0f0; padding: 10px; border-bottom: 2px solid #ccc; }
    td { padding: 10px; border-bottom: 1px dotted #ccc; vertical-align: top; }
    p { line-height: 1.6; }
    hr { margin: 50px 0; border: none; border-top: 1px solid #ccc; }
    .back-button {
        display: inline-block;
        background-color: #5f5f5f;
        color: white;
        padding: 10px 16px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        margin-bottom: 30px;
    }
</style>
</head><body>
<a href="../../index.html" class="back-button">←</a>
<h1>📄 Earnings Call Transcript 번역 결과</h1>

    <h2>📊 Presentation</h2>
    <table style="width:100%; border-collapse:collapse; margin-bottom: 40px;">
        <tr>
            <th style="width:50%; border-bottom: 2px solid #333;">Original</th>
            <th style="width:50%; border-bottom: 2px solid #333;">Translation</th>
        </tr>
        <tr><td>NVIDIA Corporation (NASDAQ:NVDA) Q2 2026 Earnings Conference Call August 27, 2025 5:00 PM ET<br><br>Company Participants<br><br>Colette M. Kress - Executive VP & CFO<br>Jen-Hsun Huang - Co-Founder, CEO, President & Director<br>Toshiya Hari - Vice President of Investor Relations & Strategic Finance<br><br>Conference Call Participants<br><br>Aaron Christopher Rakers - Wells Fargo Securities, LLC, Research Division<br>Benjamin Alexander Reitzes - Melius Research LLC<br>Christopher James Muse - Cantor Fitzgerald & Co., Research Division<br>James Edward Schneider - Goldman Sachs Group, Inc., Research Division<br>Joseph Lawrence Moore - Morgan Stanley, Research Division<br>Stacy Aaron Rasgon - Sanford C.</td><td>NVIDIA Corporation (NASDAQ:NVDA) 2026년 2분기 실적 컨퍼런스 콜 2025년 8월 27일 오후 5:00 ET<br><br>회사 참석자<br><br>Colette M. Kress - 부사장 겸 CFO<br>Jen-Hsun Huang - 공동창립자, CEO, 사장 겸 이사<br>Toshiya Hari - 투자자 관계 및 전략 재무 담당 부사장<br><br>컨퍼런스 콜 참석자<br><br>Aaron Christopher Rakers - Wells Fargo Securities, LLC, 리서치 부문<br>Benjamin Alexander Reitzes - Melius Research LLC<br>Christopher James Muse - Cantor Fitzgerald & Co., 리서치 부문<br>James Edward Schneider - Goldman Sachs Group, Inc., 리서치 부문<br>Joseph Lawrence Moore - Morgan Stanley, 리서치 부문<br>Stacy Aaron Rasgon - Sanford C.</td></tr>
<tr><td>Bernstein & Co., LLC., Research Division<br>Timothy Michael Arcuri - UBS Investment Bank, Research Division<br>Vivek Arya - BofA Securities, Research Division<br><br>Operator<br><br>Good afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Second Quarter Fiscal 2026 Financial Results Conference Call. [Operator Instructions] Thank you. Toshiya Hari, you may begin your conference. Toshiya Hari<br><br>Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the second quarter of fiscal 2026.</td><td>번스타인 앤 컴퍼니, LLC., 리서치 부문<br>티모시 마이클 아르쿠리 - UBS 인베스트먼트 뱅크, 리서치 부문<br>비벡 아리야 - 뱅크오브아메리카 시큐리티즈, 리서치 부문<br><br>운영자<br><br>안녕하십니까. 저는 사라이며, 오늘 컨퍼런스 운영자를 맡고 있습니다. 이 시간에 NVIDIA의 2026 회계연도 2분기 재무실적 컨퍼런스 콜에 참석해 주신 모든 분들을 환영합니다. [운영자 안내사항] 감사합니다. 토시야 하리 님, 컨퍼런스를 시작하셔도 됩니다.<br><br>토시야 하리<br><br>감사합니다. 안녕하십니까, 여러분. NVIDIA의 2026 회계연도 2분기 컨퍼런스 콜에 오신 것을 환영합니다.</td></tr>
<tr><td>With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer; and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the third quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations.</td><td>오늘 NVIDIA에서 참석해 주신 분들은 젠슨 황(Jensen Huang) 사장 겸 최고경영자와 콜레트 크레스(Colette Kress) 부사장 겸 최고재무책임자입니다. 저희 컨퍼런스 콜이 NVIDIA 투자자 관계 웹사이트에서 실시간으로 웹캐스트되고 있음을 알려드립니다. 웹캐스트는 2026 회계연도 3분기 재무실적을 논의하는 컨퍼런스 콜까지 다시보기로 제공될 예정입니다. 오늘 콜의 내용은 NVIDIA의 자산이며, 당사의 사전 서면 동의 없이는 복제하거나 전사할 수 없습니다. 이번 콜에서 저희는 현재의 기대치를 바탕으로 미래전망진술을 할 수 있습니다.</td></tr>
<tr><td>These are subject to a number of significant risks and uncertainties, and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, August 27, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures.</td><td>이러한 내용들은 다수의 중대한 위험과 불확실성을 수반하며, 당사의 실제 결과는 크게 다를 수 있습니다. 당사의 향후 재무 실적과 사업에 영향을 미칠 수 있는 요인들에 대한 논의는 오늘 발표된 실적 보고서, 당사의 최신 Form 10-K 및 10-Q, 그리고 증권거래위원회에 제출할 수 있는 Form 8-K 보고서의 공시 내용을 참조해 주시기 바랍니다. 당사의 모든 발언은 현재 당사가 이용 가능한 정보를 바탕으로 오늘인 2025년 8월 27일 기준으로 작성되었습니다. 법률에서 요구하는 경우를 제외하고, 당사는 이러한 발언을 업데이트할 의무를 지지 않습니다. 이번 컨퍼런스 콜에서는 비GAAP 재무 지표에 대해 논의할 예정입니다.</td></tr>
<tr><td>You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. With that, let me turn the call over to Colette. Colette M. Kress<br><br>Thank you, Toshiya. We delivered another record quarter while navigating what continues to be a dynamic external environment. Total revenue was $46.7 billion, exceeded our outlook as we grew sequentially across all market platforms. Data center revenue grew 56% year-over-year. Data center revenue also grew sequentially despite the $4 billion decline in H20 revenue. NVIDIA's Blackwell platform reached record levels, growing sequentially by 17%.</td><td>이러한 비GAAP 재무지표와 GAAP 재무지표 간의 조정표는 저희 웹사이트에 게시된 CFO 코멘터리에서 확인하실 수 있습니다. 그럼 이제 콜렛에게 마이크를 넘기겠습니다.<br><br>콜렛 M. 크레스<br><br>토시야, 감사합니다. 저희는 지속적으로 역동적인 외부 환경을 헤쳐나가면서도 또 다른 기록적인 분기를 달성했습니다. 총 매출은 467억 달러로 전망치를 상회했으며, 모든 시장 플랫폼에서 순차적 성장을 기록했습니다. 데이터센터 매출은 전년 동기 대비 56% 증가했습니다. H20 매출이 40억 달러 감소했음에도 불구하고 데이터센터 매출은 순차적으로도 성장했습니다. NVIDIA의 블랙웰 플랫폼은 기록적인 수준에 도달하여 순차적으로 17% 성장했습니다.</td></tr>
<tr><td>We began production shipments of GB300 in Q2. Our full stack AI solutions for cloud service providers, neoclouds, enterprises and sovereigns are all contributing to our growth. We are at the beginning of an industrial revolution that will transform every industry. We see $3 trillion to $4 trillion in AI infrastructure spend in the -- by the end of the decade. The scale and scope of these build-outs present significant long-term growth opportunities for NVIDIA. The GB200 NVL system is seeing widespread adoption with deployments at CSPs and consumer Internet companies.</td><td>2분기에 GB300의 생산 출하를 시작했습니다. 클라우드 서비스 제공업체, 네오클라우드, 기업 및 국가 기관을 위한 당사의 풀스택 AI 솔루션이 모두 성장에 기여하고 있습니다. 우리는 모든 산업을 변화시킬 산업 혁명의 시작점에 있습니다. 2020년대 말까지 AI 인프라 지출이 3조 달러에서 4조 달러에 이를 것으로 예상합니다. 이러한 구축의 규모와 범위는 NVIDIA에게 상당한 장기 성장 기회를 제공합니다. GB200 NVL 시스템은 CSP와 소비자 인터넷 기업에서의 배포를 통해 광범위한 채택을 보이고 있습니다.</td></tr>
<tr><td>Lighthouse model builders, including OpenAI, Meta and Mistral are using the GB200 NVL72 at data center scale for both training, next- generation models and serving inference models in production. The new Blackwell Ultra platform has also had a strong quarter, generating tens of billions in revenue. The transition to the GB300 has been seamless for major cloud service providers due to its shared architecture, software and physical footprint with the GB200, enabling them to build and deploy GB300 racks with ease. The transition to the new GB300 rack-based architecture has been seamless.</td><td>등대 모델 구축업체들인 OpenAI, Meta, Mistral은 차세대 모델 훈련과 프로덕션 환경에서의 추론 모델 서빙 모두를 위해 데이터센터 규모로 GB200 NVL72를 사용하고 있습니다. 새로운 Blackwell Ultra 플랫폼 역시 강력한 분기 실적을 기록하며 수백억 달러의 매출을 창출했습니다. GB300으로의 전환은 GB200과 공유하는 아키텍처, 소프트웨어, 물리적 설치공간 덕분에 주요 클라우드 서비스 제공업체들에게 매끄럽게 진행되었으며, 이를 통해 GB300 랙을 쉽게 구축하고 배포할 수 있게 되었습니다. 새로운 GB300 랙 기반 아키텍처로의 전환은 원활하게 이루어졌습니다.</td></tr>
<tr><td>Factory builds in late July and early August were successfully converted to support the GB300 ramp, and today, full production is underway. The current run rate is back at full speed, producing approximately 1,000 racks per week. This output is expected to accelerate even further throughout the third quarter as additional capacity comes online. We expect widespread market availability in the second half of the year as CoreWeave prepares to bring their GB300 instance to market as they are already seeing 10x more inference performance on reasoning models compared to H100.</td><td>7월 말과 8월 초 공장 구축이 성공적으로 GB300 램프업을 지원하도록 전환되었으며, 현재 전면 생산이 진행 중입니다. 현재 가동률은 다시 최고 속도로 돌아와 주당 약 1,000개 랙을 생산하고 있습니다. 3분기 내내 추가 생산능력이 온라인으로 투입됨에 따라 이러한 생산량은 더욱 가속화될 것으로 예상됩니다. CoreWeave가 이미 H100 대비 추론 모델에서 10배 더 높은 추론 성능을 확인하고 있어 GB300 인스턴스를 시장에 출시할 준비를 하고 있는 만큼, 하반기에는 광범위한 시장 출시가 이루어질 것으로 예상합니다.</td></tr>
<tr><td>Compared to the previous Hopper generation, GB300 NVL72 AI factories promise a 10x improvement in token per watt energy efficiency, which translates to revenues as data centers are power limited. The chips of the Rubin platform are in fab, the Vera CPU, Rubin GPU, CX9 SuperNIC, NVLink 144 scale up switch, Spectrum-X scale out and scale across switch, and the silicon photonics processor. Rubin remains on schedule for volume production next year. Rubin will be our third-generation NVLink rack scale AI supercomputer with a mature and full-scale supply chain.</td><td>이전 Hopper 세대와 비교하여, GB300 NVL72 AI 팩토리는 토큰당 와트 에너지 효율성에서 10배 개선을 약속하며, 이는 데이터센터가 전력 제약을 받는 상황에서 매출로 직결됩니다. Rubin 플랫폼의 칩들이 파운드리에 투입되었습니다 - Vera CPU, Rubin GPU, CX9 SuperNIC, NVLink 144 스케일업 스위치, Spectrum-X 스케일아웃 및 스케일어크로스 스위치, 그리고 실리콘 포토닉스 프로세서입니다. Rubin은 내년 대량생산 일정을 그대로 유지하고 있습니다. Rubin은 성숙하고 완전한 규모의 공급망을 갖춘 우리의 3세대 NVLink 랙 스케일 AI 슈퍼컴퓨터가 될 것입니다.</td></tr>
<tr><td>This keeps us on track with our pace of an annual product cadence and continuous innovation across compute, networking, systems and software. In late July, the U.S. government began reviewing licenses for sales of H20 to China customers. While a select number of our China- based customers have received licenses over the past few weeks, we have not shipped any H20 based on those licenses. USG officials have expressed an expectation that the USG will receive 15% of the revenue generated from licensed H20 sales, but to date, the USG has not published a regulation codifying such requirement. We have not included H20 in our Q3 outlook as we continue to work through geopolitical issues.</td><td>이를 통해 컴퓨팅, 네트워킹, 시스템 및 소프트웨어 전반에 걸친 연간 제품 출시 주기와 지속적인 혁신이라는 우리의 속도를 유지하고 있습니다. 7월 말, 미국 정부는 중국 고객에 대한 H20 판매 라이선스 검토를 시작했습니다. 지난 몇 주 동안 일부 중국 소재 고객들이 라이선스를 받았지만, 저희는 해당 라이선스를 기반으로 한 H20 제품을 아직 출하하지 않았습니다. 미국 정부 관계자들은 라이선스가 승인된 H20 판매로 발생하는 매출의 15%를 미국 정부가 받을 것으로 기대한다고 표명했지만, 현재까지 미국 정부는 그러한 요구사항을 성문화한 규정을 발표하지 않았습니다. 저희는 지정학적 이슈들을 계속 해결해 나가고 있는 상황이므로 3분기 전망에 H20을 포함하지 않았습니다.</td></tr>
<tr><td>If geopolitical issues reside, we should ship $2 billion to $5 billion in H20 revenue in Q3. And if we had more orders, we can bill more. We continue to advocate for the U.S. government to approve Blackwell for China. Our products are designed and sold for beneficial commercial use, and every license sale we make will benefit the U.S. economy, the U.S. leadership. In highly competitive markets, we want to win the support of every developer. America's AI technology stack can be the world's standard if we race and compete globally. Notably, in the quarter was an increase in Hopper 100 and H200 shipments.</td><td>지정학적 이슈가 해결된다면, 3분기에 H20 매출로 20억 달러에서 50억 달러를 출하할 수 있을 것입니다. 그리고 더 많은 주문이 있다면 더 많이 청구할 수 있습니다. 저희는 미국 정부가 중국에 대한 Blackwell 승인을 해주도록 지속적으로 요청하고 있습니다. 저희 제품들은 유익한 상업적 용도를 위해 설계되고 판매되며, 저희가 하는 모든 라이선스 판매는 미국 경제와 미국의 리더십에 도움이 될 것입니다. 경쟁이 치열한 시장에서 저희는 모든 개발자들의 지지를 얻고 싶습니다. 저희가 전 세계적으로 경쟁하고 달려간다면 미국의 AI 기술 스택이 세계 표준이 될 수 있습니다. 이번 분기에 주목할 점은 Hopper 100과 H200 출하량이 증가했다는 것입니다.</td></tr>
<tr><td>We also sold approximately $650 million of H20 in Q2 to an unrestricted customer outside of China. The sequential increase in Hopper demand indicates the breadth of data center workloads that run on accelerated computing and the power of CUDA libraries and full stack optimizations, which continuously enhance the performance and economic value of our platform. As we continue to deliver both Hopper and Blackwell GPUs, we are focusing on meeting the soaring global demand. This growth is fueled by capital expenditures from the cloud to enterprises, which are on track to invest $600 billion in data center infrastructure and compute this calendar year alone, nearly doubling in 2 years.</td><td>또한 2분기에 중국 외 비제한 고객에게 H20을 약 6억 5천만 달러 규모로 판매했습니다. Hopper 수요의 순차적 증가는 가속 컴퓨팅에서 실행되는 데이터센터 워크로드의 폭넓음과 CUDA 라이브러리 및 풀스택 최적화의 강력함을 보여주며, 이는 지속적으로 우리 플랫폼의 성능과 경제적 가치를 향상시키고 있습니다. Hopper와 Blackwell GPU를 모두 지속적으로 공급하면서, 우리는 급증하는 글로벌 수요를 충족하는 데 집중하고 있습니다. 이러한 성장은 클라우드부터 기업에 이르는 자본 지출에 의해 견인되고 있으며, 올해만 데이터센터 인프라와 컴퓨팅에 6천억 달러를 투자할 예정으로, 이는 2년 만에 거의 두 배 증가한 수준입니다.</td></tr>
<tr><td>We expect annual AI infrastructure investments to continue growing, driven by the several factors: reasoning agentic AI requiring orders of magnitude more training and inference compute, global build- outs for sovereign AI, enterprise AI adoption, and the arrival of physical AI and robotics. Blackwell has set the benchmark as it is the new standard for AI inference performance. The market for AI inference is expanding rapidly with reasoning and agentic AI gaining traction across industries. Blackwell's rack scale NVLink and CUDA full stack architecture addresses this by redefining the economics of inference.</td><td>우리는 연간 AI 인프라 투자가 지속적으로 증가할 것으로 예상하며, 이는 여러 요인에 의해 견인될 것입니다: 추론 에이전틱 AI가 기존 대비 몇 배 더 많은 훈련 및 추론 컴퓨팅을 요구하는 점, 주권 AI를 위한 글로벌 구축, 기업 AI 도입, 그리고 물리적 AI와 로보틱스의 등장입니다. Blackwell은 AI 추론 성능의 새로운 표준으로서 벤치마크를 설정했습니다. AI 추론 시장은 추론 및 에이전틱 AI가 산업 전반에서 견인력을 얻으면서 급속히 확장되고 있습니다. Blackwell의 랙 스케일 NVLink와 CUDA 풀스택 아키텍처는 추론의 경제성을 재정의함으로써 이러한 요구를 해결합니다.</td></tr>
<tr><td>New NVFP4 4-bit precision and NVLink 72 on the GB300 platform delivers a 50x increase in energy efficiency per token compared to Hopper, enabling companies to monetize their compute at unprecedented scale. For instance, a $3 million investment in GB200 infrastructure can generate $30 million in token revenue, a 10x return. NVIDIA software innovation, combined with the strength of our developer ecosystem, has already improved Blackwell's performance by more than 2x since its launch. Advances in CUDA, TensorRT-LLM and Dynamo are unlocking maximum efficiency.</td><td>새로운 NVFP4 4비트 정밀도와 GB300 플랫폼의 NVLink 72는 Hopper 대비 토큰당 에너지 효율성을 50배 향상시켜, 기업들이 전례 없는 규모로 컴퓨팅 자원을 수익화할 수 있게 합니다. 예를 들어, GB200 인프라에 300만 달러를 투자하면 3,000만 달러의 토큰 수익을 창출할 수 있어 10배의 수익률을 달성할 수 있습니다. NVIDIA의 소프트웨어 혁신과 개발자 생태계의 강점이 결합되어 Blackwell의 성능을 출시 이후 이미 2배 이상 개선했습니다. CUDA, TensorRT-LLM, Dynamo의 발전이 최대 효율성을 실현하고 있습니다.</td></tr>
<tr><td>CUDA library contributions from the open source community, along with NVIDIA's open libraries and frameworks are now integrated into millions of workflows. This powerful flywheel of collaborative innovation between NVIDIA and global community contribution strengthens NVIDIA's performance leadership. NVIDIA is a top contributor to OpenAI models, data and software. Blackwell has introduced a groundbreaking numerical approach to large language model pretraining. Using NVFP4 computations on the GB300 can now achieve 7x faster training than the H100, which uses FP8.</td><td>CUDA 라이브러리에 대한 오픈소스 커뮤니티의 기여와 NVIDIA의 오픈 라이브러리 및 프레임워크가 이제 수백만 개의 워크플로우에 통합되었습니다. NVIDIA와 글로벌 커뮤니티 기여 간의 이러한 강력한 협력적 혁신의 선순환 구조는 NVIDIA의 성능 리더십을 더욱 강화하고 있습니다. NVIDIA는 OpenAI 모델, 데이터 및 소프트웨어의 주요 기여자입니다. Blackwell은 대규모 언어 모델 사전 훈련에 획기적인 수치적 접근법을 도입했습니다. GB300에서 NVFP4 연산을 사용하면 이제 FP8을 사용하는 H100 대비 7배 빠른 훈련을 달성할 수 있습니다.</td></tr>
<tr><td>This innovation delivers the accuracy of 16-bit precision with the speed and efficiency of 4 bit, setting a new standard for AI factor efficiency and scalability. The AI industry is quickly adopting this revolutionary technology with major players such as AWS, Google Cloud, Microsoft Azure and OpenAI as well as Cohere, Mistral, Kimi AI, Perplexity, Reflection and Runway already embracing it. NVIDIA's performance leadership was further validated in the latest MLPerf Training benchmarks, where the GB200 delivered a clean sweep. Be on the lookout for the upcoming MLPerf Inference results in September, which will include benchmarks based on the Blackwell Ultra.</td><td>이 혁신 기술은 4비트의 속도와 효율성으로 16비트 정밀도의 정확성을 구현하여 AI 요소 효율성과 확장성의 새로운 기준을 제시합니다. AI 업계는 이 혁신적인 기술을 빠르게 도입하고 있으며, AWS, Google Cloud, Microsoft Azure, OpenAI를 비롯해 Cohere, Mistral, Kimi AI, Perplexity, Reflection, Runway 등 주요 업체들이 이미 이를 채택하고 있습니다. NVIDIA의 성능 리더십은 최신 MLPerf Training 벤치마크에서 GB200이 전 부문 1위를 석권하며 다시 한번 입증되었습니다. Blackwell Ultra 기반 벤치마크가 포함될 9월 MLPerf Inference 결과 발표를 주목해 주시기 바랍니다.</td></tr>
<tr><td>NVIDIA RTX PRO servers are in full production for the world system makers. These are air-cooled PCIe-based systems integrated seamlessly into standard IT environments and run traditional enterprise IT applications as well as the most advanced agentic and physical AI applications. Nearly 90 companies including many global leaders are already adopting RTX PRO servers. Hitachi uses them for real-time simulation and digital twins, Lilly for drug discovery, Hyundai for factory design and AV validation, and Disney for immersive storytelling. As enterprises modernize data centers, RTX PRO servers are poised to become a multibillion-dollar product line.</td><td>NVIDIA RTX PRO 서버는 전 세계 시스템 제조업체들을 위해 본격적인 생산에 들어갔습니다. 이는 표준 IT 환경에 완벽하게 통합되는 공랭식 PCIe 기반 시스템으로, 기존의 기업용 IT 애플리케이션뿐만 아니라 가장 진보된 에이전틱 AI 및 물리적 AI 애플리케이션을 구동합니다. 다수의 글로벌 리더들을 포함한 거의 90개 기업이 이미 RTX PRO 서버를 도입하고 있습니다. 히타치는 실시간 시뮬레이션과 디지털 트윈에, 릴리는 신약 개발에, 현대는 공장 설계 및 자율주행 검증에, 그리고 디즈니는 몰입형 스토리텔링에 이를 활용하고 있습니다. 기업들이 데이터센터를 현대화함에 따라, RTX PRO 서버는 수십억 달러 규모의 제품군으로 성장할 준비가 되어 있습니다.</td></tr>
<tr><td>Sovereign AI is one on the rise as the nation's ability to develop its own AI using domestic infrastructure, data and talent presents a significant opportunity for NVIDIA. NVIDIA is at the forefront of landmark initiatives across the U.K. and Europe. The European<br><br>Union plans to invest EUR 20 billion to establish 20 AI factories across France, Germany, Italy and Spain, including 5 gigafactories to increase its AI compute infrastructure by tenfold. In the U.K., the Isambard-AI supercomputer powered by NVIDIA was unveiled at the country's most powerful AI system, delivering 21 exaflops of AI performance to accelerate breakthroughs in fields of drug discovery and climate modeling.</td><td>주권 AI는 각국이 자국의 인프라, 데이터, 인재를 활용하여 독자적인 AI를 개발할 수 있는 능력이 부상하면서 NVIDIA에게 상당한 기회를 제공하고 있습니다. NVIDIA는 영국과 유럽 전역의 획기적인 이니셔티브 최전선에 있습니다. 유럽연합은 프랑스, 독일, 이탈리아, 스페인에 20개의 AI 팩토리를 구축하기 위해 200억 유로를 투자할 계획이며, 이 중 5개의 기가팩토리를 통해 AI 컴퓨팅 인프라를 10배 증대시킬 예정입니다. 영국에서는 NVIDIA가 구동하는 Isambard-AI 슈퍼컴퓨터가 영국 최강의 AI 시스템으로 공개되었으며, 21 엑사플롭스의 AI 성능을 제공하여 신약 개발과 기후 모델링 분야의 혁신을 가속화하고 있습니다.</td></tr>
<tr><td>We are on track to achieve over [ 20 billion ] in Sovereign AI revenue this year, more than double than that last year. Networking delivered record revenue of $7.3 billion, and escalating demands of AI compute clusters necessitate high efficiency and low latency networking. This represents a 46% sequential and 98% year-on-year increase with strong demand across Spectrum- X Ethernet, InfiniBand and NVLink. Our Spectrum-X enhanced Ethernet solutions provide the highest throughput and lowest latency network for Ethernet AI workloads. Spectrum-X Ethernet delivered double-digit sequential and year-over-year growth with annualized revenue exceeding $10 billion.</td><td>우리는 올해 Sovereign AI 매출에서 200억 달러 이상을 달성할 궤도에 있으며, 이는 작년 대비 두 배 이상 증가한 수치입니다. 네트워킹 부문은 73억 달러의 기록적인 매출을 달성했으며, AI 컴퓨팅 클러스터의 증가하는 수요로 인해 고효율성과 저지연 네트워킹이 필요해지고 있습니다. 이는 전분기 대비 46%, 전년 동기 대비 98% 증가한 수치로, Spectrum-X 이더넷, InfiniBand, NVLink 전반에 걸쳐 강력한 수요를 보여줍니다. 당사의 Spectrum-X 강화 이더넷 솔루션은 이더넷 AI 워크로드를 위한 최고 처리량과 최저 지연시간 네트워크를 제공합니다. Spectrum-X 이더넷은 전분기 및 전년 동기 대비 두 자릿수 성장을 달성했으며, 연간 매출이 100억 달러를 초과했습니다.</td></tr>
<tr><td>At Hot Chips, we introduced Spectrum-XGS Ethernet, a technology design to unify disparate data centers into giga-scale AI super factories. [ CoreWeave ] is an initial adopter of the solution, which is projected to double GPU-to-GPU communication speed. InfiniBand revenue nearly doubled sequentially, fueled by the adoption of XDR technology, which provides double the bandwidth improvement over its predecessor, especially valuable for the model builders. The world's fastest switch, NVLink, with 14x the bandwidth of PCIe Gen 5 delivered strong growth as customers deployed Grace Blackwell NVLink rack scale systems.</td><td>Hot Chips에서 우리는 서로 다른 데이터센터들을 기가스케일 AI 슈퍼팩토리로 통합하도록 설계된 기술인 Spectrum-XGS Ethernet을 소개했습니다. CoreWeave는 이 솔루션의 초기 도입업체로, GPU 간 통신 속도를 2배로 향상시킬 것으로 예상됩니다. InfiniBand 매출은 전분기 대비 거의 2배 증가했으며, 이는 이전 세대 대비 2배의 대역폭 개선을 제공하는 XDR 기술 도입에 힘입은 것으로, 특히 모델 구축업체들에게 가치가 높습니다. 세계에서 가장 빠른 스위치인 NVLink는 PCIe Gen 5 대비 14배의 대역폭을 제공하며, 고객들이 Grace Blackwell NVLink 랙 스케일 시스템을 배포함에 따라 강력한 성장을 보였습니다.</td></tr>
<tr><td>The positive reception to NVLink Fusion, which allows semi-custom AI infrastructure, has been widespread. Japan's upcoming FugakuNEXT will integrate Fujitsu's CPUs with our architecture via NVLink Fusion. It will run a range of workloads, including AI, supercomputing and quantum computing. FugakuNEXT joins a rapidly expanding list of leading quantum supercomputing and research centers running on NVIDIA's CUDA-Q quantum platform, including [ ULIC ], AIST, [ NNF ] and NERSC, supported by over 300 ecosystem partners, including AWS, Google Quantum AI, Quantinuum, QuEra and PsiQuantum. Jetson Thor, our new robotics computing platform, is now available.</td><td>반맞춤형 AI 인프라를 가능하게 하는 NVLink Fusion에 대한 긍정적인 반응이 광범위하게 나타나고 있습니다. 일본의 차세대 FugakuNEXT는 NVLink Fusion을 통해 후지쯔의 CPU와 우리 아키텍처를 통합할 예정입니다. 이 시스템은 AI, 슈퍼컴퓨팅, 양자컴퓨팅을 포함한 다양한 워크로드를 실행할 것입니다. FugakuNEXT는 NVIDIA의 CUDA-Q 양자 플랫폼에서 운영되는 선도적인 양자 슈퍼컴퓨팅 및 연구센터들의 빠르게 확장되는 목록에 합류하게 됩니다. 여기에는 ULIC, AIST, NNF, NERSC가 포함되며, AWS, Google Quantum AI, Quantinuum, QuEra, PsiQuantum을 포함한 300개 이상의 생태계 파트너들의 지원을 받고 있습니다. 우리의 새로운 로보틱스 컴퓨팅 플랫폼인 Jetson Thor가 이제 출시되었습니다.</td></tr>
<tr><td>Thor delivers an order of magnitude greater AI performance and energy efficiency than NVIDIA AGX Orin. It runs the latest generative and reasoning AI models at the edge in real time, enabling state-of-the-art robotics. Adoption of NVIDIA's robotics full stack platform is growing at rapid rate, over 2 million developers and 1,000-plus hardware software applications and sensor partners taking our platform to market. Leading enterprises across industries have adopted Thor, including Agility Robotics, Amazon Robotics, Boston Dynamics, Caterpillar, Figure, Hexagon, Medtronic and Meta.</td><td>Thor는 NVIDIA AGX Orin 대비 한 자릿수 더 뛰어난 AI 성능과 에너지 효율성을 제공합니다. Thor는 최신 생성형 AI 및 추론 AI 모델을 엣지에서 실시간으로 구동하여 최첨단 로보틱스를 구현합니다. NVIDIA의 로보틱스 풀스택 플랫폼 채택이 급속도로 증가하고 있으며, 200만 명 이상의 개발자와 1,000개 이상의 하드웨어 소프트웨어 애플리케이션 및 센서 파트너들이 우리 플랫폼을 시장에 출시하고 있습니다. Agility Robotics, Amazon Robotics, Boston Dynamics, Caterpillar, Figure, Hexagon, Medtronic, Meta를 포함한 각 산업 분야의 선도 기업들이 Thor를 채택했습니다.</td></tr>
<tr><td>Robotic applications require exponentially more compute on the device and in infrastructure, representing a significant long-term demand driver for our data center platform. NVIDIA Omniverse with Cosmos is our data center physical AI digital twin platform built for development of robot and robotic systems. This quarter, we announced a major expansion of our partnership with Siemens to enable AI automatic factories. Leading European robotics companies, including Agile Robots, NEURA Robotics and Universal Robots are building their latest innovations with the Omniverse platform. Transitioning to a quick summary of our revenue by geography.</td><td>로봇 애플리케이션은 디바이스와 인프라에서 기하급수적으로 더 많은 컴퓨팅 파워를 요구하며, 이는 당사 데이터센터 플랫폼에 대한 중요한 장기 수요 동력을 나타냅니다. Cosmos를 탑재한 NVIDIA Omniverse는 로봇 및 로봇 시스템 개발을 위해 구축된 당사의 데이터센터 물리적 AI 디지털 트윈 플랫폼입니다. 이번 분기에 저희는 AI 자동화 공장을 구현하기 위해 지멘스와의 파트너십을 대폭 확대한다고 발표했습니다. Agile Robots, NEURA Robotics, Universal Robots를 포함한 유럽의 주요 로봇 기업들이 Omniverse 플랫폼으로 최신 혁신 기술을 구축하고 있습니다. 이제 지역별 매출 현황을 간략히 요약해 드리겠습니다.</td></tr>
<tr><td>China declined on a sequential basis to low single-digit percentage of data center revenue. Note, our Q3 outlook does not include H20 shipments to China customers. Singapore revenue represented 22% of second quarter's billed revenue as customers have centralized their invoicing in Singapore. Over 99% of data center compute revenue billed to Singapore was for U.S.-based customers. Our gaming revenue was a record $4.3 billion, a 14% sequential increase and a 49% jump year-on-year. This was driven by the ramp of Blackwell GeForce GPUs and strong sales continued as we increase supply availability. This quarter, we shipped GeForce RTX 5060 desktop GPU.</td><td>중국은 순차적으로 데이터센터 매출의 한 자릿수 낮은 비율로 감소했습니다. 참고로, 3분기 전망에는 중국 고객에 대한 H20 출하량이 포함되지 않았습니다. 싱가포르 매출은 고객들이 싱가포르에서 청구서 발행을 중앙집중화함에 따라 2분기 청구 매출의 22%를 차지했습니다. 싱가포르에 청구된 데이터센터 컴퓨팅 매출의 99% 이상이 미국 기반 고객을 위한 것이었습니다. 게이밍 매출은 기록적인 43억 달러를 달성했으며, 이는 전분기 대비 14% 증가, 전년 동기 대비 49% 급증한 수치입니다. 이는 Blackwell GeForce GPU의 램프업과 공급 가용성 증대에 따른 강력한 판매 지속에 의해 견인되었습니다. 이번 분기에 우리는 GeForce RTX 5060 데스크톱 GPU를 출하했습니다.</td></tr>
<tr><td>It brings double the performance along with advanced ray tracing, neural rendering and AI-powered DLSS 4 gameplay to millions of gamers worldwide. Blackwell is coming to GeForce NOW in September. This is GeForce NOW's most significant upgrade, offering RTX 5080 cost performance, minimal latency and 5K resolution at 120 frames per second. We are also doubling the GeForce NOW catalog to over 4,500 titles, the largest library of any cloud gaming service. For AI enthusiasts, on-device AI performs the best RTX GPUs. We partnered with OpenAI to optimize their open source GPT models for high-quality, fast and efficient inference on millions of RTX-enabled Window devices.</td><td>이는 전 세계 수백만 게이머들에게 고급 레이 트레이싱, 뉴럴 렌더링, AI 기반 DLSS 4 게임플레이와 함께 두 배의 성능을 제공합니다. Blackwell은 9월에 GeForce NOW에 출시될 예정입니다. 이는 GeForce NOW의 가장 중요한 업그레이드로, RTX 5080 비용 대비 성능, 최소 지연시간, 그리고 초당 120프레임의 5K 해상도를 제공합니다. 또한 GeForce NOW 카탈로그를 4,500개 이상의 타이틀로 두 배 확장하여, 모든 클라우드 게이밍 서비스 중 가장 큰 라이브러리를 구축하고 있습니다. AI 애호가들을 위해서는, 온디바이스 AI가 최고의 RTX GPU에서 최적의 성능을 발휘합니다. 우리는 OpenAI와 파트너십을 맺고 수백만 대의 RTX 지원 Windows 디바이스에서 고품질의 빠르고 효율적인 추론을 위해 그들의 오픈소스 GPT 모델을 최적화했습니다.</td></tr>
<tr><td>With the RTX platform stack, Window developers can create AI applications designed to run on the world's largest AI PC user base. Professional Visualization revenue reached $601 million, a 32% year-on-year increase. Growth was driven by an adoption of the high- end RTX Workstation GPUs and AI-powered workload like design, simulation and prototyping. Key customers are leveraging our solutions to accelerate their operations. Activision Blizzard uses RTX workstations to enhance creative workflows. While robotics innovator Figure AI powers its humanoid robots with RTX-embedded GPUs.</td><td>RTX 플랫폼 스택을 통해 Windows 개발자들은 전 세계 최대 규모의 AI PC 사용자 기반에서 실행되도록 설계된 AI 애플리케이션을 개발할 수 있습니다. 전문 시각화(Professional Visualization) 매출은 6억 100만 달러를 기록하여 전년 동기 대비 32% 증가했습니다. 이러한 성장은 고급형 RTX 워크스테이션 GPU의 채택과 설계, 시뮬레이션, 프로토타이핑과 같은 AI 기반 워크로드에 의해 견인되었습니다. 주요 고객들이 운영을 가속화하기 위해 당사의 솔루션을 활용하고 있습니다. 액티비전 블리자드는 창작 워크플로우를 향상시키기 위해 RTX 워크스테이션을 사용하고 있으며, 로보틱스 혁신 기업인 Figure AI는 RTX 임베디드 GPU로 휴머노이드 로봇을 구동하고 있습니다.</td></tr>
<tr><td>Automotive revenue, which includes only in-car compute revenue, was $586 million, up 69% year-on-year, primarily driven by self- driving solutions. We have begun shipments of NVIDIA Thor SoC, the successor to Orin. Thor's arrival coincides with the industry's accelerating shift to vision language model architecture, generative AI and higher levels of autonomy. Thor is the most successful robotics and AV computer we've ever created. Thor will power. Our full stack Drive AV software platform is now in production, opening up billions to new revenue opportunities for NVIDIA while improving vehicle safety and autonomy. Now moving to the rest of our P&L.</td><td>자동차 매출은 차량 내 컴퓨팅 매출만을 포함하여 5억 8,600만 달러를 기록했으며, 이는 주로 자율주행 솔루션에 힘입어 전년 동기 대비 69% 증가한 수치입니다. 저희는 Orin의 후속 제품인 NVIDIA Thor SoC의 출하를 시작했습니다. Thor의 출시는 업계가 비전 언어 모델 아키텍처, 생성형 AI, 그리고 더 높은 수준의 자율성으로 가속화되는 전환과 때를 같이 합니다. Thor는 저희가 지금까지 개발한 로보틱스 및 자율주행차 컴퓨터 중 가장 성공적인 제품입니다. Thor가 구동할 예정입니다. 저희의 풀스택 Drive AV 소프트웨어 플랫폼이 현재 생산 단계에 있으며, 이는 차량 안전성과 자율성을 개선하는 동시에 NVIDIA에게 수십억 달러의 새로운 매출 기회를 열어주고 있습니다. 이제 손익계산서의 나머지 부분으로 넘어가겠습니다.</td></tr>
<tr><td>GAAP gross margin was 72.4% and non-GAAP gross margin was 72.7%. These figures include a $180 million or 40 basis point benefit from releasing previously reserved H20 inventory. Excluding this benefit, non-GAAP gross margins would have been 72.3%, still exceeding our outlook. GAAP operating expenses rose 8% and 6% on a non-GAAP basis sequentially. This increase was driven by higher compute and infrastructure costs as well as higher compensation and benefit costs. To support the ramp of Blackwell and Blackwell Ultra, inventory increased sequentially from $11 billion to $15 billion in Q2.</td><td>GAAP 총이익률은 72.4%였고 non-GAAP 총이익률은 72.7%였습니다. 이 수치에는 이전에 충당했던 H20 재고를 해제하면서 발생한 1억 8천만 달러 또는 40bp의 이익이 포함되어 있습니다. 이 이익을 제외하면 non-GAAP 총이익률은 72.3%로, 여전히 당사의 전망치를 상회하는 수준입니다. GAAP 영업비용은 전분기 대비 8% 증가했고, non-GAAP 기준으로는 6% 증가했습니다. 이러한 증가는 컴퓨팅 및 인프라 비용 상승과 보상 및 복리후생비 증가에 기인합니다. Blackwell과 Blackwell Ultra의 생산 확대를 지원하기 위해 재고는 2분기에 110억 달러에서 150억 달러로 전분기 대비 증가했습니다.</td></tr>
<tr><td>While we prioritize funding our growth and strategic initiatives, in Q2, we returned $10 billion to shareholders through share repurchases and cash dividends. Our Board of Directors recently approved a $60 billion share repurchase authorization to add to our remaining $14.7 billion of authorization at the end of Q2. Okay. Let me turn it to the outlook for the third quarter. Total revenue is expected to be $54 billion, plus or minus 2%. This represents over $7 billion in sequential growth. Again, we do not assume any H20 shipments to China customers in our outlook. GAAP and non- GAAP gross margins are expected to be 73.3%, 73.5%, respectively, plus or minus 50 basis points.</td><td>성장과 전략적 이니셔티브 자금 조달을 우선시하는 가운데, 2분기에는 자사주 매입과 현금배당을 통해 주주들에게 100억 달러를 환원했습니다. 이사회는 최근 600억 달러 규모의 자사주 매입 승인을 결정했으며, 이는 2분기 말 기준 잔여 승인 한도 147억 달러에 추가되는 것입니다. <br><br>이제 3분기 전망에 대해 말씀드리겠습니다. 총 매출은 540억 달러, 오차범위 플러스마이너스 2%로 예상됩니다. 이는 전분기 대비 70억 달러 이상의 성장을 의미합니다. 다시 한번 말씀드리지만, 저희 전망에는 중국 고객에 대한 H20 출하는 전혀 가정하지 않았습니다. GAAP 및 non-GAAP 매출총이익률은 각각 73.3%, 73.5%로 예상되며, 오차범위는 플러스마이너스 50bp입니다.</td></tr>
<tr><td>We continue to expect to exit the year with non-GAAP gross margins in the mid-70s. GAAP and non-GAAP operating expenses are expected to be approximately $5.9 billion and $4.2 billion, respectively. For the full year, we expect operating expenses to grow in the high 30s range year-over-year, up from our prior expectations of the mid-30s. We are accelerating investments in the business to address the magnitude of growth opportunities that lie ahead. GAAP and non-GAAP other income and expenses are expected to be an income of approximately $500 million, excluding gains and losses from nonmarketable and public held equity securities.</td><td>연말 기준으로 non-GAAP 매출총이익률은 70% 중반대를 유지할 것으로 계속 예상하고 있습니다. GAAP 및 non-GAAP 영업비용은 각각 약 59억 달러와 42억 달러로 예상됩니다. 연간 기준으로 영업비용은 전년 대비 30% 후반대 성장할 것으로 예상하며, 이는 기존 예상치인 30% 중반대에서 상향 조정된 것입니다. 앞으로 다가올 대규모 성장 기회에 대응하기 위해 사업 투자를 가속화하고 있습니다. GAAP 및 non-GAAP 기타 수익 및 비용은 비상장 및 공개 지분증권의 손익을 제외하고 약 5억 달러의 수익으로 예상됩니다.</td></tr>
<tr><td>GAAP and non-GAAP tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items. Further financial data are included in the CFO commentary and other information available on our website. In closing, let me highlight upcoming events for the financial community. We will be at the Goldman Sachs Technology Conference on September 8 in San Francisco. Our annual NDR will commence the first part of October. GTC data center begins on October 27, with Jensen's keynote scheduled for the 28. We look forward to seeing you at these events. Our earnings call to discuss the results of our third quarter of fiscal 2026 is scheduled for November 19.</td><td>GAAP 및 non-GAAP 세율은 개별 항목을 제외하고 16.5% 플러스마이너스 1%로 예상됩니다. 추가적인 재무 데이터는 CFO 코멘터리와 당사 웹사이트에서 확인할 수 있는 기타 정보에 포함되어 있습니다. <br><br>마지막으로 금융 커뮤니티를 위한 향후 일정을 안내드리겠습니다. 9월 8일 샌프란시스코에서 열리는 골드만삭스 테크놀로지 컨퍼런스에 참석할 예정입니다. 연례 NDR은 10월 첫째 주에 시작됩니다. GTC 데이터센터는 10월 27일에 시작되며, 젠슨의 기조연설은 28일로 예정되어 있습니다. 이러한 행사에서 여러분을 만나뵐 수 있기를 기대합니다. 2026 회계연도 3분기 실적을 논의하는 어닝콜은 11월 19일로 예정되어 있습니다.</td></tr>
<tr><td>We will now open the call for questions. Operator, would you please poll for questions?</td><td>이제 질의응답 시간을 시작하겠습니다. 교환원님, 질문을 받아주시기 바랍니다.</td></tr>
    </table>
    <h3>📌 요약</h3>
    <p style="background:#f0f0f0; padding:15px; border-left: 5px solid #333;">• 실적 하이라이트:<br>- 2분기 매출 466억 달러 기록, 데이터센터 부문 전년 대비 56% 성장<br>- 블랙웰(Blackwell) 플랫폼 매출 전분기 대비 17% 증가<br>- 게이밍 부문 매출 43억 달러로 사상 최대 기록<br><br>• 중국 관련 주요 내용:<br>- H20 제품의 중국 수출 라이센스 검토 중이나 3분기 가이던스에는 미포함<br>- 지정학적 이슈 해소시 3분기 H20 매출 20-50억 달러 예상<br>- 중국 매출 비중 데이터센터 부문의 한 자릿수 퍼센트로 감소<br><br>• 향후 전망:<br>- 3분기 매출 가이던스 540억 달러 (±2%)<br>- 연말까지 Non-GAAP 매출총이익률 mid-70% 대 예상<br>- AI 인프라 투자 2030년까지 3-4조 달러 규모로 성장 전망<br><br>• 운영 하이라이트:<br>- GB300 양산 개시, 주당 1,000대 생산 중<br>- 네트워킹 부문 매출 73억 달러로 신기록<br>- 600억 달러 규모의 자사주 매입 프로그램 승인</p>
    <hr style="margin:50px 0;">
    

    <h2>❓ Q&A</h2>
    <table style="width:100%; border-collapse:collapse; margin-bottom: 40px;">
        <tr>
            <th style="width:50%; border-bottom: 2px solid #333;">Original</th>
            <th style="width:50%; border-bottom: 2px solid #333;">Translation</th>
        </tr>
        <tr><td>Operator: [Operator Instructions] Your first question comes from CJ Muse with Cantor Fitzgerald.</td><td>**Operator:** [운영자 안내] 첫 번째 질문은 Cantor Fitzgerald의 CJ Muse님께서 주셨습니다.</td></tr>
<tr><td>Christopher James Muse: I guess with wafer in to rack out lead times of 12 months, you confirmed on the call today that Rubin is on track for ramp in the second half. And obviously, many of these investments are multiyear projects contingent upon power, cooling, et cetera. I was hoping perhaps could you take a high-level view and speak to your vision for growth into 2026. And as part of that, if you can kind of comment between network and data center would be very helpful.</td><td>**Christopher James Muse:** 웨이퍼 투입부터 랙 출하까지의 리드타임이 12개월인 상황에서, 오늘 콜에서 Rubin이 하반기 램프업 계획대로 진행되고 있다고 확인해 주셨습니다. 그리고 분명히 이러한 투자들 중 상당수는 전력, 냉각 등에 따라 좌우되는 다년간 프로젝트입니다. 혹시 높은 차원에서 2026년까지의 성장 비전에 대해 말씀해 주실 수 있을까요? 그리고 그 일환으로 네트워크와 데이터센터 간의 구분에 대해서도 코멘트해 주시면 매우 도움이 될 것 같습니다.</td></tr>
<tr><td>Jen-Hsun Huang: Yes. Thanks, CJ. At the highest level of growth drivers would be the evolution, the introduction, if you will, of reasoning agentic AI. Where chatbots used to be one shot, you give it a prompt and it would generate the answer, now the AI does research. It thinks and does a plan, and it might use tools. And so it's called long thinking; and the longer it thinks, oftentimes, it produces better answers. And the amount of computation necessary for 1 shot versus reasoning agentic AI models could be 100x, 1,000x and potentially even more as the amount of research and basically reading and comprehension that it goes off to do. And so the amount of computation that has resulted in agentic AI has grown tremendously. And of course, the effectiveness has also grown tremendously. Because of agentic AI, the amount of hallucination has dropped significantly. You can now use tools and perform tasks. Enterprises have been opened up. As a result of agentic AI and vision language models, we now are seeing a breakthrough in physical AI, in robotics, autonomous systems. So the last year, AI has made tremendous progress and agentic systems, reasoning systems is completely revolutionary. Now we built the Blackwell NVLink 72 system, a rack scale computing system, for this moment. We've been working on it for several years. This last year, we transitioned from NVLink 8, which is a node scale computing, each node is a computer, to now NVLink 72, where each rack is a computer. That disaggregation of NVLink 72 into a rack scale system was extremely hard to do, but the results are extraordinary. We're seeing orders of magnitude speed up and therefore, energy efficiency and therefore, cost effectiveness of token generation because of NVLink 72. And so over the next couple of years, you're going to -- well, you asked about longer term. Over the next 5 years, we're going to scale into with Blackwell, with Rubin and follow-ons to scale into effectively a $3 trillion to $4 trillion AI infrastructure opportunity. The last couple of years, you have seen that CapEx has grown in just the top 4 CSPs by -- has doubled and grown to about $600 billion. So we're in the beginning of this build-out, and the AI technology advances has really enabled AI to be able to adopt and solve problems to many different industries.</td><td>**Jen-Hsun Huang:** 네, 감사합니다, CJ. 가장 높은 수준의 성장 동력은 추론형 에이전틱 AI(reasoning agentic AI)의 진화, 즉 도입이라고 할 수 있습니다. 기존 챗봇은 원샷 방식이었죠. 프롬프트를 주면 답변을 생성하는 식이었는데, 이제 AI가 연구를 합니다. 생각하고 계획을 세우며, 도구들을 사용하기도 하죠. 이를 긴 사고(long thinking)라고 부르는데, 더 오래 생각할수록 더 나은 답변을 만들어내는 경우가 많습니다. 원샷 방식 대비 추론형 에이전틱 AI 모델에 필요한 연산량은 100배, 1,000배, 그리고 AI가 수행하는 연구와 기본적인 읽기, 이해 작업의 양에 따라 잠재적으로 그보다 훨씬 더 많을 수 있습니다. 따라서 에이전틱 AI로 인한 연산량이 엄청나게 증가했습니다. 물론 효과성도 엄청나게 향상되었고요. 에이전틱 AI 덕분에 환각(hallucination) 현상이 크게 줄어들었습니다. 이제 도구를 사용하고 작업을 수행할 수 있게 되었죠. 기업들에게 새로운 기회가 열렸습니다. 에이전틱 AI와 비전 언어 모델의 결과로, 이제 물리적 AI, 로보틱스, 자율 시스템에서 획기적인 발전을 보고 있습니다. 지난 한 해 동안 AI는 엄청난 진전을 이뤘고, 에이전틱 시스템과 추론 시스템은 완전히 혁명적입니다. <br><br>이제 우리는 이 순간을 위해 Blackwell NVLink 72 시스템, 즉 랙 규모의 컴퓨팅 시스템을 구축했습니다. 몇 년간 작업해온 것이죠. 지난해에 우리는 각 노드가 하나의 컴퓨터인 노드 규모 컴퓨팅인 NVLink 8에서, 이제 각 랙이 하나의 컴퓨터인 NVLink 72로 전환했습니다. NVLink 72를 랙 규모 시스템으로 분해하는 것은 극도로 어려운 작업이었지만, 그 결과는 놀라웠습니다. NVLink 72 덕분에 토큰 생성에서 몇 배 수준의 속도 향상을 보고 있고, 따라서 에너지 효율성과 비용 효과성도 크게 개선되었습니다. 앞으로 몇 년간 - 아니, 장기적인 전망에 대해 물어보셨죠. 향후 5년간 Blackwell, Rubin 그리고 후속 제품들과 함께 사실상 3조에서 4조 달러 규모의 AI 인프라 기회로 확장해 나갈 예정입니다. 지난 몇 년간 상위 4개 CSP만 봐도 자본지출(CapEx)이 두 배로 증가해서 약 6천억 달러까지 성장한 것을 보셨을 겁니다. 이제 이러한 구축의 초기 단계에 있으며, AI 기술의 발전으로 인해 AI가 다양한 산업의 문제들을 채택하고 해결할 수 있게 되었습니다.</td></tr>
<tr><td>Operator: Your next question comes from Vivek Arya with Bank of America Securities.</td><td>**Operator:** 다음 질문은 뱅크 오브 아메리카 증권의 비벡 아리야님께서 주신 질문입니다.</td></tr>
<tr><td>Vivek Arya: Colette, I just wanted to clarify the $2 billion to $5 billion in China. What needs to happen? And what is the sustainable pace of that China business as you get into Q4? And then, Jensen, for you on the competitive landscape. Several of your large customers already have or are planning many ASIC projects. I think 1 of your ASIC competitors, Broadcom, signaled that they could grow their AI business almost 55%, 60% next year. Any scenario in which you see the market moving more towards ASICs and away from NVIDIA GPU? Just what are you hearing from your customers? How are they managing this split between the use of merchant silicon and ASICs?</td><td>**Vivek Arya:** 콜레트, 중국에서의 20억 달러에서 50억 달러 규모에 대해 명확히 하고 싶습니다. 무엇이 필요한가요? 그리고 4분기로 접어들면서 중국 사업의 지속 가능한 속도는 어떻게 될까요? <br><br>그리고 젠슨, 경쟁 환경에 대해 질문드리겠습니다. 여러 대형 고객사들이 이미 많은 ASIC 프로젝트를 진행하고 있거나 계획하고 있습니다. ASIC 경쟁사 중 하나인 브로드컴은 내년에 AI 사업을 거의 55%, 60% 성장시킬 수 있다고 시사했습니다. 시장이 NVIDIA GPU에서 벗어나 ASIC 쪽으로 더 많이 이동하는 시나리오가 있을까요? 고객들로부터 어떤 이야기를 듣고 계신가요? 그들이 범용 실리콘과 ASIC 사용 간의 분할을 어떻게 관리하고 있나요?</td></tr>
<tr><td>Colette M. Kress: Thanks, Vivek. So let me first answer your question regarding what will it take for the H20s to be shipped. There is interest in our H20s. There is the initial set of license that we received. And then additionally, we do have supply that we are ready, and that's why we communicated that somewhere in the range of about $2 billion to $5 billion this quarter, we could potentially ship. We're still waiting on several of the geopolitical issues going back and forth between the governments and the companies trying to determine their purchases and what they want to do. So it's still open at this time, and we're not exactly sure what that full amount will be this quarter. However, if more interest arrives, more licenses arrives, again, we can also still build additional H20 and ship more as well.</td><td>**Colette M. Kress:** 비벡, 감사합니다. 먼저 H20 출하를 위해 무엇이 필요한지에 대한 질문부터 답변드리겠습니다. H20에 대한 관심은 있습니다. 초기 라이선스 세트를 받았고, 추가로 준비된 공급량도 보유하고 있어서 이번 분기에 약 20억 달러에서 50억 달러 범위에서 잠재적으로 출하할 수 있다고 말씀드린 것입니다. <br><br>하지만 여전히 정부와 기업들 간의 지정학적 이슈들이 오가고 있어서 기다리고 있는 상황입니다. 기업들이 구매를 결정하고 무엇을 하고 싶어하는지 파악하려고 하고 있죠. 그래서 현재로서는 아직 열려있는 상황이고, 이번 분기 전체 금액이 정확히 얼마가 될지는 확실하지 않습니다. <br><br>하지만 더 많은 관심이 생기고 더 많은 라이선스가 나온다면, 추가로 H20을 더 제조해서 더 많이 출하할 수도 있습니다.</td></tr>
<tr><td>Jen-Hsun Huang: NVIDIA builds very different things in ASICs. So let's talk about ASICs first. A lot of projects are started. Many start-up companies are created. Very few products go into production. And the reason for that is it's really hard. Accelerated computing is unlike general- purpose computing. You don't write software and just compile it into a processor. Accelerated computing is a full-stack co-design problem. And AI factories in the last several years have become so much more complex because of the scale of the problems have grown so significantly. It is really the ultimate, the most extreme computer science problem the world's ever seen obviously. And so the stack is complicated. The models are changing incredibly fast from generative based on auto regressive to degenerative based on diffusion to mixed models to multi-modality. The number of different models that are coming out that are either derivatives of transformers or evolutions of transformers is just daunting. One of the advantages that we have is that NVIDIA is available in every cloud. We're available from every computer company. We're available from the cloud to on-prem to edge to robotics on the same programming model. And so it's sensible that every framework in the world supports NVIDIA. When you're building a new model architecture, releasing it on NVIDIA is most sensible. And so the diversity of our platform, both in the ability to evolve into any architecture, the fact that we're everywhere, and also, we accelerate the entire pipeline, everything from data processing to pretraining to post training with reinforcement learning, all the way out to inference. And so when you build a data center with NVIDIA platform in it, the utility of it is best. The lifetime usefulness is much, much longer. And then I would just say that in addition to all of that -- and it's just a really extremely complex systems problem anymore. People talk about the chip itself. There's one ASIC, the GPU that many people talk about. But in order to build Blackwell the platform and Rubin the platform, we had to build CPUs that connect fast memory, low -- extremely energy-efficient memory for large KB caching necessary for agentic AI to the GPU to a SuperNIC to a scale up switch, we call NVLink, completely revolutionary, we're in our fifth generation now, to a scale out switch, whether it's Quantum or Spectrum-X Ethernet, to now scale across switches so that we could prepare for these AI super factories with multiple gigawatts of computing all connected together. We call that Spectrum-XGS. We just announced that at Hot Chips this week. And so the complications, the complexity of everything that we do is really quite extraordinary. It's just done at a really, really extreme scale now. And then lastly, if I could just say one more thing, we're in every cloud for a good reason. Not only are we the most energy efficient. Our perf per watt is the best of any computing platform. And in a world of power-limited data centers, perf per watt drives directly to revenues. And you've heard me say before that, in a lot of ways, the more you buy, the more you grow. And because our perf per dollar, the performance per dollar is so incredible, you also have extremely great margins. So the growth opportunity with NVIDIA's architecture and the gross margins opportunity with NVIDIA's architecture is absolutely the best. And so there's a lot of reasons why NVIDIA is chosen by every cloud and every start-up and every computer company. We're really a holistic full-stack solution for AI factories.</td><td>**Jen-Hsun Huang:** NVIDIA는 ASIC에서 매우 다른 것들을 구축합니다. 먼저 ASIC에 대해 이야기해보겠습니다. 많은 프로젝트가 시작되고, 수많은 스타트업 회사들이 생겨납니다. 하지만 실제로 제품화까지 가는 경우는 매우 적습니다. 그 이유는 정말 어렵기 때문입니다. 가속 컴퓨팅(accelerated computing)은 범용 컴퓨팅과는 다릅니다. 소프트웨어를 작성해서 프로세서로 컴파일하기만 하면 되는 게 아닙니다. 가속 컴퓨팅은 풀스택 공동 설계 문제입니다. 그리고 지난 몇 년간 AI 팩토리들은 문제의 규모가 엄청나게 커지면서 훨씬 더 복잡해졌습니다. 이는 정말로 세상이 지금까지 본 것 중 가장 극한의, 가장 극단적인 컴퓨터 사이언스 문제입니다. 그래서 스택이 복잡한 것입니다. 모델들이 자동 회귀 기반의 생성형에서 확산 기반의 탈생성형으로, 그리고 혼합 모델과 멀티모달리티로 엄청나게 빠르게 변화하고 있습니다. 트랜스포머의 파생형이나 진화형인 다양한 모델들이 쏟아져 나오는 수가 정말 압도적입니다. 저희가 가진 장점 중 하나는 NVIDIA가 모든 클라우드에서 이용 가능하다는 것입니다. 모든 컴퓨터 회사에서 저희를 이용할 수 있고요. 클라우드부터 온프레미스, 엣지, 로보틱스까지 동일한 프로그래밍 모델로 이용 가능합니다. 그래서 전 세계 모든 프레임워크가 NVIDIA를 지원하는 것이 당연합니다. 새로운 모델 아키텍처를 구축할 때 NVIDIA에서 출시하는 것이 가장 합리적입니다. 저희 플랫폼의 다양성은 어떤 아키텍처로든 진화할 수 있는 능력과 어디에나 존재한다는 점, 그리고 데이터 처리부터 사전 훈련, 강화 학습을 통한 후속 훈련, 추론까지 전체 파이프라인을 가속화한다는 점에 있습니다. 따라서 NVIDIA 플랫폼이 탑재된 데이터센터를 구축하면 그 활용도가 최고가 됩니다. 수명 동안의 유용성이 훨씬 더 길어지죠. 그리고 이 모든 것에 더해서 말씀드리고 싶은 것은 - 이제는 정말 극도로 복잡한 시스템 문제라는 점입니다. 사람들은 칩 자체에 대해 이야기합니다. 많은 사람들이 이야기하는 하나의 ASIC인 GPU가 있죠. 하지만 Blackwell 플랫폼과 Rubin 플랫폼을 구축하기 위해서는 빠른 메모리, 저전력이면서 극도로 에너지 효율적인 메모리를 연결하는 CPU를 구축해야 했습니다. 이는 에이전트 AI(agentic AI)에 필요한 대용량 KB 캐싱을 위한 것으로, GPU에서 SuperNIC으로, 그리고 우리가 NVLink라고 부르는 스케일 업 스위치로 연결됩니다. 이는 완전히 혁신적인 기술로 현재 5세대에 이르렀습니다. 그리고 Quantum이든 Spectrum-X 이더넷이든 스케일 아웃 스위치로 연결되어, 이제는 스위치 간 확장이 가능해져서 수 기가와트의 컴퓨팅이 모두 연결된 AI 슈퍼 팩토리를 준비할 수 있게 되었습니다. 이를 Spectrum-XGS라고 부르는데, 이번 주 Hot Chips에서 발표했습니다. 그래서 우리가 하는 모든 일의 복잡성과 난이도는 정말 대단합니다. 다만 이제는 정말, 정말 극한의 규모로 진행되고 있습니다. 마지막으로 한 가지만 더 말씀드리자면, 저희가 모든 클라우드에 있는 데는 충분한 이유가 있습니다. 저희가 가장 에너지 효율적일 뿐만 아니라, 와트당 성능(perf per watt)이 어떤 컴퓨팅 플랫폼보다도 최고입니다. 전력 제약이 있는 데이터센터 환경에서 와트당 성능은 직접적으로 매출로 이어집니다. 제가 전에도 말씀드렸듯이, 많은 면에서 더 많이 구매할수록 더 많이 성장하게 됩니다. 그리고 달러당 성능, 즉 가격 대비 성능이 정말 놀라울 정도로 뛰어나기 때문에 마진도 매우 훌륭합니다. 따라서 NVIDIA 아키텍처를 통한 성장 기회와 총마진 기회는 절대적으로 최고입니다. 이런 여러 이유들 때문에 모든 클라우드 업체와 모든 스타트업, 모든 컴퓨터 회사들이 NVIDIA를 선택하는 것입니다. 저희는 정말로 AI 팩토리를 위한 전체적이고 완전한 풀스택 솔루션을 제공하고 있습니다.</td></tr>
<tr><td>Operator: Your next question comes from Ben Reitzes with Melius.</td><td>**Operator:** 다음 질문은 Melius의 Ben Reitzes님께서 주신 질문입니다.</td></tr>
<tr><td>Benjamin Alexander Reitzes: Jensen, I wanted to ask you about your $3 trillion to $4 trillion in data center infrastructure spend by the end of the decade. Previously, you talked about something in the $1 billion range, which I believe was just for compute by 2028. If you take past comments, $3 trillion to $4 trillion would imply maybe $2 billion plus in compute spend. And just wanted to know if that was right and that's what you're seeing by the end of the decade. And wondering what you think your share will be of that. Your share right now of total infrastructure compute-wise is very high, so I wanted to see. And also if there's any bottlenecks you're concerned about like power to get to the $3 trillion to $4 trillion.</td><td>**Benjamin Alexander Reitzes:** 데이터센터 인프라에 대한 10년 말까지의 3조~4조 달러 투자 전망에 대해 질문드리고 싶습니다. 이전에 2028년까지 컴퓨팅 부문만으로 약 1조 달러 규모라고 말씀하신 것으로 기억합니다. 과거 발언들을 종합해보면, 3조~4조 달러라면 컴퓨팅 투자만으로도 2조 달러 이상을 의미하는 것 같은데요. 이것이 맞는지, 그리고 실제로 10년 말까지 그런 규모를 보고 계신지 궁금합니다. <br><br>또한 그 중에서 귀사의 점유율이 어느 정도가 될 것으로 보시는지도 알고 싶습니다. 현재 전체 인프라에서 컴퓨팅 부문의 점유율이 매우 높은 상황이니까요. 그리고 3조~4조 달러 규모에 도달하는 데 전력 공급 같은 병목 현상에 대한 우려는 없으신지도 궁금합니다.</td></tr>
<tr><td>Jen-Hsun Huang: Thanks. As you know, the CapEx of just the top 4 hyperscalers has doubled in 2 years. As the AI revolution went into full steam, as the AI race is now on, the CapEx spend has doubled to $600 billion per year. There's 5 years between now and the end of the decade, and $600 billion only represents the top 4 hyperscalers. We still have the rest of the enterprise companies building on-prem. You have cloud service providers building around the world. United States represents about 60% of the world's compute. And over time, you would think that artificial intelligence would reflect GDP scale and growth and so -- and would be, of course, accelerating GDP growth. And so our contribution to that is a large part of the AI infrastructure. Out of a gigawatt AI factory, which can go anywhere from $50 billion to plus or minus 10%, let's say, $50 billion to $60 billion, we represent about $35 billion plus or minus of that and $35 billion out of $50 billion per gigawatt data center. And of course, what you get for that is not a GPU. I think people -- we're famous for building the GPU and inventing the GPU, but as you know, over the last decade, we've really transitioned to become an AI infrastructure company. It takes 6 chips just to build -- 6 different types of chips just to build a Rubin AI supercomputer. And just to scale that out to a gigawatt, you have hundreds of thousands of GPU compute nodes and a whole bunch of racks. And so we're really an AI infrastructure company, and we're hoping to continue to contribute to growing this industry, making AI more useful and then very importantly, driving the performance per watt because the world, as you mentioned, limiters, it will always likely be power limitations or AI -- building limitations. And so we need to squeeze as much out of that factory as possible. NVIDIA's performance per unit of energy used drives the revenue growth of that factory. It directly translates. If you have a 100- megawatt factory, perf per 100 megawatt drives your revenues. It's tokens per 100 megawatts of factory. In our case also, the performance per dollar spent is so high that your gross margins are also the best. But anyhow, these are the limiters going forward and $3 trillion to $4 trillion is fairly sensible for the next 5 years.</td><td>**Jen-Hsun Huang:** 감사합니다. 아시다시피 상위 4개 하이퍼스케일러들의 자본지출(CapEx)이 2년 만에 두 배로 증가했습니다. AI 혁명이 본격화되고 AI 경쟁이 시작되면서 자본지출이 연간 6천억 달러로 두 배가 되었습니다. 지금부터 2030년 말까지 5년이 남았고, 6천억 달러는 상위 4개 하이퍼스케일러만을 나타낸 것입니다. 온프레미스를 구축하는 나머지 기업들도 여전히 있습니다. 전 세계적으로 클라우드 서비스 제공업체들이 구축되고 있습니다. 미국이 전 세계 컴퓨팅의 약 60%를 차지하고 있습니다. 시간이 지나면서 인공지능이 GDP 규모와 성장을 반영하게 될 것이고, 물론 GDP 성장을 가속화할 것으로 생각됩니다. 그리고 우리는 그 AI 인프라의 상당 부분에 기여하고 있습니다. 1기가와트 AI 팩토리에서, 대략 500억 달러에서 플러스마이너스 10% 정도, 그러니까 500억에서 600억 달러 정도 되는데, 저희는 그 중에서 약 350억 달러 플러스마이너스 정도를 차지합니다. 1기가와트 데이터센터당 500억 달러 중에서 350억 달러 정도죠. 물론 그렇게 해서 얻는 것은 단순한 GPU가 아닙니다. 사람들이 생각하기로는 - 저희가 GPU를 만들고 GPU를 발명한 것으로 유명하지만, 아시다시피 지난 10년간 저희는 실제로 AI 인프라 회사로 전환했습니다. 루빈(Rubin) AI 슈퍼컴퓨터 하나를 구축하는 데만 해도 6개의 칩이 필요합니다. 6가지 다른 종류의 칩이 필요하죠. 그리고 이것을 1기가와트 규모로 확장하려면 수십만 개의 GPU 컴퓨트 노드와 엄청난 수의 랙이 필요합니다. 저희는 실제로 AI 인프라 회사이고, 이 산업의 성장에 계속 기여하면서 AI를 더욱 유용하게 만들고, 그리고 매우 중요하게는 와트당 성능을 향상시키기를 희망합니다. 왜냐하면 말씀하신 대로 세상에는 항상 전력 제약이나 AI 구축 제약이 있을 가능성이 높기 때문입니다. 그래서 우리는 그 팩토리에서 가능한 한 최대한 많은 것을 뽑아내야 합니다. NVIDIA의 사용 에너지 단위당 성능이 그 팩토리의 매출 성장을 견인합니다. 이는 직접적으로 연결됩니다. 100메가와트 팩토리가 있다면, 100메가와트당 성능이 매출을 견인합니다. 팩토리 100메가와트당 토큰 수인 셈이죠. 저희의 경우에도 지출 달러당 성능이 너무 높아서 총마진도 최고 수준입니다. 어쨌든 이런 것들이 앞으로의 제약 요인들이고, 향후 5년간 3조 달러에서 4조 달러 정도가 상당히 합리적인 수준이라고 봅니다.</td></tr>
<tr><td>Operator: Next question comes from Joe Moore of Morgan Stanley.</td><td>**Operator:** 다음 질문은 모건스탠리의 조 무어님께서 주셨습니다.</td></tr>
<tr><td>Joseph Lawrence Moore: Great. Congratulations on reopening the China opportunity. Can you talk about the long-term prospects there? You've talked about, I think, half of AI software world being there. How much can NVIDIA grow in that business? And how important is it that you get the Blackwell architecture ultimately licensed there?</td><td>**Joseph Lawrence Moore:** 훌륭합니다. 중국 기회 재개를 축하드립니다. 그곳의 장기적 전망에 대해 말씀해 주실 수 있나요? AI 소프트웨어 세계의 절반이 그곳에 있다고 말씀하신 것 같은데요. NVIDIA가 그 사업에서 얼마나 성장할 수 있을까요? 그리고 궁극적으로 Blackwell 아키텍처를 그곳에 라이선스하는 것이 얼마나 중요한가요?</td></tr>
<tr><td>Jen-Hsun Huang: The China market, I've estimated to be about $50 billion of opportunity for us this year if we were able to address it with competitive products. And if it's $50 billion this year, you would expect it to grow, say, 50% per year. As the rest of the world's AI market is growing as well. It is the second largest computing market in the world, and it is also the home of AI researchers. About 50% of the world's AI researchers are in China. The vast majority of the leading open source models are created in China. And so it's fairly important, I think, for the American technology companies to be able to address that market. And open source, as you know, is created in one country, but it's used all over the world. The open source models that have come out of China are really excellent. DeepSeek, of course, gained global notoriety. Qwen is excellent. Kimi's excellent. There's a whole bunch of new models that are coming out. They're multimodal. They're great language models. And it's really fueled the adoption of AI in enterprises around the world because enterprises want to build their own custom proprietary software stacks. And so open source model's really important for enterprise. It's really important for SaaS who also would like to build proprietary systems. It has been really incredible for robotics around the world. And so open source is really important, and it's important that the American companies are able to address it. This is -- it's going to be a very large market. We're talking to the administration about the importance of American companies to be able to address the Chinese market. And as you know, H20 has been approved for companies that are not on the entities list, and many licenses have been approved. And so I think the opportunity for us to bring Blackwell to the China market is a real possibility. And so we just have to keep advocating the sensibility of and the importance of American tech companies to be able to lead and win the AI race and help make the American tech stack the global standard.</td><td>**Jen-Hsun Huang:** 중국 시장은 올해 우리가 경쟁력 있는 제품으로 대응할 수 있다면 약 500억 달러 규모의 기회가 될 것으로 추정하고 있습니다. 그리고 올해 500억 달러라면, 세계 다른 지역의 AI 시장도 성장하고 있는 것처럼 연간 50% 정도 성장할 것으로 예상됩니다. 중국은 세계에서 두 번째로 큰 컴퓨팅 시장이며, AI 연구자들의 본거지이기도 합니다. 전 세계 AI 연구자의 약 50%가 중국에 있습니다. 주요 오픈소스 모델의 대부분이 중국에서 만들어지고 있습니다. 따라서 미국 기술 기업들이 그 시장에 대응할 수 있는 것이 상당히 중요하다고 생각합니다. 아시다시피 오픈소스는 한 나라에서 만들어지지만 전 세계에서 사용됩니다. 중국에서 나온 오픈소스 모델들은 정말 뛰어납니다. DeepSeek은 물론 전 세계적으로 주목받았죠. Qwen도 훌륭하고, Kimi도 뛰어납니다. 새로 출시되는 모델들이 정말 많아요. 멀티모달이고 훌륭한 언어 모델들이죠. 그리고 이것이 전 세계 기업들의 AI 도입을 정말 가속화시켰습니다. 기업들이 자체적인 맞춤형 독점 소프트웨어 스택을 구축하고 싶어하기 때문이죠. 그래서 오픈소스 모델이 기업에게 정말 중요합니다. 독점 시스템을 구축하려는 SaaS 기업들에게도 정말 중요하고요. 전 세계 로보틱스 분야에서도 정말 놀라운 성과를 보이고 있습니다. 그래서 오픈소스가 정말 중요하고, 미국 기업들이 이를 다룰 수 있다는 것이 중요합니다. 이건 정말 거대한 시장이 될 것입니다. 우리는 행정부와 미국 기업들이 중국 시장에 진출할 수 있는 것의 중요성에 대해 논의하고 있습니다. 아시다시피 H20은 제재 대상 목록에 없는 기업들에 대해 승인되었고, 많은 라이선스들이 승인되었습니다. 따라서 우리가 블랙웰을 중국 시장에 출시할 수 있는 기회는 충분히 실현 가능하다고 생각합니다. 그래서 우리는 미국 기술 기업들이 AI 경쟁에서 선도하고 승리할 수 있도록 하는 것의 합리성과 중요성, 그리고 미국 기술 스택을 글로벌 표준으로 만드는 데 도움이 되도록 하는 것에 대해 계속 옹호해야 합니다.</td></tr>
<tr><td>Operator: Your next question comes from the line of Aaron Rakers with Wells Fargo.</td><td>**Operator:** 다음 질문은 Wells Fargo의 Aaron Rakers님께서 주신 질문입니다.</td></tr>
<tr><td>Aaron Christopher Rakers: Yes. Thank you for the question. I want to go back to the Spectrum-XGS announcement this week and thinking about the Ethernet product now pushing over $10 billion of annualized revenue. Jensen, how -- what is the opportunity set that you see for Spectrum- XGS? Do we think about this as kind of the data center interconnect layer? Any thoughts on the sizing of this opportunity within that Ethernet portfolio?</td><td>**Aaron Christopher Rakers:** 네, 질문해 주셔서 감사합니다. 이번 주 Spectrum-XGS 발표로 다시 돌아가서, 이더넷 제품이 이제 연간 매출 100억 달러를 넘어서고 있는 상황에서 말씀드리겠습니다. 젠슨, Spectrum-XGS의 기회 영역을 어떻게 보고 계신지요? 이를 데이터센터 상호연결 계층으로 생각해야 할까요? 이더넷 포트폴리오 내에서 이 기회의 규모에 대한 견해가 있으시다면 말씀해 주세요.</td></tr>
<tr><td>Jen-Hsun Huang: We now offer 3 networking technologies. One is for scale up. One is for scale out and one for scale across. Scale up is so that we could build the largest possible virtual GPU, the virtual compute node. NVLink is revolutionary. NVLink 72 is what made it possible for Blackwell to deliver such an extraordinary generational jump over Hopper's NVLink 8. At a time when we have long thinking models, agentic AI reasoning systems, the NVLink basically amplifies the memory bandwidth, which is really critical for reasoning systems. And so NVLink 72 is fantastic. We then scale out with networking, which we have 2. We have InfiniBand, which is unquestionably the lowest latency, the lowest jitter, the best scale-out network. It does require more expertise in managing those networks. And for supercomputing, for the leading model makers, InfiniBand, Quantum InfiniBand is the unambiguous choice. If you were to benchmark an AI factory, the ones with InfiniBand are the best performance. For those who would like to use Ethernet because their whole data center is built with Ethernet, we have a new type of Ethernet called Spectrum Ethernet. Spectrum Ethernet is not off the shelf. It has a whole bunch of new technologies designed for low latency and low jitter and congestion control. And it has the ability to come closer, much, much closer to InfiniBand than anything that's out there. And that is -- we call that Spectrum-X Ethernet. And then finally, we have Spectrum-XGS, a giga scale for connecting multiple data centers, multiple AI factories into a super factory, a gigantic system. And we're going to -- you're going to see that networking obviously is very important in AI factories. In fact, choosing the right networking, the performance, the throughput improvement, going from 65% to 85% or 90%, that kind of step-up because of your networking capability effectively makes networking free. Choosing the right networking, you're basically paying -- you'll get a return on it like you can't believe because the AI factory, a gigawatt, as I mentioned before, could be $50 billion. And so the ability to improve the efficiency of that factory by tens of percent is -- results in $10 billion, $20 billion worth of effective benefit. And so this -- the networking is a very important part of it. It's the reason why NVIDIA dedicates so much in networking. That's the reason why we purchased Mellanox 5.5 years ago. And Spectrum-X, as we mentioned earlier, is now quite a sizable business, and it's only about 1.5 years old. So Spectrum-X is a home run. All 3 of them are going to be fantastic. NVLink scale up, Spectrum-X and InfiniBand scale out, and then Spectrum-XGS for scale across.</td><td>**Jen-Hsun Huang:** 저희는 이제 3가지 네트워킹 기술을 제공합니다. 하나는 스케일 업용이고, 하나는 스케일 아웃용, 그리고 하나는 스케일 어크로스용입니다. 스케일 업은 가능한 한 가장 큰 가상 GPU, 즉 가상 컴퓨트 노드를 구축할 수 있도록 하는 것입니다. NVLink는 혁신적입니다. NVLink 72가 있었기에 Blackwell이 Hopper의 NVLink 8 대비 이렇게 놀라운 세대적 도약을 달성할 수 있었습니다. 장기 사고 모델과 에이전틱 AI 추론 시스템이 있는 시대에, NVLink는 기본적으로 메모리 대역폭을 증폭시키는데, 이는 추론 시스템에 정말 중요합니다. 그래서 NVLink 72는 정말 훌륭합니다. 그 다음으로 네트워킹을 통한 스케일 아웃이 있는데, 저희는 2가지를 보유하고 있습니다. InfiniBand가 있는데, 이는 의심할 여지없이 가장 낮은 지연시간, 가장 낮은 지터, 최고의 스케일 아웃 네트워크입니다. 다만 이런 네트워크를 관리하는 데는 더 많은 전문성이 필요합니다. 슈퍼컴퓨팅에서 선도적인 모델 제작업체들에게는 InfiniBand, 즉 Quantum InfiniBand가 확실한 선택입니다. AI 팩토리를 벤치마킹해보면 InfiniBand를 사용하는 곳들이 최고의 성능을 보여줍니다. 데이터센터 전체가 이더넷으로 구축되어 있어서 이더넷을 사용하고 싶어하는 분들을 위해서는 Spectrum Ethernet이라는 새로운 유형의 이더넷을 제공합니다. Spectrum Ethernet은 기성품이 아닙니다. 저지연, 낮은 지터, 혼잡 제어를 위해 설계된 완전히 새로운 기술들이 집약되어 있습니다. 그리고 기존 어떤 제품보다도 InfiniBand에 훨씬, 훨씬 더 가까운 성능을 낼 수 있는 능력을 갖추고 있습니다. 이것을 저희는 Spectrum-X Ethernet이라고 부릅니다. 그리고 마지막으로 Spectrum-XGS가 있는데, 이는 여러 데이터센터와 여러 AI 팩토리를 슈퍼 팩토리, 즉 거대한 시스템으로 연결하는 기가 스케일 솔루션입니다. 그리고 보시게 될 것은 네트워킹이 AI 팩토리에서 매우 중요하다는 점입니다. 실제로 올바른 네트워킹을 선택하는 것, 성능과 처리량 개선을 통해 65%에서 85% 또는 90%로 향상시키는 것, 이런 네트워킹 역량으로 인한 성능 향상은 사실상 네트워킹을 무료로 만들어줍니다. 올바른 네트워킹을 선택하면 기본적으로 믿을 수 없을 정도의 수익을 얻게 됩니다. 왜냐하면 앞서 말씀드린 것처럼 1기가와트 AI 팩토리는 500억 달러 규모가 될 수 있기 때문입니다. 그래서 그 팩토리의 효율성을 수십 퍼센트 개선할 수 있는 능력은 100억 달러, 200억 달러 상당의 실질적인 이익을 가져다줍니다. 그래서 이 네트워킹이 매우 중요한 부분입니다. 이것이 NVIDIA가 네트워킹에 그렇게 많은 투자를 하는 이유입니다. 이것이 우리가 5년 반 전에 Mellanox를 인수한 이유이기도 합니다. 그리고 앞서 언급했듯이 Spectrum-X는 이제 상당한 규모의 사업이 되었는데, 출시된 지 겨우 1년 반밖에 되지 않았습니다. 그래서 Spectrum-X는 홈런입니다. 이 세 가지 모두 환상적일 것입니다. 스케일 업을 위한 NVLink, 스케일 아웃을 위한 Spectrum-X와 InfiniBand, 그리고 전체 확장을 위한 Spectrum-XGS까지 말이죠.</td></tr>
<tr><td>Operator: Your next question comes from Stacy Rasgon with Bernstein Research.</td><td>**Operator:** 다음 질문은 번스타인 리서치의 스테이시 라스곤님께서 주신 질문입니다.</td></tr>
<tr><td>Stacy Aaron Rasgon: I have a more tactical question for Colette. So on the guidance, we're up over $7 billion. The vast bulk of that is going to be from data center. How do I think about apportioning that $7 billion out across Blackwell versus Hopper versus networking? I mean it looks like Blackwell was probably $27 billion in the quarter, up from maybe $23 billion last quarter. Hopper is still $6 billion or $7 billion post the H20. Like do you think the Hopper strength continues? Just how do I think about parsing that $7 billion out across those 3 different components?</td><td>**Stacy Aaron Rasgon:** 가이던스에서 70억 달러 이상 증가한 부분에 대해 좀 더 구체적으로 질문드리겠습니다. 이 증가분의 대부분이 데이터센터에서 나올 것으로 보이는데, 이 70억 달러를 블랙웰 대 호퍼 대 네트워킹으로 어떻게 배분해서 생각해야 할까요? <br><br>블랙웰은 이번 분기에 아마 270억 달러 정도로, 지난 분기 230억 달러에서 증가한 것 같습니다. 호퍼는 H20 이후에도 여전히 60억에서 70억 달러 수준이고요. 호퍼의 강세가 계속될 것으로 보시는지, 그리고 이 70억 달러 증가분을 이 세 가지 구성요소로 어떻게 나누어 생각해야 하는지 궁금합니다.</td></tr>
<tr><td>Colette M. Kress: Thanks, Stacy, for the question. First part of it, looking at our growth between Q2 and Q3, Blackwell is still going to be the lion's share of what we have in terms of data center. But keep in mind, that helps both our compute side as well as it helps our networking side because we are selling those significant systems that are incorporating the NVLink that Jensen just spoke about. Selling Hopper, we are still selling it. H100, H200s, we are. Again, they are HGX systems, and I still believe our Blackwell will be the lion's share of what we're doing on there. So we'll continue. We don't have any more specific details in terms of how we'll finish our quarter, but you should expect Blackwell again to be the driver of the growth.</td><td>**Colette M. Kress:** 스테이시, 질문 감사합니다. 첫 번째 부분인 2분기와 3분기 간 성장을 보면, 블랙웰이 여전히 데이터센터 부문에서 가장 큰 비중을 차지할 것입니다. 하지만 이것이 컴퓨팅 부문뿐만 아니라 네트워킹 부문에도 도움이 된다는 점을 염두에 두시기 바랍니다. 젠슨이 방금 언급한 NVLink를 통합한 대규모 시스템을 판매하고 있기 때문입니다. <br><br>호퍼 제품도 여전히 판매하고 있습니다. H100, H200 모두 판매 중이며, 이들은 HGX 시스템입니다. 그럼에도 불구하고 블랙웰이 우리 사업에서 가장 큰 비중을 차지할 것이라고 생각합니다. <br><br>분기 마감에 대한 구체적인 세부사항은 더 이상 제공할 수 없지만, 블랙웰이 다시 한번 성장의 동력이 될 것으로 예상하시면 됩니다.</td></tr>
<tr><td>Operator: Your next question comes from Jim Schneider of Goldman Sachs.</td><td>**Operator:** 다음 질문은 골드만삭스의 짐 슈나이더님께서 주셨습니다.</td></tr>
<tr><td>James Edward Schneider: You've been very clear about the reasoning model opportunity that you see, and you've also been relatively clear about technical specs for Rubin. But maybe you could provide a little bit of context about how you view the Rubin product transition going forward. What incremental capability does that offer to customers? And would you say that Rubin is a bigger, smaller or similar step-up in terms of performance from a capability perspective relative to what we saw with Blackwell?</td><td>**James Edward Schneider:** 추론 모델 기회에 대해서는 매우 명확하게 설명해주셨고, Rubin의 기술 사양에 대해서도 상당히 구체적으로 말씀해주셨습니다. 하지만 향후 Rubin 제품 전환을 어떻게 보고 계신지에 대해 조금 더 맥락을 제공해주실 수 있을까요? 고객들에게 어떤 추가적인 역량을 제공하는 건가요? 그리고 성능 관점에서 Rubin이 Blackwell에서 봤던 것과 비교해서 더 큰 도약인지, 더 작은 도약인지, 아니면 비슷한 수준의 향상인지 어떻게 평가하시나요?</td></tr>
<tr><td>Jen-Hsun Huang: Yes, thanks. Rubin. Rubin, we're on an annual cycle. And the reason why we're on an annual cycle is because we can do so to accelerate the cost reduction and maximize the revenue generation for our customers. When we increase the perf per watt, the token generation per amount of usage of energy, we are effectively driving the revenues of our customers. The perf per watt of Blackwell will be for reasoning systems in order of magnitude higher than Hopper. And so for the same amount of energy, and everybody's data center is energy limited by definition, for any data center, we -- using Blackwell, you'll be able to maximize your revenues compared to anything we've done in the past, compared to anything in the world today and because the perf per dollar, the performance is so good that the perf per dollar invested in the capital would also allow you to improve your gross margins. To the extent that we have great ideas for every single generation, we could improve the revenue generation, improve the AI capability, improve the margins of our customers by releasing new architectures. And so we advise our partners, our customers to pace themselves and to build these data centers on an annual rhythm. And Rubin is going to have a whole bunch of new ideas. I'll pause for a second because I've got plenty of time between now and a year from now to tell you about all the breakthroughs that Rubin is going to bring, but Rubin has a lot of great ideas. I'm anxious to tell you, but I can't right now. And I'll save it for GTC to tell you more and more about it. But nonetheless, for the next year, we're ramping really hard into now Grace Blackwell, GB200, and then now Blackwell Ultra, GB300, we're ramping really hard into data centers. This year is obviously a record-breaking year. I expect next year to be a record-breaking year. And while we continue to increase the performance of AI capabilities as we race towards artificial superintelligence on the one hand and continue to increase the revenue generation capabilities of our hyperscalers on the other hand.</td><td>**Jen-Hsun Huang:** 네, 감사합니다. 루빈, 저희는 연간 주기로 운영하고 있습니다. 연간 주기로 하는 이유는 비용 절감을 가속화하고 고객들의 수익 창출을 극대화할 수 있기 때문입니다. 와트당 성능(perf per watt)을 높이면, 에너지 사용량 대비 토큰 생성량이 늘어나면서 실질적으로 고객들의 수익을 증대시키게 됩니다. 블랙웰의 추론 시스템용 와트당 성능은 호퍼보다 한 자릿수 높은 수준이 될 것입니다. 그리고 같은 양의 에너지로 - 모든 데이터센터는 정의상 에너지 제약을 받습니다 - 어떤 데이터센터든 블랙웰을 사용하면 과거에 저희가 만든 어떤 것과 비교해도, 오늘날 세계의 어떤 것과 비교해도 수익을 극대화할 수 있을 것입니다. 그리고 달러당 성능, 즉 성능이 너무 뛰어나서 투자한 자본 대비 성능도 고객들의 총마진을 개선할 수 있게 해줄 것입니다. 저희가 모든 세대마다 훌륭한 아이디어를 가지고 있는 한, 새로운 아키텍처를 출시함으로써 수익 창출을 개선하고, AI 역량을 향상시키고, 고객들의 마진을 개선할 수 있습니다. 그래서 저희는 파트너들과 고객들에게 속도를 조절하면서 이런 데이터센터들을 연간 주기로 구축하라고 조언하고 있습니다. 그리고 루빈은 완전히 새로운 아이디어들을 많이 가지고 있습니다. 잠깐 멈춰서 말씀드리자면, 지금부터 1년 후까지는 루빈이 가져올 모든 혁신적인 기술들에 대해 말씀드릴 충분한 시간이 있는데, 루빈은 정말 훌륭한 아이디어들을 많이 가지고 있습니다. 여러분께 말씀드리고 싶어서 안달이 나지만, 지금은 할 수 없습니다. GTC에서 더 자세히 말씀드리도록 하겠습니다. 하지만 그럼에도 불구하고, 내년에는 그레이스 블랙웰(Grace Blackwell), GB200, 그리고 이제 블랙웰 울트라(Blackwell Ultra), GB300까지 데이터센터로 정말 강력하게 확장하고 있습니다. 올해는 분명히 기록적인 해입니다. 내년도 기록적인 해가 될 것으로 기대합니다. 한편으로는 인공 초지능을 향해 나아가면서 AI 역량의 성능을 지속적으로 향상시키고, 다른 한편으로는 하이퍼스케일러들의 수익 창출 능력을 계속 증대시키고 있습니다.</td></tr>
<tr><td>Operator: Your final question comes from Timothy Arcuri with UBS.</td><td>**Operator:** 마지막 질문은 UBS의 Timothy Arcuri님께서 주셨습니다.</td></tr>
<tr><td>Timothy Michael Arcuri: Jensen, I wanted to ask you, just answered the question. You threw out a number. You said 50% CAGR for the AI market. So I'm wondering how much visibility that you have into next year. Is that kind of a reasonable bogey in terms of how much your data center revenue should grow next year? I would think you'll grow at least in line with that CAGR? And maybe are there any puts and takes to that?</td><td>**Timothy Michael Arcuri:** 젠슨, 방금 답변하신 내용에 대해 질문드리고 싶습니다. 수치를 하나 언급하셨는데, AI 시장의 연평균 성장률(CAGR)이 50%라고 하셨죠. 내년에 대한 가시성이 어느 정도인지 궁금합니다. 이 수치가 내년 데이터센터 매출 성장률의 합리적인 목표치라고 볼 수 있을까요? 최소한 그 CAGR에 맞춰 성장할 것으로 생각하는데, 이에 대한 긍정적 요인이나 부정적 요인이 있는지요?</td></tr>
<tr><td>Jen-Hsun Huang: Well, I think the best way to look at it is we have reasonable forecasts from our large customers for next year, a very, very significant forecast. And we still have a lot of businesses that we're still winning and a lot of start-ups that are still being created. Don't forget that the number of start-ups for -- native-AI start-ups was $100 billion was funded last year. This year, the year is not even over yet, it's $180 billion funded. If you look at AI native, the top AI-native start-ups that are generating revenues last year was $2 billion. This year, it's $20 billion. Next year be 10x higher than this year is not inconceivable. And the open source models is now opening up large enterprises, SaaS companies, industrial companies, robotics companies to now join the AI revolution, another source of growth. And whether it's AI natives or enterprise SaaS or industrial AI or start-ups, we're just seeing just enormous amount of interest in AI and demand for AI. Right now, the buzz is -- I'm sure all of you know about the buzz out there. The buzz is everything sold out. H100 sold out. H200s are sold out. Large CSPs are coming out renting capacity from other CSPs. And so the AI-native start-ups are really scrambling to get capacity so that they could train their reasoning models. And so the demand is really, really high. But the long-term outlook between where we are today, CapEx has doubled in 2 years. It is now running about $600 billion a year just in the large hyperscalers. For us to grow into that $600 billion a year, representing a significant part of that CapEx isn't unreasonable. And so I think the next several years, surely through the decade, we see just a really fast growing, really significant growth opportunities ahead. Let me conclude with this. Blackwell is the next-generation AI platform the world has been waiting for. It delivers an exceptional generational leap. NVIDIA's NVLink 72 rack scale computing is revolutionary, arriving just in time as reasoning AI models drive order of magnitude increases in training and inference performance requirement. Blackwell Ultra is ramping at full speed and the demand is extraordinary. Our next platform Rubin, is already in fab. We have 6 new chips that represents the Rubin platform. They have all ticked up at TSMC. Rubin will be our third-generation NVLink rack scale AI supercomputer. And so we expect to have a much more mature and fully scaled up supply chain. Blackwell and Rubin AI factory platforms will be scaling into the $3 trillion to $4 trillion global AI factory build out through the end of the decade. Customers are building ever greater scale AI factories from thousands of Hopper GPUs in tens of megawatt data centers to now hundreds of thousands of Blackwells in 100-megawatt facilities. And soon, we'll be building millions of Rubin GPU platforms, powering multi-gigawatt multisite AI super factories. With each generation, demand only grows. One shot chatbots have evolved into reasoning agentic AI that research, plan and use tools, driving orders of magnitude jump in compute for both training and inference. Agentic AI is reaching maturity and has opened the enterprise market to build domain and company-specific AI agents for enterprise workflows, products and services. The age of physical AI has arrived, unlocking entirely new industries in robotics, industrial automation. Every industrial company will need to build 2 factories: 1 to build the machines and another to build their robotic AI. This quarter, NVIDIA reached record revenues and an extraordinary milestone in our journey. The opportunity ahead is immense. A new industrial revolution has started. The AI race is on. Thanks for joining us today, and I look forward to addressing you next week -- next earnings call. Thank you.</td><td>**Jen-Hsun Huang:** 이를 살펴보는 가장 좋은 방법은 내년에 대한 대형 고객들의 합리적인 예측치가 있다는 것입니다. 매우, 매우 중요한 예측치죠. 그리고 여전히 우리가 수주하고 있는 많은 사업들과 계속 생겨나고 있는 많은 스타트업들이 있습니다. AI 네이티브 스타트업들에 대한 투자 규모를 잊지 마세요. 작년에는 1,000억 달러가 투자되었습니다. 올해는 아직 끝나지도 않았는데 1,800억 달러가 투자되었어요. AI 네이티브를 보면, 작년에 수익을 창출한 상위 AI 네이티브 스타트업들의 매출이 20억 달러였습니다. 올해는 200억 달러예요. 내년이 올해보다 10배 높아진다는 것도 상상할 수 없는 일은 아닙니다. 그리고 오픈소스 모델들이 이제 대기업, SaaS 기업, 산업 기업, 로보틱스 기업들이 AI 혁명에 참여할 수 있는 길을 열어주고 있어서, 또 다른 성장 동력이 되고 있습니다. AI 네이티브든, 기업용 SaaS든, 산업용 AI든, 스타트업이든 상관없이 AI에 대한 엄청난 관심과 수요를 보고 있습니다. 지금 시장의 화두는 - 여러분도 다 아시겠지만 - 모든 게 품절이라는 겁니다. H100 품절, H200도 품절입니다. 대형 CSP(클라우드 서비스 제공업체)들이 다른 CSP들로부터 용량을 임대하고 있는 상황이에요. 그래서 AI 네이티브 스타트업들이 추론 모델을 훈련시키기 위한 용량을 확보하려고 정말 안간힘을 쓰고 있습니다. 수요가 정말, 정말 높은 상황입니다. 하지만 현재 상황과 장기 전망을 보면, 자본지출(CapEx)이 2년 만에 두 배로 늘었습니다. 현재 대형 하이퍼스케일러들만으로도 연간 약 6천억 달러 규모로 운영되고 있습니다. 우리가 그 연간 6천억 달러 시장으로 성장해서 그 자본지출(CapEx)의 상당 부분을 차지하는 것은 무리가 아닙니다. 그래서 앞으로 몇 년간, 확실히 이번 10년 동안은 정말 빠르게 성장하는, 정말 중요한 성장 기회들이 앞에 있다고 봅니다.<br><br>이것으로 마무리하겠습니다. 블랙웰(Blackwell)은 세상이 기다려온 차세대 AI 플랫폼입니다. 이는 뛰어난 세대적 도약을 제공합니다. 엔비디아의 NVLink 72 랙 스케일 컴퓨팅은 혁신적이며, 추론 AI 모델들이 훈련과 추론 성능 요구사항을 몇 배나 증가시키는 시점에 딱 맞춰 출시되었습니다. 블랙웰 울트라(Blackwell Ultra)는 최고 속도로 램프업하고 있으며 수요가 엄청납니다. 우리의 다음 플랫폼인 루빈(Rubin)은 이미 파운드리에 들어가 있습니다. 루빈 플랫폼을 대표하는 6개의 새로운 칩이 있습니다. 이들은 모두 TSMC에서 테이프아웃되었습니다. 루빈은 우리의 3세대 NVLink 랙 스케일 AI 슈퍼컴퓨터가 될 것입니다. 따라서 훨씬 더 성숙하고 완전히 확장된 공급망을 갖출 것으로 예상합니다. 블랙웰과 루빈 AI 팩토리 플랫폼은 2020년대 말까지 3조 달러에서 4조 달러 규모의 글로벌 AI 팩토리 구축에 맞춰 확장될 것입니다. 고객들은 수십 메가와트 데이터센터의 수천 개 호퍼 GPU에서 이제는 100메가와트 시설의 수십만 개 블랙웰에 이르기까지 점점 더 큰 규모의 AI 팩토리를 구축하고 있습니다. 그리고 곧 우리는 멀티 기가와트 멀티사이트 AI 슈퍼 팩토리를 구동하는 수백만 개의 루빈 GPU 플랫폼을 구축하게 될 것입니다. 세대가 거듭될수록 수요는 계속 증가하고 있습니다. 일회성 챗봇이 연구하고 계획하며 도구를 사용하는 추론형 에이전틱 AI로 발전하면서, 훈련과 추론 모두에서 컴퓨팅 요구량이 기하급수적으로 증가하고 있습니다. 에이전틱 AI가 성숙 단계에 도달하면서 기업 워크플로우, 제품, 서비스를 위한 도메인별, 회사별 AI 에이전트를 구축할 수 있는 기업 시장이 열렸습니다. 물리적 AI 시대가 도래하여 로보틱스와 산업 자동화 분야에서 완전히 새로운 산업들이 열리고 있습니다. 모든 산업 기업들은 두 개의 공장을 건설해야 할 것입니다. 하나는 기계를 만들기 위한 공장이고, 다른 하나는 로봇 AI를 구축하기 위한 공장입니다. 이번 분기에 NVIDIA는 기록적인 매출을 달성했으며 우리 여정에서 특별한 이정표에 도달했습니다. 앞으로의 기회는 엄청납니다. 새로운 산업혁명이 시작되었습니다. AI 경쟁이 본격화되었습니다. 오늘 참여해 주셔서 감사합니다. 다음 주 실적발표에서 다시 뵙겠습니다. 감사합니다.</td></tr>
<tr><td>Operator: This concludes today's conference call. You may now disconnect.</td><td>**Operator:** 오늘 컨퍼런스 콜을 마치겠습니다. 이제 연결을 종료하셔도 됩니다.</td></tr>
    </table>
    <h3>📌 요약</h3>
    <p style="background:#f0f0f0; padding:15px; border-left: 5px solid #333;">Here are the key points from the earnings call transcript in Korean:<br><br>• 실적 및 전망:<br>- NVIDIA의 데이터센터 사업이 기록적인 성장을 보임<br>- 중국 시장 기회는 올해 약 500억 달러 규모로 추정되며, 연간 50% 성장 예상<br>- 2030년까지 AI 인프라 시장이 3-4조 달러 규모로 성장할 것으로 전망<br><br>• 기술 및 제품 전략:<br>- Blackwell 플랫폼이 성공적으로 출시되었으며, 차세대 Rubin 플랫폼도 개발 중<br>- NVLink 72 기술을 통해 이전 세대 대비 획기적인 성능 향상 달성<br>- 네트워킹 사업(Spectrum-X, InfiniBand 등)이 연간 100억 달러 규모로 성장<br><br>• 시장 환경:<br>- 추론형 AI(Reasoning AI)의 발전으로 컴퓨팅 수요가 급증<br>- 주요 클라우드 사업자들의 설비투자가 2년만에 2배 증가하여 연간 6,000억 달러 규모<br>- 오픈소스 AI 모델의 확산으로 기업용 AI 시장이 확대되는 추세<br><br>경영진은 전반적으로 매우 자신감 있는 톤을 보였으며, 단기 및 장기 성장 전망에 대해 긍정적인 견해를 제시했습니다.</p>
    <hr style="margin:50px 0;">
    
</body></html>