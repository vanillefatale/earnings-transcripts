<!DOCTYPE html>
<html><head><meta charset="UTF-8">
<title>Earnings Call 번역</title>
<style>
    body { font-family: Arial; margin: 40px; background-color: #fdfdfd; }
    h1 { text-align: center; }
    h2 { margin-top: 50px; color: #003366; }
    h3 { color: #333; }
    table { border: 1px solid #ddd; width: 100%; border-collapse: collapse; }
    th { background: #f0f0f0; padding: 10px; border-bottom: 2px solid #ccc; }
    td { padding: 10px; border-bottom: 1px dotted #ccc; vertical-align: top; }
    p { line-height: 1.6; }
    hr { margin: 50px 0; border: none; border-top: 1px solid #ccc; }
    .back-button {
        display: inline-block;
        background-color: #5f5f5f;
        color: white;
        padding: 10px 16px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        margin-bottom: 30px;
    }
</style>
</head><body>
<a href="../../index.html" class="back-button">←</a>
<h1>📄 Earnings Call Transcript 번역 결과</h1>

    <h2>📊 Presentation</h2>
    <table style="width:100%; border-collapse:collapse; margin-bottom: 40px;">
        <tr>
            <th style="width:50%; border-bottom: 2px solid #333;">Original</th>
            <th style="width:50%; border-bottom: 2px solid #333;">Translation</th>
        </tr>
        <tr><td>NVIDIA Corporation (NASDAQ:NVDA) Q1 2026 Earnings Conference Call May 28, 2025 5:00 PM ET<br><br>Company Participants<br><br>Toshiya Hari - Investor Relations<br>Colette Kress - Executive Vice President & Chief Financial Officer<br>Jensen Huang - President & Chief Executive Officer<br><br>Conference Call Participants<br><br>Joe Moore - Morgan Stanley<br>Vivek Arya - Bank of America Securities<br>CJ Muse - Cantor Fitzgerald<br>Ben Reitzes - Melius<br>Timothy Arcuri - UBS<br><br>Operator<br><br>Good afternoon. My name is Sarah, and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's First Quarter Fiscal 2026 Financial Results Conference Call.</td><td>NVIDIA Corporation (NASDAQ:NVDA) 2026년도 1분기 실적 컨퍼런스 콜 2025년 5월 28일 오후 5:00 ET<br><br>참석자<br><br>회사 관계자<br>토시야 하리 - 투자자 관계 담당<br>콜레트 크레스 - 부사장 겸 최고재무책임자<br>젠슨 황 - 사장 겸 최고경영자<br><br>컨퍼런스 콜 참석자<br>조 무어 - 모건 스탠리<br>비벡 아리야 - 뱅크 오브 아메리카 증권<br>CJ 뮤즈 - 캔터 피츠제럴드<br>벤 라이체스 - 멜리우스<br>티모시 아르쿠리 - UBS<br><br>진행자<br><br>안녕하세요. 저는 사라이며, 오늘 컨퍼런스 진행을 맡게 되었습니다. 이 시간에 NVIDIA의 2026년도 1분기 재무 실적 컨퍼런스 콜에 참석해 주신 모든 분들을 환영합니다.</td></tr>
<tr><td>All lines have been placed on mute to prevent any background noise. After the speakers' remarks, there will be a question-and-answer session. [Operator Instructions] Thank you. Toshiya Hari, you may begin your conference. Toshiya Hari<br><br>Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2026. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website.</td><td>모든 회선이 배경 소음을 방지하기 위해 음소거 상태로 설정되었습니다. 발표자들의 발언 후에는 질의응답 시간이 있을 예정입니다. [운영자 안내] 감사합니다. 토시야 하리님, 회의를 시작하셔도 됩니다.<br><br>토시야 하리<br><br>감사합니다. 안녕하세요 여러분, NVIDIA의 2026 회계연도 1분기 실적 발표 컨퍼런스 콜에 오신 것을 환영합니다. 오늘 NVIDIA에서 함께 참석한 분들은 젠슨 황 사장 겸 최고경영자와 콜레트 크레스 부사장 겸 최고재무책임자입니다. 이번 콜은 NVIDIA 투자자 관계 웹사이트에서 실시간으로 웹캐스트되고 있음을 알려드립니다.</td></tr>
<tr><td>The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.</td><td>웹캐스트는 2026 회계연도 2분기 재무실적을 논의하는 컨퍼런스 콜까지 재시청이 가능합니다. 오늘 콜의 내용은 NVIDIA의 자산입니다. 당사의 사전 서면 동의 없이는 복제하거나 전사할 수 없습니다. 이번 콜에서 당사는 현재의 기대에 기반한 미래전망진술을 할 수 있습니다. 이러한 진술들은 다수의 중대한 리스크와 불확실성에 노출되어 있으며, 실제 결과는 상당히 다를 수 있습니다.</td></tr>
<tr><td>For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 28, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.</td><td>향후 재무 성과와 사업에 영향을 미칠 수 있는 요인들에 대한 논의는 오늘 발표된 실적 발표자료, 최근 제출한 Form 10-K 및 10-Q, 그리고 증권거래위원회에 Form 8-K로 제출할 수 있는 보고서들의 공시 내용을 참조해 주시기 바랍니다. 모든 발언은 현재 저희가 보유한 정보를 바탕으로 오늘 2025년 5월 28일 기준으로 작성되었습니다. 법적으로 요구되는 경우를 제외하고는, 저희는 이러한 발언들을 업데이트할 의무를 지지 않습니다. 이번 컨퍼런스 콜에서는 비GAAP 재무지표들을 논의할 예정입니다. 이러한 비GAAP 재무지표와 GAAP 재무지표 간의 조정표는 저희 웹사이트에 게시된 CFO 해설자료에서 확인하실 수 있습니다.</td></tr>
<tr><td>With that, let me turn the call over to Colette. Colette Kress<br><br>Thank you, Toshiya. We delivered another strong quarter with revenue of $44 billion, up 69% year-over-year, exceeding our outlook in what proved to be a challenging operating environment. Data Center revenue of $39 billion grew 73% year-on-year. AI workloads have transitioned strongly to inference, and AI factory buildouts are driving significant revenue. Our customers' commitments are firm. On April 9th, the U.S. government issued new export controls on H20, our data center GPU designed specifically for the China market. We sold H20 with the approval of the previous administration.</td><td>그럼 이제 콜레트에게 마이크를 넘기겠습니다. 콜레트 크레스<br><br>감사합니다, 토시야. 저희는 또 한 번 강력한 분기 실적을 달성했습니다. 매출은 440억 달러로 전년 동기 대비 69% 증가하여, 어려운 운영 환경 속에서도 전망치를 상회했습니다. 데이터센터 매출은 390억 달러로 전년 대비 73% 성장했습니다. AI 워크로드는 추론(inference)으로 강력하게 전환되고 있으며, AI 팩토리 구축이 상당한 매출을 견인하고 있습니다. 고객들의 약속은 확고합니다. 4월 9일, 미국 정부는 중국 시장을 위해 특별히 설계된 저희 데이터센터 GPU인 H20에 대한 새로운 수출 통제 조치를 발표했습니다. 저희는 이전 행정부의 승인 하에 H20을 판매했습니다.</td></tr>
<tr><td>Although our H20 has been in the market for over a year and does not have a market outside of China, the new export controls on H20 did not provide a grace period to allow us to sell through our inventory. In Q1, we recognized $4.6 billion in H20 revenue, which occurred prior to April 9, but also recognized a $4.5 billion charge as we wrote down inventory and purchase obligations tied to orders we had received prior to April 9. We were unable to ship $2.5 billion in H20 revenue in the first quarter due to the new export controls. The $4.5 billion charge was less than what we initially anticipated as we were able to reuse certain materials.</td><td>저희 H20은 1년 넘게 시장에 출시되어 있었고 중국 외 시장이 없음에도 불구하고, H20에 대한 새로운 수출 통제 조치는 저희가 재고를 판매할 수 있는 유예 기간을 제공하지 않았습니다. 1분기에 저희는 4월 9일 이전에 발생한 46억 달러의 H20 매출을 인식했지만, 동시에 4월 9일 이전에 받은 주문과 관련된 재고 및 구매 의무를 평가절하하면서 45억 달러의 손실을 인식했습니다. 새로운 수출 통제로 인해 1분기에 25억 달러의 H20 매출을 출하할 수 없었습니다. 45억 달러의 손실은 특정 자재들을 재사용할 수 있었기 때문에 저희가 초기에 예상했던 것보다는 적은 금액이었습니다.</td></tr>
<tr><td>We are still evaluating our limited options to supply data center compute products compliant with the U.S. government's revised export control rules. Losing access to the China AI accelerator market, which we believe will grow to nearly $50 billion, would have a material adverse impact on our business going forward and benefit our foreign competitors in China and worldwide. Our Blackwell ramp, the fastest in our company's history, drove a 73% year-on-year increase in Data Center revenue. Blackwell contributed nearly 70% of Data Center compute revenue in the quarter with a transition from Hopper nearly complete.</td><td>미국 정부의 개정된 수출 통제 규정을 준수하는 데이터센터 컴퓨팅 제품 공급을 위한 제한적인 옵션들을 여전히 검토하고 있습니다. 약 500억 달러 규모로 성장할 것으로 예상되는 중국 AI 가속기 시장에 대한 접근권을 잃게 된다면, 향후 당사 사업에 중대한 악영향을 미치고 중국 및 전 세계의 해외 경쟁업체들에게 이익을 가져다 줄 것입니다. 당사 역사상 가장 빠른 속도로 진행된 Blackwell 램프업은 데이터센터 매출의 전년 동기 대비 73% 증가를 견인했습니다. Blackwell은 이번 분기 데이터센터 컴퓨팅 매출의 거의 70%를 차지했으며, Hopper로부터의 전환이 거의 완료되었습니다.</td></tr>
<tr><td>The introduction of GB200 NVL was a fundamental architectural change to enable data center scale workloads and to achieve the lowest cost per inference token. While these systems are complex to build, we have seen a significant improvement in manufacturing yields, and rack shipments are moving to strong rates to end customers. GB200 NVL racks are now generally available for model builders, enterprises and sovereign customers to develop and deploy AI. On average, major hyperscalers are each deploying nearly 1,000 NVL72 racks or 72,000 Blackwell GPUs per week and are on track to further ramp output this quarter.</td><td>GB200 NVL의 도입은 데이터센터 규모의 워크로드를 가능하게 하고 추론 토큰당 최저 비용을 달성하기 위한 근본적인 아키텍처 변화였습니다. 이러한 시스템은 구축하기 복잡하지만, 제조 수율에서 상당한 개선을 보였으며, 랙 출하량이 최종 고객들에게 강력한 속도로 이동하고 있습니다. GB200 NVL 랙은 이제 모델 빌더, 기업 및 주권 고객들이 AI를 개발하고 배포할 수 있도록 일반적으로 이용 가능합니다. 평균적으로 주요 하이퍼스케일러들은 각각 주당 거의 1,000개의 NVL72 랙 또는 72,000개의 Blackwell GPU를 배포하고 있으며, 이번 분기에 생산량을 더욱 확대할 예정입니다.</td></tr>
<tr><td>Microsoft, for example, has already deployed tens of thousands of Blackwell GPUs and is expected to ramp to hundreds of thousands of GB200s with OpenAI as one of its key customers. Key learnings from the GB200 ramp will allow for a smooth transition to the next phase of our product roadmap, Blackwell Ultra. Sampling of GB300 systems began earlier this month at the major CSPs, and we expect production shipments to commence later this quarter. GB300 will leverage the same architecture, same physical footprint and the same electrical and mechanical specifications as GB200.</td><td>예를 들어, 마이크로소프트는 이미 수만 개의 Blackwell GPU를 배치했으며, OpenAI를 주요 고객 중 하나로 하여 수십만 개의 GB200으로 확대할 것으로 예상됩니다. GB200 램프업에서 얻은 주요 학습 내용은 우리 제품 로드맵의 다음 단계인 Blackwell Ultra로의 원활한 전환을 가능하게 할 것입니다. GB300 시스템의 샘플링은 이번 달 초 주요 CSP들에서 시작되었으며, 이번 분기 후반에 양산 출하가 시작될 것으로 예상합니다. GB300은 GB200과 동일한 아키텍처, 동일한 물리적 풋프린트, 그리고 동일한 전기적 및 기계적 사양을 활용할 것입니다.</td></tr>
<tr><td>The GB300 drop-in design will allow CSPs to seamlessly transition their systems and manufacturing used for GB200 while maintaining high yields. B300 GPUs with 50% more HBM will deliver another 50% increase in dense FP4 inference compute performance compared to the B200. We remain committed to our annual product cadence with our roadmap extending through 2028, tightly aligned with the multiple year planning cycles of our customers. We are witnessing a sharp jump in inference demand. OpenAI, Microsoft and Google are seeing a step function leap in token generation. Microsoft processed over 100 trillion tokens in Q1, a five-fold increase on a year-over-year basis.</td><td>GB300 드롭인 설계는 CSP들이 GB200에 사용되던 시스템과 제조 공정을 원활하게 전환하면서도 높은 수율을 유지할 수 있도록 해줄 것입니다. 50% 더 많은 HBM을 탑재한 B300 GPU는 B200 대비 고밀도 FP4 추론 컴퓨팅 성능에서 추가로 50% 향상을 제공할 것입니다. 저희는 고객들의 다년간 계획 주기와 긴밀하게 연계된 2028년까지의 로드맵과 함께 연간 제품 출시 일정에 대한 약속을 지켜나가고 있습니다. 저희는 추론 수요의 급격한 증가를 목격하고 있습니다. OpenAI, Microsoft, Google은 토큰 생성에서 단계적 도약을 보이고 있습니다. Microsoft는 1분기에 100조 개가 넘는 토큰을 처리했으며, 이는 전년 동기 대비 5배 증가한 수치입니다.</td></tr>
<tr><td>This exponential growth in Azure OpenAI is representative of strong demand for Azure AI Foundry as well as other AI services across Microsoft's platform. Inference serving startups are now serving models using B200, tripling their token generation rate and corresponding revenues for high-value reasoning models such as DeepSeek-R1 as reported by artificial analysis. NVIDIA Dynamo on Blackwell NVL72 turbocharges AI inference throughput by 30x for the new reasoning models, sweeping the industry.</td><td>이러한 Azure OpenAI의 기하급수적 성장은 Azure AI Foundry와 Microsoft 플랫폼 전반의 기타 AI 서비스에 대한 강력한 수요를 보여주는 대표적인 사례입니다. 추론 서빙 스타트업들은 현재 B200을 사용하여 모델을 서비스하고 있으며, 인공 분석(artificial analysis)에서 보고된 바와 같이 DeepSeek-R1과 같은 고부가가치 추론 모델에 대해 토큰 생성률과 그에 따른 수익을 3배 증가시키고 있습니다. Blackwell NVL72의 NVIDIA Dynamo는 새로운 추론 모델에 대한 AI 추론 처리량을 30배 향상시켜 업계를 석권하고 있습니다.</td></tr>
<tr><td>Developer engagements increased with adoption ranging from LLM providers such as Perplexity to financial services institutions such as Capital One, who reduced agentic chatbot latency by 5x with Dynamo. In the latest MLPerf Inference results, we submitted our first results using GB200 NVL72, delivering up to 30x higher inference throughput compared to our 8-GPU H200 submission on the challenging Llama 3.1 benchmark. This feat was achieved through a combination of tripling the performance for GPU as well as 9x more GPUs all connected on a single NVLink domain.</td><td>개발자 참여가 증가했으며, Perplexity와 같은 LLM 제공업체부터 Dynamo를 통해 에이전틱 챗봇 지연시간을 5배 단축한 Capital One과 같은 금융 서비스 기관에 이르기까지 다양한 분야에서 도입이 이루어지고 있습니다. 최신 MLPerf Inference 결과에서 저희는 GB200 NVL72를 사용한 첫 번째 결과를 제출했으며, 까다로운 Llama 3.1 벤치마크에서 8-GPU H200 제출 대비 최대 30배 높은 추론 처리량을 달성했습니다. 이러한 성과는 GPU당 성능을 3배 향상시키고, 단일 NVLink 도메인에 연결된 GPU를 9배 더 많이 사용한 조합을 통해 달성되었습니다.</td></tr>
<tr><td>And while Blackwell is still early in its life cycle, software optimizations have already improved its performance by 1.5x in the last month alone. We expect to continue improving the performance of Blackwell through its operational life as we have done with Hopper and Ampere. For example, we increased the inference performance of Hopper by four times over two years. This is the benefit of NVIDIA's programmable CUDA architecture and rich ecosystem.</td><td>블랙웰이 아직 라이프사이클 초기 단계에 있음에도 불구하고, 소프트웨어 최적화를 통해 지난 한 달 동안만 성능이 1.5배 향상되었습니다. 저희는 호퍼와 암페어에서 그랬듯이 블랙웰의 운영 기간 전반에 걸쳐 성능을 지속적으로 개선해 나갈 것으로 예상합니다. 예를 들어, 저희는 2년에 걸쳐 호퍼의 추론 성능을 4배 향상시켰습니다. 이것이 바로 엔비디아의 프로그래머블 CUDA 아키텍처와 풍부한 생태계가 주는 이점입니다.</td></tr>
<tr><td>The pace and scale of AI factory deployments are accelerating with nearly 100 NVIDIA-powered AI factories in flight this quarter, a two-fold increase year-over-year, with the average number of GPUs powering each factory also doubling in the same period. And more AI factory projects are starting across industries and geographies. NVIDIA's full stack architecture is underpinning AI factory deployments as industry leaders like AT&T, BYD, Capital One, Foxconn, MediaTek, and Telenor, are strategically vital sovereign clouds like those recently announced in Saudi Arabia, Taiwan and the UAE.</td><td>AI 팩토리 배치의 속도와 규모가 가속화되고 있으며, 이번 분기에는 NVIDIA 기반 AI 팩토리가 거의 100개에 달해 전년 동기 대비 2배 증가했습니다. 또한 각 팩토리를 구동하는 GPU의 평균 개수도 같은 기간 동안 2배로 증가했습니다. 그리고 더 많은 AI 팩토리 프로젝트들이 다양한 산업과 지역에서 시작되고 있습니다. NVIDIA의 풀스택 아키텍처는 AT&T, BYD, Capital One, Foxconn, MediaTek, Telenor와 같은 업계 리더들과 최근 사우디아라비아, 대만, UAE에서 발표된 것과 같은 전략적으로 중요한 소버린 클라우드들의 AI 팩토리 배치를 뒷받침하고 있습니다.</td></tr>
<tr><td>We have a line of sight to projects requiring tens of gigawatts of NVIDIA AI infrastructure in the not-too-distant future. The transition from generative to agentic AI, AI capable of receiving, reasoning, planning and acting will transform every industry, every company and country. We envision AI agents as a new digital workforce capable of handling tasks ranging from customer service to complex decision-making processes. We introduced the Llama Nemotron family of open reasoning models designed to supercharge agentic AI platforms for enterprises.</td><td>머지않은 미래에 수십 기가와트 규모의 NVIDIA AI 인프라를 필요로 하는 프로젝트들이 가시권에 들어와 있습니다. 생성형 AI에서 에이전틱 AI로의 전환, 즉 수신, 추론, 계획 및 행동이 가능한 AI로의 전환은 모든 산업, 모든 기업과 국가를 변화시킬 것입니다. 저희는 AI 에이전트를 고객 서비스부터 복잡한 의사결정 프로세스에 이르기까지 다양한 업무를 처리할 수 있는 새로운 디지털 인력으로 구상하고 있습니다. 저희는 기업용 에이전틱 AI 플랫폼을 강화하도록 설계된 오픈 추론 모델인 Llama Nemotron 패밀리를 출시했습니다.</td></tr>
<tr><td>Built on the Llama architecture, these models are available as NIMs, or NVIDIA Inference Microservices, with multiple sizes to meet diverse deployment needs. Our post training enhancements have yielded a 20% accuracy boost and a 5x increase in inference speed. Leading platform companies, including Accenture, Cadence, Deloitte, and Microsoft are transforming work with our reasoning models. NVIDIA NeMo microservices are generally available across industries are being leveraged by leading enterprises to build, optimize and scale AI applications. With NeMo, Cisco increased model accuracy by 40% and improved response time by 10x in its code assistant.</td><td>Llama 아키텍처를 기반으로 구축된 이러한 모델들은 다양한 배포 요구사항을 충족하기 위해 여러 크기로 제공되는 NIM(NVIDIA Inference Microservices) 형태로 이용 가능합니다. 저희의 사후 훈련 개선을 통해 정확도는 20% 향상되었고 추론 속도는 5배 증가했습니다. Accenture, Cadence, Deloitte, Microsoft를 포함한 주요 플랫폼 기업들이 저희의 추론 모델을 활용하여 업무를 혁신하고 있습니다. NVIDIA NeMo 마이크로서비스는 전 산업에 걸쳐 일반적으로 이용 가능하며, 주요 기업들이 AI 애플리케이션을 구축, 최적화 및 확장하는 데 활용하고 있습니다. NeMo를 통해 Cisco는 코드 어시스턴트에서 모델 정확도를 40% 향상시키고 응답 시간을 10배 개선했습니다.</td></tr>
<tr><td>NASDAQ realized a 30% improvement in accuracy and response time in its AI platform's search capabilities. And Shell's custom LLM achieved a 30% increase in accuracy when trained with NVIDIA NeMo. NeMo's parallelism, techniques accelerated model training time by 20% when compared to other frameworks. We also announced a partnership with Yum! Brands, the world's largest restaurant company, to bring NVIDIA AI to 500 of its restaurants this year and expanding to 61,000 restaurants over time to streamline order-taking, optimize operations and enhance service across its restaurants.</td><td>NASDAQ는 AI 플랫폼의 검색 기능에서 정확도와 응답 시간이 30% 개선되었습니다. 그리고 Shell의 맞춤형 LLM은 NVIDIA NeMo로 훈련했을 때 정확도가 30% 향상되었습니다. NeMo의 병렬 처리 기법은 다른 프레임워크와 비교했을 때 모델 훈련 시간을 20% 단축시켰습니다. 또한 세계 최대 레스토랑 기업인 Yum! Brands와의 파트너십을 발표했는데, 올해 500개 매장에 NVIDIA AI를 도입하고 향후 61,000개 매장으로 확대하여 주문 접수를 간소화하고 운영을 최적화하며 레스토랑 전반의 서비스를 향상시킬 예정입니다.</td></tr>
<tr><td>For AI-powered cybersecurity leading companies like Check Point, CrowdStrike and Palo Alto Networks are using NVIDIA's AI security and software stack to build, optimize and secure agentic workflows, with CrowdStrike realizing 2x faster detection triage with 50% less compute cost. Moving to networking. Sequential growth in networking resumed in Q1 with revenue up 64% quarter-over-quarter to $5 billion. Our customers continue to leverage our platform to efficiently scale up and scale out AI factory workloads. We created the world's fastest switch, NVLink, for scale up. Our NVLink compute fabric in its fifth generation offers 14x the bandwidth of PCIe Gen 5.</td><td>AI 기반 사이버보안 분야에서 Check Point, CrowdStrike, Palo Alto Networks와 같은 선도 기업들이 NVIDIA의 AI 보안 및 소프트웨어 스택을 활용하여 에이전틱 워크플로우를 구축, 최적화 및 보안하고 있으며, CrowdStrike는 컴퓨팅 비용을 50% 절감하면서도 탐지 분류 속도를 2배 향상시키는 성과를 거두었습니다. <br><br>네트워킹 부문으로 넘어가면, 1분기에 네트워킹의 순차적 성장이 재개되어 매출이 전분기 대비 64% 증가한 50억 달러를 기록했습니다. 고객들은 AI 팩토리 워크로드를 효율적으로 스케일업 및 스케일아웃하기 위해 당사의 플랫폼을 지속적으로 활용하고 있습니다. 당사는 스케일업을 위한 세계에서 가장 빠른 스위치인 NVLink를 개발했습니다. 5세대 NVLink 컴퓨트 패브릭은 PCIe Gen 5 대비 14배의 대역폭을 제공합니다.</td></tr>
<tr><td>NVLink72 carries 130 terabytes per second of bandwidth in a single rack, equivalent to the entirety of the world's peak Internet traffic. NVLink is a new growth vector and is off to a great start with Q1 shipments exceeding $1 billion. At COMPUTEX, we announced NVLink Fusion. Hyperscale customers can now build semi-custom CCUs and accelerators that connect directly to the NVIDIA platform with NVLink. We are now enabling key partners, including ASIC providers such as MediaTek, Marvell, Alchip Technologies and Astera Labs, as well as CPU suppliers, such as Fujitsu and Qualcomm to leverage LVLink Fusion to connect our respective ecosystems.</td><td>NVLink72는 단일 랙에서 초당 130테라바이트의 대역폭을 제공하며, 이는 전 세계 인터넷 트래픽 피크 전체와 맞먹는 수준입니다. NVLink는 새로운 성장 동력이며, 1분기 출하량이 10억 달러를 초과하는 등 훌륭한 출발을 보이고 있습니다. COMPUTEX에서 저희는 NVLink Fusion을 발표했습니다. 이제 하이퍼스케일 고객들은 NVLink를 통해 NVIDIA 플랫폼에 직접 연결되는 세미 커스텀 CCU와 가속기를 구축할 수 있습니다. 저희는 현재 MediaTek, Marvell, Alchip Technologies, Astera Labs와 같은 ASIC 공급업체들과 Fujitsu, Qualcomm과 같은 CPU 공급업체들을 포함한 주요 파트너들이 LVLink Fusion을 활용하여 각자의 생태계를 연결할 수 있도록 지원하고 있습니다.</td></tr>
<tr><td>For scale out, our enhanced Ethernet offerings delivered the highest throughput, lowest latency networking for AI. Spectrum-X posted strong sequential and year-on-year growth and is now annualizing over $8 billion in revenue. Adoption is widespread across major CSPs and consumer Internet companies, including CoreWeave, Microsoft Azure and Oracle Cloud and xAI. This quarter, we added Google Cloud and Meta to the growing list of Spectrum-X customers. We introduced Spectrum-X and Quantum-X silicon photonics switches featuring the world's most advanced co-packaged optics.</td><td>스케일 아웃 측면에서, 당사의 향상된 이더넷 솔루션은 AI를 위한 최고 처리량과 최저 지연시간 네트워킹을 제공했습니다. Spectrum-X는 전분기 대비 및 전년 동기 대비 강력한 성장을 기록했으며, 현재 연간 80억 달러 이상의 매출을 달성하고 있습니다. CoreWeave, Microsoft Azure, Oracle Cloud, xAI를 포함한 주요 CSP(클라우드 서비스 제공업체)와 소비자 인터넷 기업들 사이에서 광범위한 도입이 이루어지고 있습니다. 이번 분기에는 Google Cloud와 Meta가 Spectrum-X 고객 목록에 새롭게 추가되었습니다. 당사는 세계에서 가장 앞선 co-packaged 광학 기술을 특징으로 하는 Spectrum-X와 Quantum-X 실리콘 포토닉스 스위치를 출시했습니다.</td></tr>
<tr><td>These platforms will enable next-level AI factory scaling to millions of GPUs through the increasingly power efficiency by 3.5x and network resiliency by 10x, while accelerating customer time to market by 1.3x. Transitioning to a quick summary of our revenue by geography. China as a percentage of our Data Center revenue was slightly below our expectations and down sequentially due to H20 export licensing controls. For Q2, we expect a meaningful decrease in China Data Center revenue. As a reminder, while Singapore represented nearly 20% of our Q1 billed revenue as many of our large customers use Singapore for centralized invoicing, our products are almost always shipped elsewhere.</td><td>이러한 플랫폼들은 전력 효율성을 3.5배, 네트워크 복원력을 10배 향상시키면서 고객의 시장 출시 시간을 1.3배 단축시켜 수백만 개의 GPU로 차세대 AI 팩토리 확장을 가능하게 할 것입니다.<br><br>지역별 매출에 대한 간략한 요약으로 넘어가겠습니다. 데이터센터 매출에서 중국이 차지하는 비중은 H20 수출 라이선스 통제로 인해 우리의 예상을 약간 하회했으며 전분기 대비 감소했습니다. 2분기에는 중국 데이터센터 매출이 의미 있는 수준으로 감소할 것으로 예상됩니다. 참고로, 싱가포르가 1분기 청구 매출의 거의 20%를 차지했는데, 이는 많은 대형 고객들이 중앙집중식 인보이싱을 위해 싱가포르를 이용하기 때문이며, 우리 제품은 거의 항상 다른 곳으로 배송됩니다.</td></tr>
<tr><td>Note that over 99% of H100, H200 and Blackwell Data Center compute revenue billed to Singapore was for orders from U.S.-based customers. Moving to Gaming and AI PCs. Gaming revenue was a record $3.8 billion, increasing 48% sequentially and 42% year-on-year. Strong adoption by gamers, creators and AI enthusiasts have made Blackwell our fastest ramp ever. Against a backdrop of robust demand, we greatly improved our supply and availability in Q1 and expect to continue these efforts in Q2. AI is transforming PC and creator and gamers. With a 100 million user installed base, GeForce represents the largest footprint for PC developers.</td><td>싱가포르로 청구된 H100, H200 및 Blackwell 데이터센터 컴퓨팅 매출의 99% 이상이 미국 기반 고객들의 주문이었다는 점을 말씀드립니다. <br><br>게이밍 및 AI PC로 넘어가겠습니다. 게이밍 매출은 38억 달러로 기록을 경신했으며, 전분기 대비 48%, 전년 동기 대비 42% 증가했습니다. 게이머, 크리에이터, AI 애호가들의 강력한 채택으로 Blackwell은 우리 역사상 가장 빠른 램프업을 기록했습니다. 견고한 수요를 배경으로, 1분기에 공급과 가용성을 크게 개선했으며 2분기에도 이러한 노력을 지속할 것으로 예상합니다. AI는 PC와 크리에이터, 게이머들을 변화시키고 있습니다. 1억 명의 사용자 설치 기반을 보유한 GeForce는 PC 개발자들에게 가장 큰 기반을 제공합니다.</td></tr>
<tr><td>This quarter, we added to our AI PC laptop offerings, including models capable of running Microsoft's Copilot+. This past quarter, we brought Blackwell architecture to mainstream gaming with its launch of GeForce RTX 5060 and 5060 Ti starting at just $299. The RTX 5060 also debuted in laptop starting at $1,099. These systems that doubled the frame rate/latency. These GeForce RTX 5060 and 5060 Ti desktop GPUs and laptops are now available. In console gaming, the recently unveiled Nintendo Switch 2 leverages NVIDIA's neural rendering and AI technologies, including next-generation custom RTX GPUs with DLSS technology, deliver a giant leap in gaming performance to millions of players worldwide.</td><td>이번 분기에 저희는 Microsoft의 Copilot+를 실행할 수 있는 모델을 포함하여 AI PC 노트북 제품군을 확대했습니다. 지난 분기에는 단 299달러부터 시작하는 GeForce RTX 5060 및 5060 Ti 출시를 통해 Blackwell 아키텍처를 주류 게이밍 시장에 도입했습니다. RTX 5060은 1,099달러부터 시작하는 노트북에도 탑재되었습니다. 이러한 시스템들은 프레임 레이트 대비 지연시간을 두 배로 개선했습니다. 이들 GeForce RTX 5060 및 5060 Ti 데스크톱 GPU와 노트북은 현재 출시되어 있습니다. 콘솔 게이밍 분야에서는 최근 공개된 Nintendo Switch 2가 DLSS 기술을 탑재한 차세대 맞춤형 RTX GPU를 포함하여 NVIDIA의 뉴럴 렌더링 및 AI 기술을 활용함으로써 전 세계 수백만 플레이어들에게 게이밍 성능의 비약적 향상을 제공합니다.</td></tr>
<tr><td>Nintendo has shipped over 150 million Switch consoles to date, making it one of the most successful gaming systems in history. Moving to Pro Visualization. Revenue of $509 million was flat sequentially and up 19% year-on-year. Tariff-related uncertainty temporarily impacted Q1 systems, and demand for our AI workstations is strong, and we expect sequential revenue growth to resume in Q2. NVIDIA DGX Spark and Station revolutionized personal computing by putting the power of an AI supercomputer in a desktop form factor. DGX Spark delivers up to 1 petaflop of AI compute while DGX Station offers an incredible 20 petaflops and is powered by the GB300 Superchip.</td><td>닌텐도는 현재까지 1억 5천만 대 이상의 스위치 콘솔을 출하하여 역사상 가장 성공적인 게임 시스템 중 하나가 되었습니다. 프로 비주얼라이제이션으로 넘어가면, 매출은 5억 900만 달러로 전분기 대비 보합이었고 전년 동기 대비 19% 증가했습니다. 관세 관련 불확실성이 1분기 시스템에 일시적으로 영향을 미쳤으나, AI 워크스테이션에 대한 수요는 강세를 보이고 있으며, 2분기에 순차적 매출 성장이 재개될 것으로 예상합니다. NVIDIA DGX Spark와 Station은 AI 슈퍼컴퓨터의 성능을 데스크톱 폼팩터에 구현함으로써 개인용 컴퓨팅에 혁명을 일으켰습니다. DGX Spark는 최대 1페타플롭의 AI 컴퓨팅 성능을 제공하며, DGX Station은 놀라운 20페타플롭을 제공하고 GB300 슈퍼칩으로 구동됩니다.</td></tr>
<tr><td>DGX Spark will be available in calendar Q3 and DGX Station later this year. We have deepened Omniverse's integration and adoption into some of the world's leading software platforms, including Databricks, SAP and Schneider Electric. New Omniverse Blueprints such as Mega for at-scale robotic fleet management are being leveraged in KION Group, Pegatron, Accenture and other leading companies to enhance industrial operations. At COMPUTEX, we showcased Omniverse's great traction with technology manufacturing leaders, including TSMC, Quanta, Foxconn, Pegatron.</td><td>DGX Spark는 3분기 중에, DGX Station은 올해 말에 출시될 예정입니다. 저희는 Databricks, SAP, Schneider Electric을 포함한 세계 최고의 소프트웨어 플랫폼들과 Omniverse의 통합과 도입을 심화시켰습니다. 대규모 로봇 함대 관리를 위한 Mega와 같은 새로운 Omniverse 블루프린트들이 KION Group, Pegatron, Accenture 및 기타 선도 기업들에서 산업 운영을 향상시키기 위해 활용되고 있습니다. COMPUTEX에서는 TSMC, Quanta, Foxconn, Pegatron을 포함한 기술 제조업계 리더들과 함께 Omniverse의 뛰어난 견인력을 선보였습니다.</td></tr>
<tr><td>Using Omniverse, TSMC saves months in work by designing fabs virtually, Foxconn accelerates thermal simulations by 150x, and Pegatron reduced assembly line defect rates by 67%. Lastly with our Automotive Group. Revenue was $567 million, down 1% sequentially, but up 72% year-on-year. Year-on-year growth was driven by the ramp of self-driving across a number of customers and robust end demand for NEVs. We are partnering with GM to build the next-gen vehicles, factories and robots using NVIDIA AI, simulation and accelerated computing. And we are now in production with our full stack solution for Mercedes-Benz starting with the new CLA, hitting roads in the next few months.</td><td>Omniverse를 활용하여 TSMC는 팹을 가상으로 설계함으로써 수개월의 작업 시간을 절약하고 있으며, Foxconn은 열 시뮬레이션을 150배 가속화하고, Pegatron은 조립라인 불량률을 67% 감소시켰습니다. <br><br>마지막으로 자동차 그룹 실적입니다. 매출은 5억 6,700만 달러로 전분기 대비 1% 감소했지만, 전년 동기 대비 72% 증가했습니다. 전년 동기 대비 성장은 다수 고객사에서 자율주행 기술의 램프업과 신에너지차(NEV)에 대한 견고한 최종 수요에 의해 견인되었습니다. <br><br>우리는 GM과 파트너십을 맺고 NVIDIA AI, 시뮬레이션 및 가속 컴퓨팅을 활용하여 차세대 차량, 공장 및 로봇을 구축하고 있습니다. 그리고 현재 Mercedes-Benz를 위한 풀스택 솔루션을 양산 중이며, 신형 CLA를 시작으로 향후 몇 개월 내에 도로에서 만나볼 수 있을 예정입니다.</td></tr>
<tr><td>We announced Isaac GR00T N1, the world's first open fully customizable foundation model for humanoid robots, enabling generalized reasoning and skill development. We also launched new open NVIDIA Cosmos World Foundation models. Leading companies include 1X, Agility Robots -- Robotics, Figure AI, Uber and Waabi. We've begun integrating Cosmos into their operations for synthetic data generation, while Agility Robotics, Boston Dynamics, and XPENG Robotics are harnessing Isaac's simulation to advance their humanoid efforts. GE Healthcare is using the new NVIDIA Isaac platform for healthcare simulation built on NVIDIA Omniverse and using NVIDIA Cosmos.</td><td>저희는 세계 최초의 완전 개방형 맞춤형 휴머노이드 로봇용 파운데이션 모델인 Isaac GR00T N1을 발표했습니다. 이는 일반화된 추론과 스킬 개발을 가능하게 합니다. 또한 새로운 오픈 NVIDIA Cosmos World Foundation 모델들을 출시했습니다. 주요 기업들로는 1X, Agility Robotics, Figure AI, Uber, Waabi가 있습니다. 이들은 합성 데이터 생성을 위해 Cosmos를 운영에 통합하기 시작했으며, Agility Robotics, Boston Dynamics, XPENG Robotics는 휴머노이드 개발을 진전시키기 위해 Isaac의 시뮬레이션을 활용하고 있습니다. GE Healthcare는 NVIDIA Omniverse를 기반으로 구축되고 NVIDIA Cosmos를 사용하는 헬스케어 시뮬레이션용 새로운 NVIDIA Isaac 플랫폼을 사용하고 있습니다.</td></tr>
<tr><td>The platform speeds development of robotic imaging and surgery systems. The era of robotics is here, billions of robots, hundreds of millions of autonomous vehicles and hundreds of thousands of robotic factories and warehouses will be developed. All right. Moving to the rest of the P&L. GAAP gross margins and non-GAAP gross margins were 60.5% and 61%, respectively. Excluding the $4.5 billion charge, Q1 non-GAAP gross margins would have been 71.3%, slightly below -- above our outlook at the beginning of the quarter. Sequentially, GAAP operating expenses were up 7% and non-GAAP operating expenses were up 6%, reflecting higher compensation and employee growth.</td><td>이 플랫폼은 로봇 이미징 및 수술 시스템 개발을 가속화합니다. 로봇 시대가 도래했으며, 수십억 대의 로봇, 수억 대의 자율주행차량, 그리고 수십만 개의 로봇 공장과 창고가 개발될 것입니다. <br><br>이제 손익계산서의 나머지 부분으로 넘어가겠습니다. GAAP 기준 매출총이익률과 non-GAAP 기준 매출총이익률은 각각 60.5%와 61%였습니다. 45억 달러 비용을 제외할 경우, 1분기 non-GAAP 매출총이익률은 71.3%로 분기 초 전망치를 소폭 상회했습니다. 전분기 대비 GAAP 기준 영업비용은 7% 증가했고, non-GAAP 기준 영업비용은 6% 증가했는데, 이는 보상비용 증가와 직원 수 증가를 반영한 것입니다.</td></tr>
<tr><td>Our investments include expanding our infrastructure capabilities and AI solutions, and we plan to grow these investments throughout the fiscal year. In Q1, we returned a record $14.3 billion to shareholders in the form of share repurchases and cash dividends. Our capital return program continues to be a key element of our capital allocation strategy. Let me turn to the outlook for the second quarter. Total revenue is expected to be $45 billion, plus or minus 2%. We expect modest sequential growth across all of our platforms. In Data Center, we anticipate the continued ramp of Blackwell to be partially offset by a decline in China revenue.</td><td>저희 투자에는 인프라 역량과 AI 솔루션 확장이 포함되어 있으며, 회계연도 전반에 걸쳐 이러한 투자를 늘려갈 계획입니다. 1분기에 저희는 자사주 매입과 현금 배당의 형태로 주주들에게 기록적인 143억 달러를 환원했습니다. 저희의 자본 환원 프로그램은 자본 배분 전략의 핵심 요소로 계속 유지되고 있습니다. <br><br>2분기 전망에 대해 말씀드리겠습니다. 총 매출은 450억 달러, 플러스 마이너스 2%로 예상됩니다. 저희는 모든 플랫폼에서 완만한 순차적 성장을 기대하고 있습니다. 데이터센터 부문에서는 Blackwell의 지속적인 램프업이 중국 매출 감소로 부분적으로 상쇄될 것으로 예상합니다.</td></tr>
<tr><td>Note, our outlook reflects a loss in H20 revenue of approximately $8 billion for the second quarter. GAAP and non-GAAP gross margins are expected to be 71.8% and 72%, respectively, plus or minus 50 basis points. We expect or Blackwell profitability to drive modest sequential improvement in gross margins. We are continuing to work towards achieving gross margins in the mid-70%s range late this year. GAAP and non-GAAP operating expenses are expected to be approximately $5.7 billion and $4 billion, respectively, and we continue to expect full year fiscal year '26 operating expense growth to be in the mid-30% range.</td><td>참고로, 저희 전망에는 2분기 H20 매출 손실이 약 80억 달러 반영되어 있습니다. GAAP 및 non-GAAP 총이익률은 각각 71.8%와 72%로 예상되며, 오차범위는 플러스마이너스 50베이시스포인트입니다. 블랙웰의 수익성이 총이익률의 순차적 개선을 견인할 것으로 기대합니다. 저희는 올해 말 총이익률을 70% 중반대 달성을 목표로 지속적으로 노력하고 있습니다. GAAP 및 non-GAAP 운영비용은 각각 약 57억 달러와 40억 달러로 예상되며, 26회계연도 전체 운영비용 증가율은 30% 중반대가 될 것으로 계속 예상하고 있습니다.</td></tr>
<tr><td>GAAP and non-GAAP other income and expenses are expected to be an income of approximately $450 million, excluding gains and losses from non-marketable and publicly-held equity securities. GAAP and non-GAAP tax rates are expected to be 16.5%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new Financial Information AI Agent. Let me highlight upcoming events for the financial community.</td><td>GAAP 및 non-GAAP 기타 수익 및 비용은 비상장 및 공개 지분증권의 손익을 제외하고 약 4억 5천만 달러의 수익이 될 것으로 예상됩니다. GAAP 및 non-GAAP 세율은 개별 항목을 제외하고 16.5% 플러스 마이너스 1%가 될 것으로 예상됩니다. 새로운 Financial Information AI Agent를 포함하여 CFO 코멘터리 및 기타 정보가 당사 IR 웹사이트에서 제공되는 추가적인 재무 세부사항이 포함되어 있습니다. 금융 커뮤니티를 위한 향후 이벤트들을 강조해 드리겠습니다.</td></tr>
<tr><td>We will be at the BofA Global Technology Conference in San Francisco on June 4th; the Rosenblatt Virtual AI Summit and NASDAQ Investor Conference in London on June 10; and GTC Paris at VivaTech on June 11th in Paris. We look forward to seeing you at these events. Our earnings call to discuss the results of our second quarter of fiscal 2026 is scheduled for August 27. Well, now, let me turn it over to Jensen to make some remarks. Jensen Huang<br><br>Thanks, Colette. We've had a busy and productive year. Let me share my perspective on some topics we're frequently asked. On export control: China is one of the world's largest AI markets and a springboard to global success.</td><td>저희는 6월 4일 샌프란시스코에서 열리는 BofA 글로벌 테크놀로지 컨퍼런스, 6월 10일 런던에서 열리는 로젠블랫 버추얼 AI 서밋과 나스닥 투자자 컨퍼런스, 그리고 6월 11일 파리에서 열리는 비바테크의 GTC 파리에 참석할 예정입니다. 이러한 행사에서 여러분을 만나뵐 수 있기를 기대합니다. 2026 회계연도 2분기 실적을 논의하는 어닝콜은 8월 27일로 예정되어 있습니다. 그럼 이제 젠슨에게 발언권을 넘겨드리겠습니다.<br><br>젠슨 황<br><br>고마워요, 콜레트. 저희는 바쁘고 생산적인 한 해를 보냈습니다. 자주 받는 질문들에 대한 제 관점을 공유해드리겠습니다. 수출 통제에 관해서는: 중국은 세계 최대 AI 시장 중 하나이며 글로벌 성공을 위한 발판입니다.</td></tr>
<tr><td>With half of the world's AI researchers based there, the platform that wins China is positioned to lead globally. Today, however, the $50 billion China market is effectively closed to U.S. industry. The H20 export ban ended our Hopper Data Center business in China. We cannot reduce Hopper further to comply. As a result, we are taking a multibillion-dollar write-off on inventory that cannot be sold or repurposed. We are exploring limited ways to compete, but Hopper is no longer an option. China's AI moves on with or without U.S. chips. It has to compute to train and deploy advanced models. The question is not whether China will have AI, it already does.</td><td>전 세계 AI 연구자의 절반이 중국에 기반을 두고 있는 상황에서, 중국 시장을 선점하는 플랫폼이 글로벌 시장을 주도할 수 있는 위치에 있습니다. 그러나 현재 500억 달러 규모의 중국 시장은 사실상 미국 업계에 폐쇄되어 있습니다. H20 수출 금지 조치로 인해 중국에서의 Hopper 데이터센터 사업이 중단되었습니다. 규제 준수를 위해 Hopper를 더 이상 축소할 수 없는 상황입니다. 그 결과, 판매하거나 용도 변경할 수 없는 재고에 대해 수십억 달러 규모의 손상차손을 계상하고 있습니다. 제한적인 경쟁 방안을 모색하고 있지만, Hopper는 더 이상 선택지가 아닙니다. 중국의 AI는 미국 칩의 유무와 관계없이 계속 발전하고 있습니다. 고급 모델을 훈련하고 배포하기 위해서는 컴퓨팅이 필요하기 때문입니다. 문제는 중국이 AI를 보유할 것인지 여부가 아니라, 이미 보유하고 있다는 것입니다.</td></tr>
<tr><td>The question is whether one of the world's largest AI markets will run on American platforms. Shielding Chinese chipmakers from U.S. competition only strengthens them abroad and weakens America's position. Export restrictions have spurred China's innovation and scale. The AI race is not just about chips. It's about which stack the world runs on. As that stack grows to include 6G and quantum, U.S. global infrastructure leadership is at stake. The U.S. has based its policy on the assumption that China cannot make AI chips. That assumption was always questionable and now it's clearly wrong. China has enormous manufacturing capability.</td><td>문제는 세계 최대 AI 시장 중 하나가 미국 플랫폼에서 운영될 것인지 여부입니다. 중국 반도체 제조업체를 미국의 경쟁으로부터 보호하는 것은 해외에서 이들을 더욱 강화시키고 미국의 입지를 약화시킬 뿐입니다. 수출 제한 조치는 중국의 혁신과 규모 확장을 촉진했습니다. AI 경쟁은 단순히 칩에 관한 것이 아닙니다. 세계가 어떤 스택에서 운영될 것인지에 관한 문제입니다. 그 스택이 6G와 양자 기술을 포함하도록 확장됨에 따라, 미국의 글로벌 인프라 리더십이 위험에 처해 있습니다. 미국은 중국이 AI 칩을 만들 수 없다는 가정에 기반해 정책을 수립해왔습니다. 그 가정은 항상 의문스러웠으며 이제는 명백히 틀렸습니다. 중국은 엄청난 제조 역량을 보유하고 있습니다.</td></tr>
<tr><td>In the end, the platform that wins the AI developers win AI -- wins AI. Export controls should strengthen U.S. platforms, not drive half of the world's AI talent to rivals. On DeepSeek: DeepSeek and Qwen from China are among the most -- among the best open-source AI models. Released freely, they've gained traction across the U.S., Europe and beyond. DeepSeek-R1, like ChatGPT, introduced reasoning AI that produces better answers, the longer it thinks. Reasoning AI enables step-by-step problem solving, planning and tool use, turning models into intelligent agents.</td><td>결국 AI 개발자들이 승리하는 플랫폼이 AI에서 승리하게 됩니다. 수출 통제는 미국 플랫폼을 강화해야 하며, 전 세계 AI 인재의 절반을 경쟁사로 내몰아서는 안 됩니다. DeepSeek에 대해 말씀드리면: 중국의 DeepSeek과 Qwen은 최고의 오픈소스 AI 모델 중 하나입니다. 무료로 공개되어 미국, 유럽 및 그 밖의 지역에서 주목을 받고 있습니다. DeepSeek-R1은 ChatGPT와 마찬가지로 더 오래 생각할수록 더 나은 답변을 제공하는 추론 AI를 도입했습니다. 추론 AI는 단계별 문제 해결, 계획 수립 및 도구 사용을 가능하게 하여 모델을 지능형 에이전트로 전환시킵니다.</td></tr>
<tr><td>Reasoning is compute-intensive, requires hundreds to thousands more -- thousands of times more tokens per task than previous one-shot inference. Reasoning models are driving a step-function surge in inference demand. AI scaling laws remain firmly intact, not only for training, but now inference too requires massive scale compute. DeepSeek also underscores the strategic value of open-source AI. When popular models are trained and optimized on U.S. platforms, it drives usage, feedback and continuous improvement, reinforcing American leadership across the stack. U.S. platforms must remain the preferred platform for open-source AI.</td><td>추론은 컴퓨팅 집약적이며, 기존의 원샷 추론 대비 작업당 수백 배에서 수천 배 더 많은 토큰을 필요로 합니다. 추론 모델들이 추론 수요의 단계적 급증을 견인하고 있습니다. AI 스케일링 법칙은 훈련뿐만 아니라 추론에서도 대규모 컴퓨팅이 필요하다는 점에서 여전히 확고히 유지되고 있습니다. DeepSeek은 또한 오픈소스 AI의 전략적 가치를 부각시킵니다. 인기 있는 모델들이 미국 플랫폼에서 훈련되고 최적화될 때, 이는 사용량, 피드백, 그리고 지속적인 개선을 촉진하여 전체 스택에 걸친 미국의 리더십을 강화합니다. 미국 플랫폼은 오픈소스 AI를 위한 선호 플랫폼으로 남아있어야 합니다.</td></tr>
<tr><td>That means supporting collaboration with top developers globally, including in China. America wins when models like DeepSeek and Qwen runs best on American infrastructure. Regarding onshore manufacturing: President Trump has outlined a bold vision to reshore advanced manufacturing, create jobs and strengthen national security. Future plants will be highly computerized in robotics. We share this vision. TSMC is building six fabs and two advanced packaging plants in Arizona to make chips for NVIDIA. Process qualification is underway with volume production expected by year-end. SPIL and Amkor are also investing in Arizona, constructing packaging, assembly and test facilities.</td><td>이는 중국을 포함한 전 세계 최고 개발자들과의 협력을 지원한다는 의미입니다. DeepSeek과 Qwen 같은 모델들이 미국 인프라에서 최적으로 구동될 때 미국이 승리하는 것입니다. <br><br>국내 제조업에 관해서는: 트럼프 대통령이 첨단 제조업 리쇼어링, 일자리 창출, 국가 안보 강화를 위한 대담한 비전을 제시했습니다. 미래의 공장들은 로보틱스 분야에서 고도로 전산화될 것입니다. 저희도 이러한 비전을 공유합니다. <br><br>TSMC는 NVIDIA용 칩을 제조하기 위해 애리조나에 6개의 팹과 2개의 첨단 패키징 공장을 건설하고 있습니다. 공정 검증이 진행 중이며 연말까지 양산이 예상됩니다. SPIL과 Amkor 역시 애리조나에 투자하여 패키징, 조립 및 테스트 시설을 건설하고 있습니다.</td></tr>
<tr><td>In Houston, we're partnering with Foxconn to construct a 1 million square foot factory to build AI supercomputers. Wistron is building a similar plant in Fort Worth, Texas. To encourage and support these investments, we've made substantial long-term purchase commitments, a deep investment in America's AI manufacturing future. Our goal from chip to supercomputer built in America within a year. Each GB200 NVLink72 racks contains 1.2 million components and weighs nearly 2 tons. No one has produced supercomputers on this scale. Our partners are doing an extraordinary job.</td><td>휴스턴에서는 Foxconn과 파트너십을 맺고 AI 슈퍼컴퓨터를 제조할 100만 평방피트 규모의 공장을 건설하고 있습니다. Wistron은 텍사스주 포트워스에 유사한 규모의 공장을 건설하고 있습니다. 이러한 투자를 장려하고 지원하기 위해 저희는 상당한 규모의 장기 구매 약정을 체결했으며, 이는 미국 AI 제조업의 미래에 대한 깊이 있는 투자입니다. 저희의 목표는 칩부터 슈퍼컴퓨터까지 1년 내에 미국에서 제조하는 것입니다. 각 GB200 NVLink72 랙은 120만 개의 부품을 포함하고 있으며 무게는 거의 2톤에 달합니다. 아무도 이 규모의 슈퍼컴퓨터를 생산한 적이 없습니다. 저희 파트너들이 놀라운 성과를 보여주고 있습니다.</td></tr>
<tr><td>On AI diffusion rule: President Trump rescinded the AI diffusion rule, calling it counterproductive, and proposed a new policy to promote U.S. AI tech with trusted partners. On his Middle East tour, he announced historic investments. I was honored to join him in announcing a 500-megawatt AI infrastructure project in Saudi Arabia and a 5-gigawatt AI campus in the UAE. President Trump wants U.S. tech to lead. The deals he announced are wins for America, creating jobs, advancing infrastructure, generating tax revenue and reducing the U.S. trade deficit. The U.S. will always be NVIDIA's largest market and home to the largest installed base of our infrastructure.</td><td>AI 확산 규칙에 관해서: 트럼프 대통령은 AI 확산 규칙이 역효과를 낳는다며 이를 철회하고, 신뢰할 수 있는 파트너들과 함께 미국 AI 기술을 촉진하는 새로운 정책을 제안했습니다. 중동 순방에서 그는 역사적인 투자를 발표했습니다. 저는 사우디아라비아의 500메가와트 AI 인프라 프로젝트와 UAE의 5기가와트 AI 캠퍼스 발표에 함께 참여하게 되어 영광이었습니다. 트럼프 대통령은 미국 기술이 선도하기를 원합니다. 그가 발표한 거래들은 일자리 창출, 인프라 발전, 세수 증대, 그리고 미국 무역적자 감소를 통해 미국에게 승리를 가져다줍니다. 미국은 항상 NVIDIA의 최대 시장이자 우리 인프라의 최대 설치 기반을 보유한 본거지가 될 것입니다.</td></tr>
<tr><td>Every nation now sees AI as core to the next industrial revolution, a new industry that produces intelligence and essential infrastructure for every economy. Countries are racing to build national AI platforms to elevate their digital capabilities. At COMPUTEX, we announced Taiwan's first AI factory in partnership with Foxconn and the Taiwan government. Last week, I was in Sweden to launch its first national AI infrastructure. Japan, Korea, India, Canada, France, the U.K., Germany, Italy, Spain, and more are now building national AI factories to empower startups, industries and societies. Sovereign AI is a new growth engine for NVIDIA. Toshiya, back to you. Thank you.</td><td>이제 모든 국가가 AI를 차세대 산업혁명의 핵심으로 인식하고 있으며, 이는 지능을 생산하고 모든 경제에 필수적인 인프라를 제공하는 새로운 산업입니다. 각국은 디지털 역량을 향상시키기 위해 국가 AI 플랫폼 구축 경쟁을 벌이고 있습니다. COMPUTEX에서 저희는 폭스콘과 대만 정부와의 파트너십을 통해 대만 최초의 AI 팩토리를 발표했습니다. 지난주에는 스웨덴을 방문하여 스웨덴 최초의 국가 AI 인프라를 출범시켰습니다. 일본, 한국, 인도, 캐나다, 프랑스, 영국, 독일, 이탈리아, 스페인 등 더 많은 국가들이 현재 스타트업, 산업계, 그리고 사회 전반에 힘을 실어주기 위한 국가 AI 팩토리를 구축하고 있습니다. 소버린 AI는 NVIDIA의 새로운 성장 동력입니다. 토시야, 다시 넘겨드리겠습니다. 감사합니다.</td></tr>
<tr><td>Toshiya Hari<br><br>Operator, we will now open the call for questions. Would you please poll for questions?</td><td>토시야 하리<br><br>교환원님, 이제 질의응답 시간을 시작하겠습니다. 질문을 받아주시기 바랍니다.</td></tr>
    </table>
    <h3>📌 요약</h3>
    <p style="background:#f0f0f0; padding:15px; border-left: 5px solid #333;">Here are the key points from NVIDIA's Q1 2026 earnings call in Korean:<br><br>• 실적 하이라이트:<br>- 매출 440억 달러 (전년 대비 69% 증가)<br>- 데이터센터 부문 매출 390억 달러 (전년 대비 73% 증가)<br>- 블랙웰(Blackwell) GPU가 데이터센터 컴퓨팅 매출의 70% 차지<br><br>• 중국 관련 이슈:<br>- H20 GPU 수출 통제로 45억 달러 재고 상각 비용 발생<br>- 중국 AI 가속기 시장(약 500억 달러 규모) 접근 제한으로 인한 사업 영향 우려<br>- 중국 시장 대응을 위한 제한적 옵션 검토 중<br><br>• 향후 전망:<br>- Q2 매출 가이던스 450억 달러 (±2%)<br>- 블랙웰 GPU 출하 증가로 전반적인 실적 개선 전망<br>- 그러나 중국 매출 감소로 인한 부분적 상쇄 예상<br>- 연말까지 총이익률 mid-70% 달성 목표<br><br>• 전략적 포인트:<br>- AI 추론 수요의 급격한 증가 확인<br>- 글로벌 AI 인프라 구축 가속화 (약 100개의 NVIDIA 기반 AI 팩토리 진행 중)<br>- 자율주행, 로봇공학 등 신규 성장 동력 확대</p>
    <hr style="margin:50px 0;">
    

    <h2>❓ Q&A</h2>
    <table style="width:100%; border-collapse:collapse; margin-bottom: 40px;">
        <tr>
            <th style="width:50%; border-bottom: 2px solid #333;">Original</th>
            <th style="width:50%; border-bottom: 2px solid #333;">Translation</th>
        </tr>
        <tr><td>Question-and-Answer Session<br><br>Operator<br><br>Thank you. [Operator Instructions] Your first question comes from the line of Joe Moore with Morgan Stanley. Your line is open. Joe Moore<br><br>Great. Thank you. You guys have talked about this scaling up of inference around reasoning models for at least a year now. And we've really seen that come to fruition as you talked about. We've heard it from your customers. Can you give us a sense for how much of that demand you're able to serve and give us a sense for maybe how big the inference business is for you guys? And do we need full-on NVL72 rack scale solutions for reasoning inference going forward?</td><td>질의응답 세션<br><br>운영자<br><br>감사합니다. [운영자 안내] 첫 번째 질문은 모건 스탠리의 조 무어(Joe Moore)님으로부터 받겠습니다. 연결되었습니다.<br><br>조 무어<br><br>감사합니다. 여러분께서는 추론 모델을 중심으로 한 추론 확장에 대해 최소 1년 동안 이야기해 오셨습니다. 그리고 말씀하신 대로 실제로 그것이 결실을 맺는 것을 보았습니다. 고객들로부터도 들었고요. 그러한 수요 중 얼마나 많은 부분을 서비스할 수 있는지, 그리고 추론 사업이 여러분에게 얼마나 큰 규모인지에 대한 감을 주실 수 있나요? 그리고 앞으로 추론 인퍼런스를 위해서는 완전한 NVL72 랙 스케일 솔루션이 필요한가요?</td></tr>
<tr><td>Jensen Huang<br><br>Well, we would like to serve all of it, and I think we're on track to serve most of it. Grace Blackwell NVLink72 is the ideal engine today, the ideal computer thinking machine, if you will, for reasoning AI. There's a couple of reasons for that. The first reason is that the token generation amount, the number of tokens reasoning goes through, is 100 times, 1,000 times more than a one-shot chatbot. It's essentially thinking to itself, breaking down a problem step-by-step. It might be planning multiple paths to an answer. It could be using tools, reading PDFs, reading web pages, watching videos, and then producing a result, an answer.</td><td>젠슨 황<br><br>저희는 모든 시장을 서비스하고 싶으며, 대부분을 서비스할 수 있는 궤도에 있다고 생각합니다. Grace Blackwell NVLink72는 현재 추론 AI를 위한 이상적인 엔진이자, 말하자면 이상적인 컴퓨터 사고 머신입니다. 이에는 몇 가지 이유가 있습니다. 첫 번째 이유는 토큰 생성량, 즉 추론이 거치는 토큰의 수가 일회성 챗봇보다 100배, 1,000배 더 많다는 것입니다. 본질적으로 스스로 사고하면서 문제를 단계별로 분해합니다. 답에 대한 여러 경로를 계획할 수도 있고, 도구를 사용하거나 PDF를 읽고, 웹페이지를 읽고, 비디오를 시청한 다음 결과나 답변을 생성할 수도 있습니다.</td></tr>
<tr><td>The longer it thinks, the better the answer, the smarter the answer is. And so, what we would like to do, and the reason why Grace Blackwell was designed to give such a giant step-up in inference performance, is so that you could do all this and still get a response as quickly as possible. Compared to Hopper, Grace Blackwell is some 40 times higher speed and throughput compared. And so, this is going to be a huge, huge benefit in driving down the cost while improving the quality of response with excellent quality of service at the same time. So, that's the fundamental reason. That was the core driving reason for Grace Blackwell NVLink72.</td><td>더 오래 생각할수록 더 나은 답변, 더 똑똑한 답변을 얻을 수 있습니다. 그래서 저희가 하고자 하는 것, 그리고 Grace Blackwell이 추론 성능에서 이렇게 거대한 도약을 제공하도록 설계된 이유는 이 모든 것을 수행하면서도 여전히 가능한 한 빠르게 응답을 받을 수 있도록 하기 위함입니다. Hopper와 비교했을 때, Grace Blackwell은 속도와 처리량 면에서 약 40배 더 높은 성능을 보입니다. 따라서 이는 비용을 대폭 절감하는 동시에 응답 품질을 향상시키고 뛰어난 서비스 품질을 제공하는 데 있어 엄청난, 정말 엄청난 이익이 될 것입니다. 이것이 바로 근본적인 이유입니다. 이것이 Grace Blackwell NVLink72의 핵심 추진 동력이었습니다.</td></tr>
<tr><td>Of course, in order to do that, we had to reinvent, literally redesign the entire way that these supercomputers are built, and -- but now we're in full production. It's going to be exciting. It's going to be incredibly exciting. Operator<br><br>The next question comes from Vivek Arya with Bank of America Securities. Your line is open. Vivek Arya<br><br>Thanks for the question. Just a clarification for Colette first. So, on the China impact, I think previously, it was mentioned at about $15 billion, so you had the $8 billion in Q2. So, is there still some left as a headwind for the remaining quarters? Just, Colette, how to model that? And then a question, Jensen, for you.</td><td>물론, 그렇게 하기 위해서는 이러한 슈퍼컴퓨터가 구축되는 전체 방식을 말 그대로 재발명하고 완전히 재설계해야 했습니다. 하지만 이제 우리는 본격적인 생산에 들어갔습니다. 정말 흥미진진할 것입니다. 엄청나게 흥미진진할 것입니다.<br><br>오퍼레이터<br><br>다음 질문은 Bank of America Securities의 Vivek Arya님께서 주셨습니다. 연결되었습니다.<br><br>Vivek Arya<br><br>질문 기회를 주셔서 감사합니다. 먼저 Colette님께 확인 차 여쭤보겠습니다. 중국 영향과 관련해서, 이전에 약 150억 달러 정도로 언급되었던 것으로 기억하는데, 2분기에 80억 달러가 있었습니다. 그렇다면 나머지 분기들에 대해서도 여전히 역풍 요인이 남아있는 건가요? Colette님, 이 부분을 어떻게 모델링해야 할까요? 그리고 Jensen님께 질문이 있습니다.</td></tr>
<tr><td>Back at GTC, you had outlined a path towards almost $1 trillion of AI spending over the next few years. Where are we in that build-out? And do you think it's going to be uniform that you will see every spender, whether it's CSP, sovereigns, enterprises or build-out should we expect some periods of digestion in between? Just what are your customer discussions telling you about how to model growth for next year? Colette Kress<br><br>Yes, Vivek. Thanks so much for the question regarding H20. Yes, we recognized $4.6 billion H20 in Q1. We were unable to ship $2.5 billion, so the total for Q1 should have been $7 billion.</td><td>GTC에서 향후 몇 년간 거의 1조 달러 규모의 AI 지출로 향하는 경로를 제시하셨는데요. 현재 그 구축 과정에서 어느 단계에 있나요? 그리고 CSP든, 국가 주권 펀드든, 기업이든 모든 지출 주체가 균등하게 구축할 것으로 보시는지, 아니면 중간에 소화 기간들이 있을 것으로 예상해야 하는지요? 내년 성장을 모델링하는 방법에 대해 고객들과의 논의에서 어떤 이야기들이 나오고 있나요?<br><br>콜레트 크레스<br><br>네, 비벡. H20 관련 질문 감사합니다. 네, 저희는 1분기에 H20에서 46억 달러를 인식했습니다. 25억 달러는 출하하지 못했기 때문에, 1분기 총액은 70억 달러였어야 합니다.</td></tr>
<tr><td>When we look at our Q2, our Q2 is going to be meaningfully down in terms of China Data Center revenue. And we had highlighted in terms of the amount of orders that we had planned for H20 in Q2, and that was $8 billion. Now, going forward, we did have other orders going forward that we will not be able to fulfill. That is what was incorporated, therefore, in the amount that we wrote down of the $4.5 billion. That write-down was about inventory and purchase commitments, and our purchase commitments were about what we expected regarding the orders that we had received. Going forward, though, it's a bigger issue regarding the amount of the market that we will not be able to serve.</td><td>2분기를 살펴보면, 중국 데이터센터 매출이 의미 있는 수준으로 감소할 예정입니다. 그리고 저희가 2분기 H20 관련해서 계획했던 주문량에 대해 말씀드렸는데, 그 규모는 80억 달러였습니다. 앞으로를 보면, 저희가 이행할 수 없는 다른 주문들이 있었습니다. 이것이 바로 저희가 45억 달러 규모로 감액한 금액에 반영된 내용입니다. 이 감액은 재고자산과 구매약정에 관한 것이었으며, 저희 구매약정은 접수한 주문과 관련해서 예상했던 부분에 대한 것이었습니다. 하지만 앞으로는 저희가 서비스할 수 없는 시장 규모와 관련해서 더 큰 이슈가 될 것입니다.</td></tr>
<tr><td>We assess that TAM to be close to about $50 billion in the future as we don't have a product to enable for China. Jensen Huang<br><br>Vivek, the -- probably the best way to think through it is that AI is several things. Of course, we know that AI is this incredible technology that's going to transform every industry, from, of course, the way we do software to healthcare and financial services to retail to, I guess, every industry, transportation, manufacturing. And we're at the beginning of that. But maybe another way to think about that is where do we need intelligence, where do we need digital intelligence? And it's in every country, it's in every industry.</td><td>저희는 중국 시장을 위한 제품이 없기 때문에 향후 TAM이 약 500억 달러 수준에 근접할 것으로 평가하고 있습니다. 젠슨 황<br><br>비벡, 이를 생각해보는 가장 좋은 방법은 AI가 여러 가지 의미를 갖는다는 것입니다. 물론 우리는 AI가 소프트웨어 개발 방식부터 헬스케어, 금융 서비스, 리테일, 그리고 모든 산업 - 운송, 제조업에 이르기까지 모든 산업을 변화시킬 놀라운 기술이라는 것을 알고 있습니다. 그리고 우리는 그 시작점에 있습니다. 하지만 이를 다른 방식으로 생각해보면, 우리가 지능이 필요한 곳, 디지털 인텔리전스가 필요한 곳이 어디인가 하는 것입니다. 그것은 모든 국가, 모든 산업에 해당됩니다.</td></tr>
<tr><td>And we know because of that, we recognize that AI is also an infrastructure. It's a way of developing a technology -- delivering a technology that requires factories and these factories produce tokens. And they, as I mentioned, are important to every single industry and every single country. And so, on that basis, we're really at the very beginning of it, because the adoption of this technology is really kind of in its early, early stages. Now, we've reached an extraordinary milestone with AIs that are reasoning, are thinking, what people call inference time scaling.</td><td>그리고 우리는 그것 때문에 AI 역시 인프라라는 것을 인식하고 있습니다. AI는 기술을 개발하는 방식이자 공장을 필요로 하는 기술을 제공하는 방식이며, 이러한 공장들은 토큰을 생산합니다. 그리고 제가 언급했듯이, 이는 모든 산업과 모든 국가에 중요합니다. 따라서 이러한 근거에서 우리는 정말로 시작 단계에 있다고 할 수 있습니다. 왜냐하면 이 기술의 도입이 정말로 초기, 초기 단계에 있기 때문입니다. 이제 우리는 추론하고 사고하는 AI, 사람들이 추론 시간 스케일링(inference time scaling)이라고 부르는 것과 함께 놀라운 이정표에 도달했습니다.</td></tr>
<tr><td>Of course, it created a whole new -- we've entered an era where inference is going to be a significant part of the compute workload. But anyhow, it's going to be a new infrastructure, and we're building it out in the cloud. The United States is really the early starter and available in U.S. clouds. And this is our largest market, our largest installed base, and we continue to see that happening. But beyond that, we're going to have to -- we're going to see AI go into enterprise, which is on-prem, because so much of the data is still on-prem. Access control is really important. It's really hard to move all of -- every company's data into the cloud.</td><td>물론, 이는 완전히 새로운 상황을 만들어냈습니다. 우리는 추론(inference)이 컴퓨팅 워크로드의 상당 부분을 차지하게 될 시대에 진입했습니다. 어쨌든, 이는 새로운 인프라가 될 것이며, 우리는 클라우드에서 이를 구축하고 있습니다. 미국이 정말로 초기 선발주자이고 미국 클라우드에서 이용 가능합니다. 그리고 이곳은 우리의 최대 시장이자 최대 설치 기반이며, 우리는 이러한 상황이 계속되는 것을 보고 있습니다. 하지만 그 이상으로, 우리는 AI가 기업으로 진입하는 것을 보게 될 것입니다. 이는 온프레미스(on-prem) 환경인데, 왜냐하면 데이터의 상당 부분이 여전히 온프레미스에 있기 때문입니다. 접근 제어가 정말 중요합니다. 모든 회사의 데이터를 클라우드로 옮기는 것은 정말 어려운 일입니다.</td></tr>
<tr><td>And so, we're going to move AI into the enterprise. And you saw that we announced a couple of really exciting new products, our RTX Pro Enterprise AI server that runs everything enterprise and AI, our DGX Spark and DGX Station, which is designed for developers who want to work on-prem. And so, enterprise AI is just taking off. Telcos, today, a lot of the telco infrastructure will be in the future software-defined and built on AI. And so, 6G is going to be built on AI, and that infrastructure needs to be built out. And as said, it's very, very early stages. And then, of course, every factory today that makes things will have an AI factory that sits with it.</td><td>그래서 저희는 AI를 기업으로 확장해 나갈 것입니다. 저희가 발표한 몇 가지 정말 흥미로운 신제품들을 보셨을 텐데요, 기업용 및 AI 모든 것을 실행하는 RTX Pro Enterprise AI 서버, 그리고 온프레미스에서 작업하고자 하는 개발자들을 위해 설계된 DGX Spark와 DGX Station이 있습니다. 그래서 기업용 AI가 막 도약하고 있습니다. 통신사들의 경우, 오늘날 통신 인프라의 상당 부분이 미래에는 소프트웨어 정의 방식으로 구축되고 AI를 기반으로 할 것입니다. 그래서 6G는 AI를 기반으로 구축될 것이며, 그 인프라가 구축되어야 합니다. 말씀드린 바와 같이, 이는 매우 초기 단계입니다. 그리고 물론, 오늘날 제품을 생산하는 모든 공장은 함께 운영되는 AI 팩토리를 갖게 될 것입니다.</td></tr>
<tr><td>And the AI factory is going to be -- drive creating AI and operating AI for the factory itself, but also to power the products and the things that are made by the factory. So, it's very clear that every car company will have AI factories. And very soon, there'll be robotics companies -- robot companies, and those companies will be also building AIs to drive the robots. And so, we're at the beginning of all of this build-out. Operator<br><br>The next question comes from CJ Muse with Cantor Fitzgerald. Your line is open. CJ Muse<br><br>Yeah, good afternoon. Thank you for taking the question.</td><td>AI 팩토리는 팩토리 자체를 위한 AI 생성 및 운영을 추진할 뿐만 아니라, 팩토리에서 만들어지는 제품과 물건들에 동력을 제공할 것입니다. 따라서 모든 자동차 회사가 AI 팩토리를 보유하게 될 것이 매우 명확합니다. 그리고 곧 로보틱스 회사들, 즉 로봇 회사들이 생겨날 것이며, 이러한 회사들도 로봇을 구동하기 위한 AI를 구축할 것입니다. 따라서 우리는 이 모든 구축의 시작점에 있습니다.<br><br>오퍼레이터<br><br>다음 질문은 Cantor Fitzgerald의 CJ Muse님으로부터 받겠습니다. 연결되었습니다.<br><br>CJ Muse<br><br>네, 안녕하세요. 질문을 받아주셔서 감사합니다.</td></tr>
<tr><td>There have been many large GPU cluster investment announcements in the last month, and you alluded to a few of them with Saudi Arabia, the UAE, and then, also, we heard from Oracle and xAI, just to name a few. So, my question, are there other that have yet to be announced of the same kind of scale and magnitude? And perhaps more importantly, how are these orders impacting your lead times for Blackwell and your current visibility sitting here today almost halfway through 2025? Jensen Huang<br><br>Well, we have more orders today than we did at the last time I spoke about orders at GTC. However, we're also increasing our supply chain and building out our supply chain. They're doing a fantastic job.</td><td>지난 한 달 동안 대규모 GPU 클러스터 투자 발표가 많이 있었고, 사우디아라비아, UAE와 관련해서 몇 가지 언급해 주셨고, 또한 Oracle과 xAI에서도 발표가 있었습니다. 그래서 제 질문은, 아직 발표되지 않은 동일한 규모와 크기의 다른 주문들이 있는지요? 그리고 아마도 더 중요한 것은, 이러한 주문들이 Blackwell의 리드타임과 2025년 거의 중반에 와 있는 현재 시점에서의 가시성에 어떤 영향을 미치고 있는지입니다?<br><br>**젠슨 황**<br><br>음, 현재 우리가 보유한 주문량은 제가 GTC에서 마지막으로 주문에 대해 말씀드렸을 때보다 더 많습니다. 하지만 우리도 공급망을 확대하고 구축하고 있습니다. 공급망 팀이 정말 훌륭한 일을 하고 있습니다.</td></tr>
<tr><td>We're building it here onshore in the United States, but we're going to keep our supply chain quite busy for several -- many more years coming. And with respect to further announcements, I'm going to be on the road next week through Europe. And it's -- just about every country needs to build out AI infrastructure and their umpteenth AI factories being planned. I think in the remarks, Colette mentioned there's some 100 AI factories being built. There's a whole bunch that haven't been announced.</td><td>우리는 미국 내에서 이를 구축하고 있지만, 향후 몇 년간 공급망을 상당히 바쁘게 유지할 예정입니다. 추가 발표와 관련해서는, 다음 주에 유럽을 순방할 예정입니다. 거의 모든 국가가 AI 인프라를 구축해야 하고, 수많은 AI 팩토리들이 계획되고 있습니다. 발언 중에 콜레트가 약 100개의 AI 팩토리가 건설되고 있다고 언급했는데, 아직 발표되지 않은 것들도 상당히 많습니다.</td></tr>
<tr><td>And I think the important concept here, which makes it easier to understand, is that like other technologies that impact literally every single industry, of course, electricity was one and it became infrastructure. Of course, the information infrastructure, which we now know as the Internet affects every single industry, every country, every society. Intelligence is surely one of those things. I don't know any company, industry, country who thinks that intelligence is optional. It's essential infrastructure. And so, we've now digitalized intelligence. And so, I think we're clearly in the beginning of the build-out of this infrastructure. And every country will have it, I'm certain of that.</td><td>그리고 여기서 중요한 개념은, 이해하기 쉽게 설명하자면, 말 그대로 모든 산업에 영향을 미치는 다른 기술들과 마찬가지로, 물론 전기도 그 중 하나였고 인프라가 되었습니다. 물론 우리가 지금 인터넷으로 알고 있는 정보 인프라는 모든 산업, 모든 국가, 모든 사회에 영향을 미칩니다. 인텔리전스도 분명히 그런 것들 중 하나입니다. 인텔리전스가 선택사항이라고 생각하는 회사나 산업, 국가는 없다고 봅니다. 이는 필수 인프라입니다. 그리고 이제 우리는 인텔리전스를 디지털화했습니다. 따라서 우리는 분명히 이 인프라 구축의 초기 단계에 있다고 생각합니다. 그리고 모든 국가가 이를 보유하게 될 것이라고 확신합니다.</td></tr>
<tr><td>Every industry will use it, that I'm certain of. And what's unique about this infrastructure is that it needs factories. It's a little bit like the energy infrastructure, electricity. It needs factories. We need factories to produce this intelligence, and the intelligence is getting more sophisticated. We were talking about earlier that we had a huge breakthrough in the last couple of years with reasoning AI. And now, there are agents that reason and there are super-agents that use a whole bunch of tools and then there's clusters of super agents where agents are working with agents, solving problems.</td><td>모든 산업이 이를 활용하게 될 것이라고 확신합니다. 그리고 이 인프라의 독특한 점은 팩토리가 필요하다는 것입니다. 전력과 같은 에너지 인프라와 조금 비슷합니다. 팩토리가 필요하죠. 우리는 이러한 인텔리전스를 생산하기 위한 팩토리가 필요하며, 인텔리전스는 점점 더 정교해지고 있습니다. 앞서 말씀드린 바와 같이 지난 몇 년간 추론 AI에서 엄청난 돌파구를 마련했습니다. 그리고 이제는 추론하는 에이전트들이 있고, 다양한 도구들을 활용하는 슈퍼 에이전트들이 있으며, 에이전트들이 다른 에이전트들과 협력하여 문제를 해결하는 슈퍼 에이전트들의 클러스터까지 존재합니다.</td></tr>
<tr><td>And so, you could just imagine, compared to one-shot chatbots and the agents that are now using AI built on these large language models, how much more compute-intensive they really need to be and are. So, I think we're in the beginning of the build-out, and there should be many, many more announcements in the future. Operator<br><br>Your next question comes from Ben Reitzes with Melius. Your line is open. Ben Reitzes<br><br>Yeah, hi. Thanks for the question. I wanted to ask, first to Colette, just a little clarification around the guidance and maybe putting it in a different way.</td><td>그래서 일회성 챗봇들과 현재 이러한 대형 언어 모델을 기반으로 한 AI를 사용하는 에이전트들을 비교해보면, 실제로 얼마나 더 많은 컴퓨팅 집약적인 작업이 필요한지, 그리고 실제로 그렇게 되고 있는지 상상해보실 수 있을 것입니다. 따라서 저는 우리가 구축의 초기 단계에 있다고 생각하며, 앞으로 훨씬 더 많은 발표들이 있을 것으로 예상합니다.<br><br>오퍼레이터<br><br>다음 질문은 Melius의 Ben Reitzes님으로부터 받았습니다. 연결되었습니다.<br><br>Ben Reitzes<br><br>네, 안녕하세요. 질문 기회를 주셔서 감사합니다. 먼저 Colette에게 가이던스에 대한 약간의 명확한 설명을 요청하고 싶고, 아마 다른 방식으로 접근해보고 싶습니다.</td></tr>
<tr><td>The $8 billion for H20 just seems like it's roughly $3 billion more than most people thought with regard to what you'd be foregoing in the second quarter. So, that would mean that with regard to your guidance, the rest of the business in order to hit $45 billion is doing $2 billion to $3 billion or so better. So, I was wondering if that math made sense to you. And then, in terms of the guidance, that would imply the non-China business is doing a bit better than the Street expected. So, wondering what the primary driver was there in your view.</td><td>H20에 대한 80억 달러는 2분기에 포기하게 될 것으로 대부분의 사람들이 예상했던 것보다 대략 30억 달러 정도 더 많은 것으로 보입니다. 그렇다면 가이던스와 관련해서, 450억 달러를 달성하기 위해서는 나머지 사업이 20억~30억 달러 정도 더 좋은 성과를 내야 한다는 의미가 됩니다. 이런 계산이 맞는지 궁금합니다. 그리고 가이던스 측면에서 보면, 중국 외 사업이 월가 예상보다 다소 좋은 성과를 내고 있다는 것을 시사하는데, 귀하가 보시기에 그 주요 동력이 무엇인지 궁금합니다.</td></tr>
<tr><td>And then, the second part of my question, Jensen, I know you guide one quarter at a time, but with regard to the AI diffusion rule being lifted and this momentum with sovereign, there's been times in your history where you guys have said on calls like this, where you have more conviction and sequential growth throughout the year, et cetera. And given the unleashing of demand with AI diffusion being revoked and the supply chain increasing, does the environment give you more conviction in sequential growth as we go throughout the year? So, first one for Colette and then next one for Jensen. Thanks so much. Colette Kress<br><br>Thanks, Ben, for the question.</td><td>그리고 제 질문의 두 번째 부분입니다. 젠슨, 한 분기씩 가이던스를 제시하신다는 것은 알고 있지만, AI 확산 규제가 해제되고 소버린 부문에서 이런 모멘텀이 나타나고 있는 상황에서, 과거 이런 컨퍼런스 콜에서 여러분이 연중 순차적 성장에 대해 더 확신을 갖고 계시다고 말씀하신 적이 있었습니다. AI 확산 규제 철회로 수요가 분출되고 공급망이 확대되는 상황을 고려할 때, 현재 환경이 연중 순차적 성장에 대한 확신을 더 높여주고 있는지요? 첫 번째는 콜레트에게, 두 번째는 젠슨에게 드리는 질문입니다. 정말 감사합니다.<br><br>콜레트 크레스: 벤, 질문 감사합니다.</td></tr>
<tr><td>When we look at our Q2 guidance and our commentary that we provided, that, had the export controls not occurred, we would have had orders of about $8 billion for H20, that's correct. That was a possibility for what we would have had in our outlook for this quarter in Q2. So, what we also have talked about here is the growth that we've seen in Blackwell, Blackwell across many of our customers as well as the growth that we continue to have in terms of supply that we need for our customers. So, putting those together, that's where we came through with the guidance that we provided. I'm going to turn the rest over to Jensen to see how he wants to take us. Jensen Huang<br><br>Yeah, thanks.</td><td>2분기 가이던스와 저희가 제공한 코멘터리를 살펴보면, 수출 통제가 발생하지 않았다면 H20에 대해 약 80억 달러의 주문을 받았을 것이라는 점이 맞습니다. 이는 2분기 이번 분기 전망에서 가능했던 수치입니다. 또한 저희가 여기서 언급한 것은 Blackwell에서 보고 있는 성장, 즉 많은 고객들에 걸친 Blackwell의 성장과 고객들을 위해 필요한 공급 측면에서 지속적으로 보이고 있는 성장입니다. 이러한 요소들을 종합하여 저희가 제공한 가이던스를 도출하게 되었습니다. 나머지는 젠슨에게 넘겨서 그가 어떻게 진행하고 싶어하는지 보겠습니다. <br><br>젠슨 황:<br><br>네, 감사합니다.</td></tr>
<tr><td>Thanks, Ben. I would say compared to the beginning of the year, compared to GTC timeframe, there are four positive surprises. The first positive surprise is the step function demand increase of reasoning AI, I think it is fairly clear now that AI is going through an exponential growth, and reasoning AI really busted through. Concerns about hallucination or its ability to really solve problems, and I think a lot of people are crossing that barrier and realizing how incredibly effective agentic AI is and reasoning AI is. So, number one is inference reasoning and the exponential growth there, demand growth. The second one, you mentioned AI diffusion.</td><td>감사합니다, Ben. 연초와 비교해서, GTC 시점과 비교해서 네 가지 긍정적인 서프라이즈가 있었다고 말씀드리겠습니다. 첫 번째 긍정적인 서프라이즈는 추론 AI의 단계적 수요 증가입니다. 이제 AI가 기하급수적인 성장을 겪고 있다는 것이 상당히 명확해졌고, 추론 AI가 정말로 돌파구를 마련했습니다. 환각 현상이나 실제 문제 해결 능력에 대한 우려들을 말이죠. 그리고 많은 사람들이 그 장벽을 넘어서면서 에이전틱 AI와 추론 AI가 얼마나 놀랍도록 효과적인지를 깨닫고 있다고 생각합니다. 따라서 첫 번째는 추론 인퍼런스와 그곳에서의 기하급수적인 성장, 수요 증가입니다. 두 번째로, 당신이 언급한 AI 확산입니다.</td></tr>
<tr><td>It's really terrific to see that the AI diffusion rule was rescinded. President Trump wants America to win, and he also realizes that we're not the only country in the race. And he wants the United States to win and recognizes that we have to get the American stack out to the world and have the world build on top of American stacks instead of alternatives. And so, AI diffusion happened, the rescinding of it happened at almost precisely the time that countries around the world are awakening to the importance of AI as an infrastructure, not just as a technology of great curiosity and great importance, but infrastructure for their industries and start-ups and society.</td><td>AI 확산 규제가 철회된 것을 보니 정말 훌륭합니다. 트럼프 대통령은 미국이 승리하기를 원하며, 또한 우리가 이 경쟁에서 유일한 국가가 아니라는 것을 인식하고 있습니다. 그는 미국이 승리하기를 원하고, 대안 기술이 아닌 미국 기술 스택을 전 세계에 보급하여 세계가 미국 스택을 기반으로 구축하도록 해야 한다는 것을 인식하고 있습니다. 그래서 AI 확산 규제 철회가 이루어졌는데, 이는 전 세계 국가들이 AI를 단순히 호기심과 중요성을 불러일으키는 기술이 아닌, 자국의 산업과 스타트업, 그리고 사회를 위한 인프라로서의 중요성을 깨닫기 시작한 시점과 거의 정확히 일치했습니다.</td></tr>
<tr><td>Just as they had to build out infrastructure for electricity and Internet, you got to build out an infrastructure for AI. I think that, that's an awakening, and that creates a lot of opportunity. The third is enterprise AI. Agents work and agents are doing -- these agents are really quite successful. Much more than generative AI, agentic AI is game-changing. Agents can understand ambiguous and rather implicit instructions and able to problem solve and use tools and have memory and so on. And so, I think this is -- enterprise AI is ready to take off.</td><td>전력과 인터넷을 위한 인프라를 구축해야 했던 것처럼, AI를 위한 인프라도 구축해야 합니다. 이것이 바로 각성이라고 생각하며, 이는 많은 기회를 창출합니다. 세 번째는 엔터프라이즈 AI입니다. 에이전트들이 작동하고 있으며, 이러한 에이전트들은 실제로 상당히 성공적입니다. 생성형 AI보다 훨씬 더, 에이전틱 AI는 게임 체인저입니다. 에이전트들은 모호하고 다소 암시적인 지시사항을 이해할 수 있으며, 문제를 해결하고 도구를 사용하며 기억 기능을 갖추고 있습니다. 따라서 엔터프라이즈 AI가 본격적으로 도약할 준비가 되었다고 생각합니다.</td></tr>
<tr><td>And it's taken us a few years to build a computing system that is able to integrate and run enterprise AI stacks, run enterprise IT stacks but add AI to it. And this is the RTX Pro Enterprise server that we announced at COMPUTEX just last week. And just about every major IT company has joined us, super excited about that. And so, computing is one stack, one part of it. But remember, enterprise IT is really three pillars; it's compute, storage, and networking. And we've now put all three of them together finally, and we're going to market with that. And then lastly, industrial AI.</td><td>그리고 기업용 AI 스택을 통합하고 실행할 수 있는 컴퓨팅 시스템을 구축하는 데 몇 년이 걸렸습니다. 기업용 IT 스택을 실행하면서 동시에 AI를 추가할 수 있는 시스템 말입니다. 이것이 바로 지난주 COMPUTEX에서 발표한 RTX Pro Enterprise 서버입니다. 거의 모든 주요 IT 기업들이 우리와 함께하고 있으며, 이에 대해 매우 흥미진진하게 생각하고 있습니다. 컴퓨팅은 하나의 스택이자 그 일부분입니다. 하지만 기억하셔야 할 점은 기업용 IT는 실제로 세 개의 기둥으로 구성되어 있다는 것입니다. 바로 컴퓨팅, 스토리지, 그리고 네트워킹입니다. 그리고 우리는 이제 마침내 이 세 가지를 모두 통합했으며, 이를 가지고 시장에 나가고 있습니다. 그리고 마지막으로 산업용 AI입니다.</td></tr>
<tr><td>Remember, one of the implications of the world reordering, if you will, is a region's onshoring manufacturing and building plants everywhere. In addition to AI factories, of course, there are new electronics manufacturing, chip manufacturing being built around the world. And all of these new plants and these new factories are creating exactly the right time when Omniverse and AI and all the work that we're doing with robotics is emerging. And so, this fourth pillar is quite important. Every factory will have an AI factory associated with it. And in order to create these physical AI systems, you really have to train a vast amount of data.</td><td>기억하세요, 세계 질서 재편의 함의 중 하나는 각 지역이 제조업을 온쇼어링하고 모든 곳에 공장을 건설하고 있다는 것입니다. AI 팩토리 외에도, 물론 새로운 전자제품 제조, 칩 제조 시설들이 전 세계에 건설되고 있습니다. 그리고 이러한 모든 새로운 플랜트와 새로운 팩토리들이 옴니버스와 AI, 그리고 우리가 로보틱스 분야에서 하고 있는 모든 작업이 부상하는 정확히 적절한 시기를 만들어내고 있습니다. 따라서 이 네 번째 기둥은 상당히 중요합니다. 모든 팩토리는 이와 연관된 AI 팩토리를 갖게 될 것입니다. 그리고 이러한 물리적 AI 시스템들을 구축하기 위해서는 정말로 방대한 양의 데이터를 훈련시켜야 합니다.</td></tr>
<tr><td>So, back to more data, more training, more AIs to be created, more computers. And so, these four drivers are really kicking into turbocharge. Operator<br><br>Your next question comes from Timothy Arcuri with UBS. Your line is open. Timothy Arcuri<br><br>Thanks a lot. Jensen, I wanted to ask about China. It sounds like the July guidance assumes there's no SKU replacement for the H20, but if the President wants the U.S. to win, it seems like you're going to have to be allowed to ship something into China. So, I guess I had two points on that. First of all, have you been approved to ship a new modified version into China? And you're currently building it, but you just can't ship it in fiscal Q2?</td><td>더 많은 데이터, 더 많은 훈련, 더 많은 AI 생성, 더 많은 컴퓨터로 이어지는 것입니다. 그래서 이 네 가지 동력이 정말로 터보차지 역할을 하고 있습니다.<br><br>오퍼레이터<br><br>다음 질문은 UBS의 Timothy Arcuri님으로부터 받았습니다. 연결되었습니다.<br><br>Timothy Arcuri<br><br>감사합니다. 젠슨, 중국에 대해 질문드리고 싶습니다. 7월 가이던스를 보면 H20을 대체할 SKU가 없다고 가정하고 있는 것 같은데, 만약 대통령이 미국의 승리를 원한다면 중국으로 뭔가를 출하할 수 있도록 허용받아야 할 것 같습니다. 그래서 이에 대해 두 가지 포인트가 있습니다. 우선, 중국으로 출하할 새로운 수정 버전에 대한 승인을 받으셨나요? 그리고 현재 제작 중이지만 2분기에는 출하할 수 없는 상황인가요?</td></tr>
<tr><td>And then, you were sort of run rating $7 billion to $8 billion a quarter into China. Can we get back to those sorts of quarterly run rates once you get something that you're allowed to ship back into China? I think we're all trying to figure out how much to add back to our models and when. So, whatever you can say there would be great. Thanks. Jensen Huang<br><br>The President has a plan. He has a vision and I trust him. With respect to our export controls, it's a set of limits. And the new set of limits pretty much make it impossible for us to reduce Hopper any further for any productive use. And so, the new limits, it's kind of the end of the road for Hopper.</td><td>그리고 중국에 분기당 70억 달러에서 80억 달러 정도의 런레이트를 기록하고 계셨는데요. 중국으로 출하가 허용되는 제품을 확보하게 되면 그런 분기별 런레이트 수준으로 다시 돌아갈 수 있을까요? 저희 모두 언제 얼마나 모델에 다시 반영해야 할지 파악하려고 노력하고 있습니다. 그 부분에 대해 말씀해 주실 수 있는 것이 있다면 감사하겠습니다.<br><br>젠슨 황:<br><br>대통령께서는 계획을 가지고 계십니다. 비전을 가지고 계시고 저는 그분을 신뢰합니다. 수출 통제와 관련해서는, 이는 일련의 제한 사항들입니다. 그리고 새로운 제한 사항들은 사실상 우리가 생산적인 용도를 위해 Hopper를 더 이상 축소하는 것을 불가능하게 만듭니다. 따라서 새로운 제한으로 인해 Hopper는 사실상 막다른 길에 다다른 상황입니다.</td></tr>
<tr><td>We have some -- we have limited options. And so, we just -- the key is to understand the limits. The key is to understand the limits and see if we can come up with interesting products that could continue to serve the Chinese market. We don't have anything at the moment, but we're considering it. We're thinking about it. Obviously, the limits are quite stringent at the moment. And we have nothing to announce today. And when the time comes, we'll engage the administration and discuss that. Operator<br><br>Your final question comes from the line of Aaron Rakers with Wells Fargo. Your line is open. Unidentified Analyst<br><br>Hi. This is Jake on for Aaron.</td><td>저희는 몇 가지 -- 저희는 제한된 옵션을 가지고 있습니다. 그래서 저희는 단지 -- 핵심은 그 한계를 이해하는 것입니다. 핵심은 그 한계를 이해하고 중국 시장에 계속 서비스를 제공할 수 있는 흥미로운 제품들을 만들어낼 수 있는지 보는 것입니다. 현재로서는 아무것도 없지만, 검토하고 있습니다. 생각해보고 있습니다. 분명히 현재 제한사항들이 상당히 엄격합니다. 그리고 오늘 발표할 것은 없습니다. 때가 되면 행정부와 협의하고 논의할 것입니다.<br><br>오퍼레이터<br><br>마지막 질문은 Wells Fargo의 Aaron Rakers님으로부터 나왔습니다. 연결해드리겠습니다.<br><br>미확인 애널리스트<br><br>안녕하세요. Aaron을 대신해서 Jake입니다.</td></tr>
<tr><td>Thanks for taking the question, and congrats on the great quarter. I was wondering if you could give some additional color around the strength you saw within the networking business, particularly around the adoption of your Ethernet solutions at CSPs as well as any change you're seeing in network attach rates. Jensen Huang<br><br>Yeah, thank you for that. We now have three networking platforms, maybe four. The first one is the scale-up platform to turn a computer into a much larger computer. Scaling up is incredibly hard to do. Scaling out is easier to do, but scaling up is hard to do. And that platform is called NVLink.</td><td>질문해 주셔서 감사하고, 훌륭한 분기 실적에 대해 축하드립니다. 네트워킹 사업에서 보신 강세에 대해 추가적인 설명을 해주실 수 있는지 궁금합니다. 특히 CSP(통신서비스 제공업체)에서의 이더넷 솔루션 도입과 네트워크 연결률(attach rate)에서 보고 계신 변화가 있다면 말씀해 주세요.<br><br>젠슨 황<br><br>네, 감사합니다. 저희는 현재 3개, 아마도 4개의 네트워킹 플랫폼을 보유하고 있습니다. 첫 번째는 컴퓨터를 훨씬 더 큰 컴퓨터로 변환하는 스케일업 플랫폼입니다. 스케일업은 구현하기가 매우 어렵습니다. 스케일아웃은 상대적으로 쉽지만, 스케일업은 어렵습니다. 그리고 그 플랫폼의 이름이 NVLink입니다.</td></tr>
<tr><td>NVLink is -- comes with it chips and switches and NVLink Spines and it's really complicated. But anyway, that's our new platform, scale-up platform. In addition to InfiniBand, we also have Spectrum-X. We've been fairly consistent that Ethernet was designed for a lot of traffic that are independent, but in the case of AI, you have a lot of computers working together. And the traffic of AI is insanely bursty. Latency matters a lot because the AI is thinking and it wants to get work done as quickly as possible, and you got a whole bunch of nodes working together.</td><td>NVLink는 칩과 스위치, 그리고 NVLink Spine으로 구성되어 있어 정말 복잡합니다. 하지만 어쨌든, 이것이 우리의 새로운 플랫폼인 스케일업 플랫폼입니다. InfiniBand 외에도 Spectrum-X도 보유하고 있습니다. 저희는 이더넷이 독립적인 많은 트래픽을 위해 설계되었다고 일관되게 말해왔지만, AI의 경우에는 많은 컴퓨터들이 함께 작업합니다. 그리고 AI 트래픽은 극도로 버스티(집중적)합니다. 지연시간이 매우 중요한데, 이는 AI가 사고하고 있고 가능한 한 빨리 작업을 완료하고자 하며, 수많은 노드들이 함께 작업하고 있기 때문입니다.</td></tr>
<tr><td>And so, we enhanced Ethernet, added capabilities like extremely low latency, congestion control, adaptive routing, the type of technologies that were available only in InfiniBand to Ethernet. And as a result, we improved the utilization of Ethernet in these clusters. These clusters are gigantic, from as low as 50% to as high as 85%, 90%. And so, the difference is if you had a cluster that's $10 billion, and you improve its effectiveness by 40%, that's worth $4 billion. It's incredible. And so, Spectrum-X has been really, quite frankly, a home run. And this last quarter, as we said in the prepared remarks, we added two very significant CSPs to the Spectrum-X adoption.</td><td>그래서 우리는 이더넷을 개선하고, 극도로 낮은 지연시간, 혼잡 제어, 적응형 라우팅과 같은 기능들을 추가했습니다. 이는 기존에 인피니밴드에서만 사용할 수 있었던 기술들을 이더넷에 적용한 것입니다. 그 결과, 이러한 클러스터에서 이더넷의 활용도를 개선할 수 있었습니다. 이 클러스터들은 규모가 거대한데, 활용도가 최저 50%에서 최고 85%, 90%까지 향상되었습니다. 따라서 만약 100억 달러 규모의 클러스터가 있고, 그 효율성을 40% 개선한다면, 그 가치는 40억 달러에 달합니다. 정말 놀라운 일입니다. 그래서 Spectrum-X는 솔직히 말해서 정말 대성공이었습니다. 그리고 지난 분기에 준비된 발언에서 말씀드린 바와 같이, 우리는 Spectrum-X 도입에 두 개의 매우 중요한 CSP를 추가했습니다.</td></tr>
<tr><td>And then the last one is BlueField, which is our control plane. And so, in those four -- the control plane of network, which is used for storage, it's used for security, and for many of these clusters that want to achieve isolation among its users, multi-tenant clusters and still be able to use and have extremely high-performance bare metal performance, BlueField is ideal for that and is used in a lot of these cases. And so, we have these four networking platforms that are all growing and we're doing really well. I'm very proud of the team. Operator<br><br>That is all the time we have for questions. Jensen, I will turn the call back to you. Jensen Huang<br><br>Thank you.</td><td>그리고 마지막으로 BlueField가 있는데, 이는 저희의 컨트롤 플레인입니다. 이 네 가지 중에서 -- 네트워크의 컨트롤 플레인으로, 스토리지에 사용되고 보안에 사용되며, 사용자 간 격리를 달성하고자 하는 많은 클러스터들, 즉 멀티 테넌트 클러스터들이 여전히 극도로 높은 성능의 베어메탈 성능을 사용하고 확보할 수 있도록 하는 데 BlueField가 이상적이며, 이러한 많은 사례에서 사용되고 있습니다. 따라서 저희는 모두 성장하고 있는 이 네 가지 네트워킹 플랫폼을 보유하고 있으며 정말 잘하고 있습니다. 팀을 매우 자랑스럽게 생각합니다.<br><br>오퍼레이터<br><br>질문 시간이 모두 끝났습니다. 젠슨, 다시 통화를 넘겨드리겠습니다.<br><br>젠슨 황<br><br>감사합니다.</td></tr>
<tr><td>This is the start of a powerful new wave of growth. Grace Blackwell is in full production. We're off to the races. We now have multiple significant growth engines. Inference, once the light of workload, is surging with revenue-generating AI services. AI is growing faster and will be larger than any platform shifts before, including the Internet, mobile and cloud. Blackwell is built to power the full AI life cycle from training frontier models to running complex inference and reasoning agents at scale. Training demand continues to rise with breakthroughs in post training and like reinforcement learning and synthetic data generation, but inference is exploding.</td><td>이것은 강력한 새로운 성장 물결의 시작입니다. Grace Blackwell이 본격 생산에 들어갔습니다. 이제 본격적으로 시작되었습니다. 우리는 이제 여러 개의 중요한 성장 엔진을 보유하고 있습니다. 한때 가벼운 워크로드였던 추론(inference)이 수익 창출 AI 서비스와 함께 급증하고 있습니다. AI는 인터넷, 모바일, 클라우드를 포함한 이전의 모든 플랫폼 전환보다 더 빠르게 성장하고 있으며, 더 큰 규모가 될 것입니다. Blackwell은 최첨단 모델 훈련부터 대규모 복잡한 추론 및 추론 에이전트 실행까지 AI의 전체 생명주기를 지원하도록 구축되었습니다. 포스트 트레이닝과 강화학습, 합성 데이터 생성의 혁신적 발전으로 훈련 수요는 계속 증가하고 있지만, 추론은 폭발적으로 성장하고 있습니다.</td></tr>
<tr><td>Reasoning AI agents require orders of magnitude more compute. The foundations of our next growth platforms are in place and ready to scale. Sovereign AI, nations are investing in AI infrastructure like they once did for electricity and Internet. Enterprise AI, AI must be deployable on-prem and integrated with existing IT. Our RTX Pro, DGX Spark and DGX Station enterprise AI systems are ready to modernize the $500 billion IT infrastructure on-prem or in the cloud. Every major IT provider is partnering with us. Industrial AI from training to digital twin simulation to deployment, NVIDIA Omniverse and Isaac GR00T are powering next-generation factories and humanoid robotic systems worldwide.</td><td>추론 AI 에이전트는 기존 대비 몇 배 더 많은 컴퓨팅 파워를 필요로 합니다. 우리의 차세대 성장 플랫폼의 기반이 마련되어 있으며 확장할 준비가 완료되었습니다. <br><br>소버린 AI 분야에서는 각국이 과거 전력과 인터넷에 투자했던 것처럼 AI 인프라에 투자하고 있습니다. <br><br>엔터프라이즈 AI의 경우, AI는 온프레미스에 배포 가능해야 하며 기존 IT와 통합되어야 합니다. 우리의 RTX Pro, DGX Spark, DGX Station 엔터프라이즈 AI 시스템은 5,000억 달러 규모의 IT 인프라를 온프레미스 또는 클라우드에서 현대화할 준비가 되어 있습니다. 모든 주요 IT 제공업체가 우리와 파트너십을 맺고 있습니다. <br><br>산업용 AI 분야에서는 훈련부터 디지털 트윈 시뮬레이션, 배포에 이르기까지 NVIDIA Omniverse와 Isaac GR00T가 전 세계 차세대 공장과 휴머노이드 로봇 시스템을 구동하고 있습니다.</td></tr>
<tr><td>The age of AI is here from AI infrastructures, inference at scale, sovereign AI, enterprise AI, and industrial AI, NVIDIA is ready. Join us at GTC Paris, our keynote at VivaTech on June 11, talking about quantum GPU computing, robotic factories and robots, and celebrate our partnerships building AI factories across the region. The NVIDIA band will tour France, the U.K., Germany and Belgium. Thank you for joining us at the earnings call today. See you in Paris. Operator<br><br>This concludes today's conference call. You may now disconnect.</td><td>AI 시대가 도래했습니다. AI 인프라, 대규모 추론, 주권 AI, 엔터프라이즈 AI, 그리고 산업용 AI에 이르기까지 NVIDIA는 준비되어 있습니다. 6월 11일 VivaTech에서 열리는 GTC 파리와 기조연설에 참여해 주십시오. 양자 GPU 컴퓨팅, 로봇 공장과 로봇에 대해 이야기하고, 이 지역 전반에 걸쳐 AI 팩토리를 구축하는 파트너십을 축하할 예정입니다. NVIDIA 팀은 프랑스, 영국, 독일, 벨기에를 순회할 것입니다. 오늘 실적 발표 컨퍼런스 콜에 참여해 주셔서 감사합니다. 파리에서 뵙겠습니다. <br><br>운영자<br><br>오늘 컨퍼런스 콜을 마치겠습니다. 이제 연결을 종료하셔도 됩니다.</td></tr>
    </table>
    <h3>📌 요약</h3>
    <p style="background:#f0f0f0; padding:15px; border-left: 5px solid #333;">• 주요 실적 및 전망<br>- 중국 수출 통제로 인한 H20 주문 $8B 차질 발생<br>- Grace Blackwell 제품 본격 양산 돌입<br>- Q2 가이던스 $45B로 제시<br><br>• AI 인프라 구축 현황<br>- 전 세계적으로 약 100개의 AI 팩토리 구축 중<br>- 추론(Inference) 수요 급증으로 인한 성장 가속화<br>- 엔터프라이즈 AI, 산업용 AI 등 새로운 성장 동력 확보<br><br>• 사업 리스크/기회 요인<br>- 중국 시장 접근성 제한으로 인한 약 $50B 규모의 TAM 상실<br>- AI Diffusion Rule 해제로 인한 글로벌 시장 기회 확대<br>- 네트워킹 사업 강화로 추가 성장 동력 확보<br><br>• 경영진 톤<br>- AI 인프라 구축이 초기 단계이며 장기 성장 잠재력 강조<br>- 공급망 확대를 통한 수요 대응 능력 강화 중<br>- 전반적으로 매우 긍정적이고 자신감 있는 톤 유지<br><br>• 주목할 만한 전략적 포인트<br>- 엔터프라이즈/산업용 AI 시장 공략 강화<br>- 네트워킹 포트폴리오 확대(NVLink, InfiniBand, Spectrum-X, BlueField)<br>- 유럽 등 글로벌 시장 확대 추진</p>
    <hr style="margin:50px 0;">
    
</body></html>