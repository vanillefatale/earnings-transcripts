<!DOCTYPE html>
<html><head><meta charset="UTF-8">
<title>Earnings Call 번역</title>
<style>
    body { font-family: Arial; margin: 40px; background-color: #fdfdfd; }
    h1 { text-align: center; }
    h2 { margin-top: 50px; color: #003366; }
    h3 { color: #333; }
    table { border: 1px solid #ddd; width: 100%; border-collapse: collapse; }
    th { background: #f0f0f0; padding: 10px; border-bottom: 2px solid #ccc; }
    td { padding: 10px; border-bottom: 1px dotted #ccc; vertical-align: top; }
    p { line-height: 1.6; }
    hr { margin: 50px 0; border: none; border-top: 1px solid #ccc; }
    .back-button {
        display: inline-block;
        background-color: #5f5f5f;
        color: white;
        padding: 10px 16px;
        border-radius: 6px;
        text-decoration: none;
        font-weight: 500;
        box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        margin-bottom: 30px;
    }
</style>
</head><body>
<a href="../../index.html" class="back-button">←</a>
<h1>📄 Earnings Call Transcript 번역 결과</h1>

    <h2>📊 Presentation</h2>
    <table style="width:100%; border-collapse:collapse; margin-bottom: 40px;">
        <tr>
            <th style="width:50%; border-bottom: 2px solid #333;">Original</th>
            <th style="width:50%; border-bottom: 2px solid #333;">Translation</th>
        </tr>
        <tr><td>NVIDIA Corporation (NASDAQ:NVDA) Q4 2025 Earnings Conference Call February 26, 2025 5:00 PM ET<br><br>Company Participants<br><br>Stewart Stecker - IR<br>Jensen Huang - President and CEO<br>Colette Kress - EVP and CFO<br><br>Conference Call Participants<br><br>C.J. Muse - Cantor Fitzgerald<br>Joe Moore - Morgan Stanley<br>Vivek Arya - Bank of America Securities<br>Harlan Sur - JPMorgan<br>Timothy Arcuri - UBS<br>Ben Reitzes - Melius Research<br>Mark Lipacis - Evercore ISI<br>Aaron Rakers - Wells Fargo<br>Atif Malik - Citigroup<br><br>Operator<br><br>Good afternoon. My name is Krista and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's Fourth Quarter Earnings Call.</td><td>NVIDIA Corporation (NASDAQ:NVDA) 2025년 4분기 실적 컨퍼런스 콜 2025년 2월 26일 오후 5:00 ET<br><br>회사 참석자<br><br>Stewart Stecker - IR<br>Jensen Huang - 사장 겸 CEO<br>Colette Kress - EVP 겸 CFO<br><br>컨퍼런스 콜 참석자<br><br>C.J. Muse - Cantor Fitzgerald<br>Joe Moore - Morgan Stanley<br>Vivek Arya - Bank of America Securities<br>Harlan Sur - JPMorgan<br>Timothy Arcuri - UBS<br>Ben Reitzes - Melius Research<br>Mark Lipacis - Evercore ISI<br>Aaron Rakers - Wells Fargo<br>Atif Malik - Citigroup<br><br>오퍼레이터<br><br>안녕하세요. 저는 크리스타이며 오늘 컨퍼런스 진행을 맡게 되었습니다. 이 시간에 NVIDIA 4분기 실적 발표 컨퍼런스 콜에 참석해 주신 모든 분들을 환영합니다.</td></tr>
<tr><td>All lines have been placed on mute to prevent any background noise. After the speakers’ remarks, there will be a question-and-answer session. [Operator Instructions]<br><br>Thank you. Stewart Stecker, you may begin your conference. Stewart Stecker<br><br>Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the fourth quarter of fiscal 2025. With me today from NVIDIA are Jensen Huang, President and Chief Executive Officer and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website.</td><td>모든 회선이 배경 소음을 방지하기 위해 음소거 상태로 설정되었습니다. 발표자들의 발언 후에는 질의응답 시간이 있을 예정입니다. [운영자 안내사항]<br><br>감사합니다. Stewart Stecker님, 컨퍼런스를 시작하셔도 됩니다.<br><br>Stewart Stecker<br><br>감사합니다. 안녕하세요 여러분, 2025 회계연도 4분기 NVIDIA 컨퍼런스 콜에 오신 것을 환영합니다. 오늘 NVIDIA에서 저와 함께하신 분들은 Jensen Huang 사장 겸 최고경영자와 Colette Kress 부사장 겸 최고재무책임자입니다. 저희 컨퍼런스 콜이 NVIDIA 투자자 관계 웹사이트에서 실시간으로 웹캐스트되고 있음을 알려드립니다.</td></tr>
<tr><td>The webcast will be available for replay until the conference call to discuss our financial results for the first quarter of fiscal 2026. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties, and our actual results may differ materially.</td><td>웹캐스트는 2026 회계연도 1분기 재무실적을 논의하는 컨퍼런스 콜까지 재시청이 가능합니다. 오늘 콜의 내용은 NVIDIA의 자산입니다. 사전 서면 동의 없이는 복제하거나 전사할 수 없습니다. 이번 콜에서 저희는 현재의 기대치를 바탕으로 미래전망진술을 할 수 있습니다. 이러한 진술들은 다수의 중대한 위험과 불확실성을 수반하며, 저희의 실제 결과는 크게 다를 수 있습니다.</td></tr>
<tr><td>For discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q, and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, February 26, 2025, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website.</td><td>미래 재무 성과와 사업에 영향을 미칠 수 있는 요인들에 대한 논의는 오늘 발표된 실적 발표자료, 당사의 최신 Form 10-K 및 10-Q, 그리고 증권거래위원회에 Form 8-K로 제출할 수 있는 보고서들의 공시 내용을 참조해 주시기 바랍니다. 당사의 모든 진술은 현재 이용 가능한 정보를 바탕으로 오늘 2025년 2월 26일 기준으로 작성되었습니다. 법률에서 요구하는 경우를 제외하고, 당사는 이러한 진술을 업데이트할 의무를 지지 않습니다. 이번 컨퍼런스 콜에서는 비GAAP 재무지표에 대해 논의할 예정입니다. 이러한 비GAAP 재무지표와 GAAP 재무지표 간의 조정표는 당사 웹사이트에 게시된 CFO 코멘터리에서 확인하실 수 있습니다.</td></tr>
<tr><td>With that, let me turn the call over to Colette. Colette Kress<br><br>Thanks, Stewart. Q4 was another record quarter. Revenue of $39.3 billion was up 12% sequentially and up 78% year-on-year, and above our outlook of $37.5 billion. For fiscal 2025, revenue was $130.5 billion, up 114% in the prior year. Let's start with data center. Data Center revenue for fiscal 2025 was $115.2 billion, more than doubling from the prior year. In the fourth quarter, data center revenue of $35.6 billion was a record of 16% sequentially and 93% year-on-year. As the Blackwell ramp commenced and Hopper 200 continued sequential growths. In Q4, Blackwell sales exceeded our expectations.</td><td>이제 콜레트에게 마이크를 넘기겠습니다. 콜레트 크레스<br><br>감사합니다, 스튜어트. 4분기는 또 다른 기록적인 분기였습니다. 매출은 393억 달러로 전분기 대비 12%, 전년 동기 대비 78% 증가했으며, 우리의 전망치인 375억 달러를 상회했습니다. 2025 회계연도 매출은 1,305억 달러로 전년 대비 114% 증가했습니다. <br><br>데이터센터부터 시작하겠습니다. 2025 회계연도 데이터센터 매출은 1,152억 달러로 전년 대비 두 배 이상 증가했습니다. 4분기 데이터센터 매출은 356억 달러로 기록을 경신하며 전분기 대비 16%, 전년 동기 대비 93% 증가했습니다. 블랙웰 램프업이 시작되고 호퍼 200이 지속적인 순차 성장을 이어가면서 이러한 성과를 달성했습니다. 4분기에 블랙웰 매출은 우리의 기대치를 넘어섰습니다.</td></tr>
<tr><td>We delivered $11 billion of Blackwell revenue to meet strong demand. This is the fastest product ramp in our company's history, unprecedented in its speed and scale. Blackwell production is in full gear across multiple configurations, and we are increasing supply quickly in expanding customer adoption. Our Q4 data center compute revenue jumped 18% sequentially and over 2 times year-on-year. Customers are racing to scale infrastructure to train the next generation of cutting edge models and unlock the next level of AI capabilities. With Blackwell, it will be common for these clusters to start with 100,000 GPUs or more. Shipments have already started for multiple infrastructures of this size.</td><td>강력한 수요를 충족하기 위해 110억 달러의 Blackwell 매출을 달성했습니다. 이는 당사 역사상 가장 빠른 제품 램프업으로, 그 속도와 규모 면에서 전례가 없는 수준입니다. Blackwell 생산은 다양한 구성으로 본격 가동 중이며, 고객 채택 확산에 따라 공급을 빠르게 늘리고 있습니다. 4분기 데이터센터 컴퓨트 매출은 전분기 대비 18%, 전년 동기 대비 2배 이상 급증했습니다. 고객들은 차세대 최첨단 모델을 훈련하고 AI 역량의 다음 단계를 실현하기 위해 인프라 확장 경쟁을 벌이고 있습니다. Blackwell을 통해 이러한 클러스터들이 10만 개 이상의 GPU로 시작하는 것이 일반적이 될 것입니다. 이러한 규모의 다수 인프라에 대한 출하가 이미 시작되었습니다.</td></tr>
<tr><td>Post-training and model customization are fueling demand for NVIDIA infrastructure and software as developers and enterprises leverage techniques such as fine tuning, reinforcement learning, and distillation to tailor models for domain-specific use cases. Hugging Face alone hosts over 90,000 derivatives created from the Llama Foundation model. The scale of post-training and model customization is massive and can collectively demand orders of magnitude more compute than pre-training. Our inference demand is accelerating, driven by test time scaling and new reasoning models like OpenAI’s o3, DeepSeek-R1, and Grok-3.</td><td>사후 훈련과 모델 커스터마이징이 NVIDIA 인프라와 소프트웨어에 대한 수요를 견인하고 있습니다. 개발자와 기업들이 파인튜닝, 강화학습, 증류와 같은 기법을 활용하여 도메인별 사용 사례에 맞게 모델을 맞춤화하고 있기 때문입니다. 허깅페이스만 해도 Llama 파운데이션 모델에서 파생된 9만 개 이상의 모델을 호스팅하고 있습니다. 사후 훈련과 모델 커스터마이징의 규모는 방대하며, 전체적으로 사전 훈련보다 몇 배 더 많은 컴퓨팅 파워를 요구할 수 있습니다. 테스트 타임 스케일링과 OpenAI의 o3, DeepSeek-R1, Grok-3와 같은 새로운 추론 모델들에 의해 우리의 추론 수요가 가속화되고 있습니다.</td></tr>
<tr><td>Long thinking reasoning AI can require 100 times more compute per task compared to one-shot inferences. Blackwell was architected for reasoning AI inference. Blackwell supercharges reasoning AI models with up to 25 times higher token throughput and 20 times lower cost versus Hopper 100. It is revolutionary. Transformer engine is built for LLM and mixture of experts in front. And its NVLink domain delivers 14 times the throughput of PCIe Gen 5, ensuring the response time, throughput and cost efficiency needed to tackle the growing complexity of inference at scale. Companies across industries are tapping into NVIDIA's full-stack inference platform to boost performance and slash costs.</td><td>긴 사고 추론 AI는 일회성 추론 대비 작업당 100배 더 많은 컴퓨팅을 요구할 수 있습니다. Blackwell은 추론 AI 인퍼런스를 위해 설계되었습니다. Blackwell은 Hopper 100 대비 최대 25배 높은 토큰 처리량과 20배 낮은 비용으로 추론 AI 모델을 대폭 강화합니다. 이는 혁신적입니다. Transformer 엔진은 LLM과 전면의 전문가 혼합 모델을 위해 구축되었습니다. 그리고 NVLink 도메인은 PCIe Gen 5 대비 14배의 처리량을 제공하여, 대규모 추론의 증가하는 복잡성을 해결하는 데 필요한 응답 시간, 처리량 및 비용 효율성을 보장합니다. 각 산업의 기업들이 성능을 향상시키고 비용을 대폭 절감하기 위해 NVIDIA의 풀스택 인퍼런스 플랫폼을 활용하고 있습니다.</td></tr>
<tr><td>Now tripled inference throughput and cut costs by 66%, using NVIDIA TensorRT for its screenshot feature. Perplexity sees 435 million monthly queries and reduced its inference costs 3 times with NVIDIA Triton Inference Server and TensorRT-LLM. Microsoft Bing achieved a 5x speed up and major TCO savings for visual search across billions of images with NVIDIA, TensorRT, and acceleration libraries. Blackwell has great demand for inference. Many of the early GB200 deployments are earmarked for inference, a first for a new architecture. Blackwell addresses the entire AI market from pre-training, post-training to inference across clouds, to on-premise, to enterprise.</td><td>이제 NVIDIA TensorRT를 사용한 스크린샷 기능으로 추론 처리량을 3배 늘리고 비용을 66% 절감했습니다. Perplexity는 월 4억 3,500만 건의 쿼리를 처리하며 NVIDIA Triton Inference Server와 TensorRT-LLM을 통해 추론 비용을 3배 절감했습니다. Microsoft Bing은 NVIDIA, TensorRT, 그리고 가속 라이브러리를 통해 수십억 개의 이미지에 대한 시각적 검색에서 5배의 속도 향상과 주요한 TCO 절감을 달성했습니다. <br><br>Blackwell은 추론 분야에서 큰 수요를 보이고 있습니다. 초기 GB200 배포의 상당 부분이 추론용으로 할당되었는데, 이는 새로운 아키텍처로서는 처음 있는 일입니다. Blackwell은 사전 훈련, 후속 훈련부터 추론까지, 클라우드에서 온프레미스, 엔터프라이즈에 이르기까지 전체 AI 시장을 다루고 있습니다.</td></tr>
<tr><td>[HUDA's] (ph) programmable architecture accelerates every AI model and over 4,400 applications, ensuring large infrastructure investments against obsolescence in rapidly evolving markets. Our performance and pace of innovation is unmatched. We're driven to a 200 times reduction in inference costs in just the last 2 years. We deliver the lowest TCO and the highest ROI. And full stack optimizations for NVIDIA and our large ecosystem, including 5.9 million developers, continuously improve our customers' economics. In Q4, large CSPs represented about half of our data center revenue. And these sales increased nearly 2 times year-on-year.</td><td>[HUDA의] 프로그래머블 아키텍처는 모든 AI 모델과 4,400개 이상의 애플리케이션을 가속화하여, 급속히 진화하는 시장에서 대규모 인프라 투자를 노후화로부터 보호합니다. 우리의 성능과 혁신 속도는 타의 추종을 불허합니다. 지난 2년간 추론 비용을 200배 절감하는 성과를 달성했습니다. 우리는 최저 TCO(총소유비용)와 최고 ROI(투자수익률)를 제공합니다. 그리고 590만 명의 개발자를 포함한 NVIDIA와 우리의 대규모 생태계를 위한 풀스택 최적화는 지속적으로 고객들의 경제성을 개선하고 있습니다. 4분기에 대형 CSP들이 우리 데이터센터 매출의 약 절반을 차지했습니다. 그리고 이러한 매출은 전년 동기 대비 거의 2배 증가했습니다.</td></tr>
<tr><td>Large CSPs were some of the first to stand up Blackwell with Azure, GCP, AWS, and OCI bringing GB200 systems to cloud regions around the world to meet surging customer demand for AI. Regional cloud hosting and video GPUs increased as a percentage of data center revenue, reflecting continued AI factory build-outs globally and rapidly rising demand for AI reasoning models and agents. Core, we've launched a 100, 000 GV200 cluster-based instance with NVLink Switch and Quantum2 InfiniBand. Consumer Internet Revenue Group 3x year-on-year driven by an expanding set of generative AI and deep learning use cases.</td><td>대형 CSP들은 Blackwell을 최초로 도입한 기업들 중 일부였으며, Azure, GCP, AWS, OCI가 급증하는 고객의 AI 수요를 충족하기 위해 전 세계 클라우드 지역에 GB200 시스템을 도입했습니다. 지역 클라우드 호스팅 및 비디오 GPU가 데이터센터 매출에서 차지하는 비중이 증가했는데, 이는 전 세계적인 AI 팩토리 구축 지속과 AI 추론 모델 및 에이전트에 대한 급속한 수요 증가를 반영한 것입니다. Core에서는 NVLink Switch와 Quantum2 InfiniBand를 기반으로 한 100,000개의 GV200 클러스터 기반 인스턴스를 출시했습니다. 소비자 인터넷 수익 그룹은 확장되는 생성형 AI 및 딥러닝 사용 사례에 힘입어 전년 대비 3배 증가했습니다.</td></tr>
<tr><td>These include recommender systems, vision language understanding, synthetic data, generation search, and agentic AI. For example, xAI is adopting the GB200 to train and inference its next generation of grog AI models. Meta's cutting-edge Andromeda advertising engine runs on NVIDIA's Grace Hopper Superchip, serving vast quantities of ads across Instagram, Facebook applications. Andromeda harnesses Grace Hopper's fast interconnect and large memory to boost inference throughput by 3x, enhanced ad personalization and deliver meaningful jumps in monetization and ROI.</td><td>이러한 분야에는 추천 시스템, 비전 언어 이해, 합성 데이터, 생성 검색, 그리고 에이전틱 AI가 포함됩니다. 예를 들어, xAI는 차세대 Grok AI 모델의 훈련과 추론을 위해 GB200을 도입하고 있습니다. Meta의 최첨단 Andromeda 광고 엔진은 NVIDIA의 Grace Hopper 슈퍼칩에서 구동되어 Instagram과 Facebook 애플리케이션 전반에 걸쳐 방대한 양의 광고를 서비스하고 있습니다. Andromeda는 Grace Hopper의 고속 인터커넥트와 대용량 메모리를 활용하여 추론 처리량을 3배 향상시키고, 광고 개인화를 강화하며, 수익화와 ROI에서 의미 있는 도약을 제공합니다.</td></tr>
<tr><td>Enterprise revenue increased nearly 2 times year on accelerating demand for model fine-tuning, RAG and Agentic AI workflows and GPU accelerated data processing. We introduced NVIDIA Llama Nemotron model family NIMs to help developers create and deploy AI agents across a range of applications including customer support, fraud detection and product supply chain and inventory management. Leading AI agent platform providers, including SAP and ServiceNow are among the first to use new models.</td><td>기업 매출은 모델 파인튜닝, RAG 및 에이전틱 AI 워크플로우와 GPU 가속 데이터 처리에 대한 수요 증가로 전년 대비 거의 2배 증가했습니다. 저희는 개발자들이 고객 지원, 사기 탐지, 제품 공급망 및 재고 관리를 포함한 다양한 애플리케이션에서 AI 에이전트를 생성하고 배포할 수 있도록 돕는 NVIDIA Llama Nemotron 모델 패밀리 NIM을 출시했습니다. SAP와 ServiceNow를 포함한 주요 AI 에이전트 플랫폼 제공업체들이 새로운 모델을 최초로 사용하는 기업들 중 하나입니다.</td></tr>
<tr><td>Health care leaders IQVIA, Illumina, Mayo Clinic and Arc Institute are using NVIDIA AI to speed drug discovery enhanced genomic research and Pioneer advanced health care services with generative and Agentic AI. As AI expands beyond the digital world, NVIDIA infrastructure and software platforms are increasingly being adopted to power robotics and physical AI development. One of the early and largest robotics applications and autonomous vehicles where virtually every AV company is developing on NVIDIA in the data center, the car or both. NVIDIA's automotive vertical revenue is expected to grow to approximately $5 billion this fiscal year.</td><td>헬스케어 선도기업인 IQVIA, Illumina, Mayo Clinic, Arc Institute는 NVIDIA AI를 활용하여 신약 개발을 가속화하고 유전체학 연구를 강화하며, 생성형 AI와 에이전틱 AI를 통해 첨단 헬스케어 서비스를 개척하고 있습니다. AI가 디지털 세계를 넘어 확장됨에 따라, NVIDIA의 인프라와 소프트웨어 플랫폼은 로보틱스와 물리적 AI 개발을 지원하기 위해 점점 더 많이 채택되고 있습니다. 초기이자 가장 큰 로보틱스 응용 분야 중 하나인 자율주행차 분야에서는 거의 모든 자율주행차 회사들이 데이터센터나 차량, 또는 양쪽 모두에서 NVIDIA를 기반으로 개발하고 있습니다. NVIDIA의 자동차 부문 매출은 올 회계연도에 약 50억 달러까지 성장할 것으로 예상됩니다.</td></tr>
<tr><td>At CES, Hyundai Motor Group announced it is adopting NVIDIA technologies to accelerate AV and robotics development and smart factory initiatives. Vision transformers, self-supervised learning, multimodal sensor fusion and high fidelity simulation are driving breakthroughs in AV development and will require 10x more compute. At CES, we announced the NVIDIA COSMO World Foundation model platform. Just as language, foundation models have revolutionized Language AI, Cosmos is a physical AI to revolutionize robotics. The robotics and automotive companies, including ridesharing giant Uber, are among the first to adopt the platform.</td><td>CES에서 현대자동차그룹은 자율주행차와 로보틱스 개발, 그리고 스마트 팩토리 이니셔티브를 가속화하기 위해 NVIDIA 기술을 채택한다고 발표했습니다. 비전 트랜스포머, 자기지도학습, 멀티모달 센서 퓨전, 그리고 고충실도 시뮬레이션이 자율주행차 개발의 혁신을 주도하고 있으며, 이는 10배 더 많은 컴퓨팅 파워를 필요로 할 것입니다. CES에서 저희는 NVIDIA COSMO World Foundation 모델 플랫폼을 발표했습니다. 언어 파운데이션 모델이 언어 AI를 혁신시킨 것처럼, Cosmos는 로보틱스를 혁신시킬 물리적 AI입니다. 라이드셰어링 대기업 우버를 포함한 로보틱스 및 자동차 회사들이 이 플랫폼을 최초로 채택하는 기업들 중 하나입니다.</td></tr>
<tr><td>From a geographic perspective, sequential growth in our Data Center revenue was strongest in the U.S., driven by the initial ramp up Blackwell. Countries across the globe are building their AI ecosystem as demand for compute infrastructure is surging. France's EUR 200 billion Euro AI investment and the EU's EUR 200 billion invest AI initiatives offer a glimpse into the build-out to set redefined global AI infrastructure in the coming years. Now as a percentage of total Data Center revenue, data center sales in China remained well below levels seen on the onset of export controls. Absent any change in regulations, we believe that China shipments will remain roughly at the current percentage.</td><td>지역별 관점에서 보면, 당사 데이터센터 매출의 순차적 성장은 미국에서 가장 강했으며, 이는 Blackwell의 초기 램프업에 의해 견인되었습니다. 컴퓨팅 인프라에 대한 수요가 급증하면서 전 세계 국가들이 AI 생태계를 구축하고 있습니다. 프랑스의 2,000억 유로 AI 투자와 EU의 2,000억 유로 AI 투자 이니셔티브는 향후 몇 년간 글로벌 AI 인프라를 재정의하는 구축 작업의 일면을 보여줍니다. <br><br>전체 데이터센터 매출 대비 비율로 보면, 중국 내 데이터센터 매출은 수출 통제 시행 초기에 보였던 수준을 크게 밑돌고 있습니다. 규제에 변화가 없는 한, 중국 출하량은 현재 비율 수준에서 대략 유지될 것으로 판단합니다.</td></tr>
<tr><td>The market in China for data center solutions remains very competitive. We will continue to comply with export controls while serving our customers. Networking revenue declined 3% sequentially. Our networking attached to GPU compute systems is robust at over 75%. We are transitioning from small NVLink 8 with InfiniBand, to large NVLink 72 with Spectrum-X. Spectrum-X and NVLink Switch revenue increased and represents a major new growth vector. We expect networking to return to growth in Q1.AI requires a new class of networking. NVIDIA offers NVLink Switch systems for scale-up compute. For scale out, we offer quantum incentive for HPC supercomputers and Spectrum X for Ethernet environments.</td><td>중국의 데이터센터 솔루션 시장은 여전히 매우 경쟁이 치열합니다. 저희는 고객에게 서비스를 제공하면서 수출 통제 규정을 계속 준수할 것입니다. 네트워킹 매출은 전분기 대비 3% 감소했습니다. GPU 컴퓨팅 시스템에 연결된 저희 네트워킹은 75% 이상으로 견고한 성과를 보이고 있습니다. 저희는 InfiniBand를 사용하는 소규모 NVLink 8에서 Spectrum-X를 사용하는 대규모 NVLink 72로 전환하고 있습니다. Spectrum-X와 NVLink Switch 매출이 증가했으며, 이는 주요한 새로운 성장 동력을 나타냅니다. 1분기에 네트워킹이 성장세로 돌아설 것으로 예상합니다.<br><br>AI는 새로운 클래스의 네트워킹을 필요로 합니다. NVIDIA는 스케일업 컴퓨팅을 위한 NVLink Switch 시스템을 제공합니다. 스케일아웃의 경우, HPC 슈퍼컴퓨터를 위한 퀀텀 인센티브와 이더넷 환경을 위한 Spectrum X를 제공합니다.</td></tr>
<tr><td>Spectrum-X enhances the Ethernet for AI computing and has been a huge success. Microsoft Azure, OCI, CoreWeave and others are building large AI factories with Spectrum-X. The first Stargate data centers will use Spectrum-X. Yesterday, Cisco announced integrating Spectrum-X into their networking portfolio to help enterprises build AI infrastructure. With its large enterprise footprint and global reach, Cisco will bring NVIDIA Ethernet to every industry. Now moving to gaming and ARPCs. Gaming revenue of $2.5 billion decreased 22% sequentially and 11% year-on-year. Full year revenue of $11.4 billion increased 9% year-on-year, and demand remains strong throughout the holiday.</td><td>Spectrum-X는 AI 컴퓨팅을 위한 이더넷을 향상시키며 큰 성공을 거두었습니다. Microsoft Azure, OCI, CoreWeave 등이 Spectrum-X로 대규모 AI 팩토리를 구축하고 있습니다. 첫 번째 Stargate 데이터센터들이 Spectrum-X를 사용할 예정입니다. 어제 Cisco는 기업들이 AI 인프라를 구축할 수 있도록 돕기 위해 Spectrum-X를 자사 네트워킹 포트폴리오에 통합한다고 발표했습니다. 대규모 기업 고객 기반과 글로벌 도달 범위를 보유한 Cisco는 NVIDIA 이더넷을 모든 산업에 제공할 것입니다. <br><br>이제 게이밍과 ARPC로 넘어가겠습니다. 게이밍 매출은 25억 달러로 전분기 대비 22%, 전년 동기 대비 11% 감소했습니다. 연간 매출은 114억 달러로 전년 대비 9% 증가했으며, 연말연시 기간 내내 수요는 강세를 유지했습니다.</td></tr>
<tr><td>However, Q4 shipments were impacted by supply constraints. We expect strong sequential growth in Q1 as supply increases. The new GeForce RTX 50 Series desktop and laptop GPUs are here. Build for gamers, creators and developers they fuse AI and graphics redefining visual computing, powered by the Blackwell architecture, fifth generation Tensor cores and fourth-generation RT cores and featuring up to 3,400 AI top. These GPUs deliver a 2x performance leap and new AI-driven rendering including neural shaders, digital human technologies, geometry and lighting. The new DLSS 4 boost frame rates up to 8 times with AI-driven frame generation, turning 1 rendered frame into 3.</td><td>하지만 4분기 출하량은 공급 제약으로 인해 영향을 받았습니다. 공급이 증가함에 따라 1분기에는 강력한 순차적 성장을 기대하고 있습니다. 새로운 GeForce RTX 50 시리즈 데스크톱 및 노트북 GPU가 출시되었습니다. 게이머, 크리에이터, 개발자를 위해 제작된 이 제품들은 AI와 그래픽을 융합하여 시각 컴퓨팅을 재정의하며, Blackwell 아키텍처, 5세대 Tensor 코어, 4세대 RT 코어로 구동되고 최대 3,400 AI TOPS를 지원합니다. 이러한 GPU들은 2배의 성능 향상과 뉴럴 셰이더, 디지털 휴먼 기술, 지오메트리 및 조명을 포함한 새로운 AI 기반 렌더링을 제공합니다. 새로운 DLSS 4는 AI 기반 프레임 생성을 통해 프레임 속도를 최대 8배까지 향상시키며, 렌더링된 1개 프레임을 3개로 변환합니다.</td></tr>
<tr><td>It also features the industry's first real-time application of transformer models packing 2 times more parameters and 4 times to compute for unprecedented visual fidelity. We also announced a wave of GeForce Blackwell laptop GPUs with new NVIDIA Max-Q technology that extends battery life by up to an incredible 40%. These laptops will be available starting in March from the world's top manufacturers. Moving to our professional visualization business. Revenue of $511 million was up 5% sequentially and 10% year-on-year. Full year revenue of $1.9 billion increased 21% year-on-year. Key industry verticals driving demand include automotive and health care.</td><td>또한 업계 최초로 트랜스포머 모델의 실시간 적용을 구현했으며, 이는 2배 많은 파라미터와 4배의 연산 성능을 제공하여 전례 없는 시각적 충실도를 실현합니다. 또한 새로운 NVIDIA Max-Q 기술을 탑재한 GeForce Blackwell 노트북 GPU 라인업을 발표했는데, 이는 배터리 수명을 최대 40%까지 놀라울 정도로 연장시킵니다. 이러한 노트북들은 3월부터 전 세계 주요 제조업체들을 통해 출시될 예정입니다. <br><br>전문 시각화 사업 부문으로 넘어가면, 매출은 5억 1,100만 달러로 전분기 대비 5%, 전년 동기 대비 10% 증가했습니다. 연간 매출은 19억 달러로 전년 대비 21% 증가했습니다. 수요를 견인하는 주요 산업 분야로는 자동차와 헬스케어가 있습니다.</td></tr>
<tr><td>NVIDIA Technologies and generative AI are reshaping design, engineering and simulation workloads. Increasingly, these technologies are being leveraged in leading software platforms from ANSYS, Cadence and Siemens fueling demand for NVIDIA RTX workstations. Now moving to Automotive. Revenue was a record $570 million, up 27% sequentially and up 103% year-on-year. Full year revenue of $1.7 billion increased 55% year-on-year. Strong growth was driven by the continued ramp in autonomous vehicles, including cars and robotaxis. At CES, we announced Toyota, the world's largest auto maker will build its next-generation vehicles on NVIDIA Orin running the safety certified NVIDIA Drive OS.</td><td>NVIDIA 기술과 생성형 AI가 설계, 엔지니어링, 시뮬레이션 워크로드를 재편하고 있습니다. 이러한 기술들이 ANSYS, Cadence, Siemens의 주요 소프트웨어 플랫폼에서 점점 더 활용되면서 NVIDIA RTX 워크스테이션에 대한 수요를 견인하고 있습니다. <br><br>이제 자동차 부문으로 넘어가겠습니다. 매출은 5억 7천만 달러로 기록을 경신했으며, 전분기 대비 27% 증가, 전년 동기 대비 103% 증가했습니다. 연간 매출은 17억 달러로 전년 대비 55% 증가했습니다. 강력한 성장은 자동차와 로보택시를 포함한 자율주행차의 지속적인 증가에 의해 견인되었습니다. CES에서 우리는 세계 최대 자동차 제조업체인 토요타가 안전 인증을 받은 NVIDIA Drive OS를 구동하는 NVIDIA Orin을 기반으로 차세대 차량을 제작할 것이라고 발표했습니다.</td></tr>
<tr><td>We announced Aurora and Continental will deploy driverless trucks at scale powered by NVIDIA Drive Thor. Finally, our end-to-end autonomous vehicle platform NVIDIA Drive Hyperion has passed industry safety assessments like TÜV SUD and TUV Rheinland, 2 of the industry's foremost authorities for automotive grade safety and cybersecurity. NVIDIA is the first AV platform that received a comprehensive set of third-party assessments. Okay. Moving to the rest of the P&L. GAAP gross margin was 73% and non-GAAP gross margin was 73.5% and down sequentially as expected with our first deliveries of the Blackwell architecture.</td><td>우리는 Aurora와 Continental이 NVIDIA Drive Thor로 구동되는 무인 트럭을 대규모로 배치할 것이라고 발표했습니다. 마지막으로, 우리의 엔드투엔드 자율주행차 플랫폼인 NVIDIA Drive Hyperion은 자동차급 안전성과 사이버보안 분야의 업계 최고 권위기관인 TÜV SUD와 TUV Rheinland 등의 업계 안전성 평가를 통과했습니다. NVIDIA는 포괄적인 제3자 평가를 받은 최초의 AV 플랫폼입니다. <br><br>이제 손익계산서의 나머지 부분으로 넘어가겠습니다. GAAP 매출총이익률은 73%였고, non-GAAP 매출총이익률은 73.5%로 Blackwell 아키텍처의 첫 출하와 함께 예상대로 전분기 대비 하락했습니다.</td></tr>
<tr><td>As discussed last quarter, Blackwell is a customizable AI infrastructure with several different types of NVIDIA build chips multiple networking options and for air and liquid-cooled data center. We exceeded our expectations in Q4 in ramping Blackwell, increasing system availability, providing several configurations to our customers. As Blackwell ramps, we expect gross margins to be in the low 70s. Initially, we are focused on expediting the manufacturing of Blackwell systems to meet strong customer demand as they race to build out Blackwell infrastructure.</td><td>지난 분기에 논의했듯이, Blackwell은 여러 종류의 NVIDIA 빌드 칩과 다양한 네트워킹 옵션, 그리고 공랭식 및 수랭식 데이터센터를 위한 맞춤형 AI 인프라입니다. 4분기에 Blackwell 램프업에서 기대치를 상회했으며, 시스템 가용성을 높이고 고객들에게 여러 구성 옵션을 제공했습니다. Blackwell이 램프업됨에 따라 매출총이익률은 70% 초반대가 될 것으로 예상합니다. 현재 우리는 Blackwell 인프라 구축 경쟁에 나선 고객들의 강력한 수요를 충족하기 위해 Blackwell 시스템 제조를 신속히 진행하는 데 집중하고 있습니다.</td></tr>
<tr><td>When fully ramped, we have many opportunities to improve the cost and gross margin will improve and return to the mid-70s, late this fiscal year. Sequentially, GAAP operating expenses were up 9% and non-GAAP operating expenses were 11%, reflecting higher engineering development costs and higher compute and infrastructure costs for new product introductions. In Q4, we returned $8.1 billion to shareholders in the form of share repurchases and cash dividends. Let me turn to the outlook in the first quarter. Total revenue is expected to be $43 billion, plus or minus 2%. Continuing with its strong demand, we expect a significant ramp of Blackwell in Q1.</td><td>완전히 가동될 때, 우리는 비용을 개선할 수 있는 많은 기회를 가지고 있으며, 매출총이익률은 개선되어 이번 회계연도 말에 70% 중반대로 회복될 것입니다. 전분기 대비 GAAP 영업비용은 9% 증가했고, non-GAAP 영업비용은 11% 증가했는데, 이는 엔지니어링 개발비용 증가와 신제품 출시를 위한 컴퓨팅 및 인프라 비용 증가를 반영한 것입니다. 4분기에 우리는 자사주 매입과 현금배당의 형태로 주주들에게 81억 달러를 환원했습니다. 1분기 전망으로 넘어가겠습니다. 총매출은 430억 달러, 플러스마이너스 2%로 예상됩니다. 강력한 수요가 지속되면서, 1분기에 Blackwell의 대폭적인 램프업을 예상하고 있습니다.</td></tr>
<tr><td>We expect sequential growth in both Data Center and Gaming. Within Data Center, we expect sequential growth from both compute and networking. GAAP and non-GAAP gross margins are expected to be 70.6% and 71%, respectively, plus or minus 50 basis points. GAAP and non-GAAP operating expenses are expected to be approximately $5.2 billion and $3.6 billion, respectively. We expect full year fiscal year '26 operating expenses to grow to be in the mid-30s. GAAP and non-GAAP other incoming expenses are expected to be an income of approximately $400 million, excluding gains and losses from nonmarketable and publicly held equity securities.</td><td>데이터센터와 게이밍 모두에서 순차적 성장을 기대하고 있습니다. 데이터센터 내에서는 컴퓨팅과 네트워킹 모두에서 순차적 성장을 예상합니다. GAAP 및 non-GAAP 매출총이익률은 각각 70.6%와 71%로 예상되며, 플러스마이너스 50베이시스포인트입니다. GAAP 및 non-GAAP 영업비용은 각각 약 52억 달러와 36억 달러로 예상됩니다. 2026 회계연도 전체 영업비용은 30% 중반대 성장할 것으로 예상합니다. GAAP 및 non-GAAP 기타수익비용은 비상장 및 공개 지분증권의 손익을 제외하고 약 4억 달러의 수익으로 예상됩니다.</td></tr>
<tr><td>GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website, including a new financial information AI agent. In closing, let me highlight upcoming events for the financial community. We will be at the TD Cowen Health Care Conference in Boston on March 3 and at the Morgan Stanley Technology, Media and Telecom Conference in San Francisco on March 5. Please join us for our Annual GTC conference starting Monday, March 17 in San Jose, California.</td><td>GAAP 및 non-GAAP 세율은 개별 항목을 제외하고 17% 내외 1%로 예상됩니다. 추가적인 재무 세부사항은 새로운 금융정보 AI 에이전트를 포함하여 당사 IR 웹사이트에서 제공되는 CFO 코멘터리 및 기타 정보에 포함되어 있습니다. 마지막으로 금융 커뮤니티를 위한 향후 일정을 안내드리겠습니다. 3월 3일 보스턴에서 열리는 TD Cowen 헬스케어 컨퍼런스와 3월 5일 샌프란시스코에서 열리는 모건 스탠리 기술, 미디어 및 통신 컨퍼런스에 참석할 예정입니다. 3월 17일 월요일부터 캘리포니아 산호세에서 시작되는 연례 GTC 컨퍼런스에도 많은 참여 부탁드립니다.</td></tr>
<tr><td>Jensen will deliver a news-packed keynote on March 18, and we will host a Q&A session for our financial analysts for the next day, March 19. We look forward to seeing you at these events. Our earnings call to discuss the results for our first quarter of fiscal 2026 is scheduled for May 28, 2025. We are going to open up the call, operator, to questions. If you could start that, that would be great.</td><td>젠슨은 3월 18일에 뉴스가 가득한 기조연설을 진행할 예정이며, 다음 날인 3월 19일에는 금융 애널리스트들을 위한 Q&A 세션을 개최할 예정입니다. 이러한 행사에서 여러분을 만나뵐 수 있기를 기대합니다. 2026 회계연도 1분기 실적을 논의하는 어닝콜은 2025년 5월 28일로 예정되어 있습니다. 이제 질의응답을 위해 통화를 개방하겠습니다. 오퍼레이터님, 시작해 주시면 감사하겠습니다.</td></tr>
    </table>
    <h3>📌 요약</h3>
    <p style="background:#f0f0f0; padding:15px; border-left: 5px solid #333;">• 실적 하이라이트:<br>- 4분기 매출 393억 달러 (전분기 대비 12% 증가, 전년 대비 78% 증가)<br>- 데이터센터 부문 매출 356억 달러로 사상 최대 기록 (전분기 대비 16% 증가)<br>- FY2025 전체 매출 1,305억 달러 (전년 대비 114% 증가)<br><br>• Blackwell 신제품 출시 성과:<br>- 4분기 Blackwell 매출 110억 달러로 기대치 상회<br>- 회사 역사상 가장 빠른 제품 출시 및 확장<br>- 대형 클라우드 사업자들이 초기 도입 주도<br><br>• 향후 전망:<br>- 1분기 매출 가이던스 430억 달러 (±2%)<br>- 데이터센터와 게이밍 부문 모두 성장 전망<br>- 총이익률은 초기에는 70% 초반대 유지, 연말에는 중반대로 회복 예상<br><br>• 주요 리스크/기회 요인:<br>- 중국 수출 규제로 인한 매출 제한 지속<br>- Blackwell 생산 확대와 공급망 관리가 핵심 과제<br>- AI 인프라 수요 지속 확대 전망</p>
    <hr style="margin:50px 0;">
    

    <h2>❓ Q&A</h2>
    <table style="width:100%; border-collapse:collapse; margin-bottom: 40px;">
        <tr>
            <th style="width:50%; border-bottom: 2px solid #333;">Original</th>
            <th style="width:50%; border-bottom: 2px solid #333;">Translation</th>
        </tr>
        <tr><td>Question-and-Answer Session<br><br>Operator<br><br>Thank you. [Operator Instructions]. And your first question comes from CJ Muse with Cantor Fitzgerald. Please go ahead. C.J. Muse<br><br>Yeah, good afternoon. Thank you taking the question. I guess for me, Jensen, as Tepcom' compute and reinforcement learning shows such promise, we're clearly seeing an increasing blurring of the lines between training and inference -- what does this mean for the potential future of potentially inference dedicated clusters? And how do you think about the overall impact to NVIDIA and your customers? Thank you. Jensen Huang<br><br>Yes, I appreciate that C.J. There are now multiple scaling laws.</td><td>질의응답 세션<br><br>운영자<br><br>감사합니다. [운영자 안내사항]. 첫 번째 질문은 Cantor Fitzgerald의 CJ Muse님께서 주셨습니다. 말씀해 주세요.<br><br>C.J. Muse<br><br>네, 안녕하세요. 질문을 받아주셔서 감사합니다. 젠슨님께 질문드리겠습니다. 테스트 타임 컴퓨트(test-time compute)와 강화학습이 그토록 유망한 성과를 보이면서, 훈련(training)과 추론(inference) 간의 경계가 점점 모호해지고 있는 것을 분명히 목격하고 있습니다. 이것이 잠재적으로 추론 전용 클러스터의 미래에 어떤 의미를 갖는지, 그리고 NVIDIA와 고객들에게 미치는 전반적인 영향을 어떻게 생각하시는지 궁금합니다. 감사합니다.<br><br>젠슨 황<br><br>네, C.J.님, 좋은 질문 감사합니다. 이제 여러 개의 스케일링 법칙(scaling laws)이 존재합니다.</td></tr>
<tr><td>There's the pre-training scaling law, and that's going to continue to scale because we have multimodality, we have data that came from reasoning that are now used to do pretraining. And then the second is post-training scaling law, using reinforcement learning human feedback, reinforcement learning AI feedback, reinforcement learning, verifiable rewards. The amount of computation you use for post training is actually higher than pretraining. And it's kind of sensible in the sense that you could, while you're using reinforcement learning, generate an enormous amount of synthetic data or synthetically generated tokens. AI models are basically generating tokens to train AI models.</td><td>사전 훈련 스케일링 법칙이 있고, 이는 계속해서 확장될 것입니다. 왜냐하면 우리에게는 멀티모달리티가 있고, 추론에서 나온 데이터가 이제 사전 훈련에 사용되고 있기 때문입니다. 그리고 두 번째는 사후 훈련 스케일링 법칙으로, 인간 피드백을 통한 강화학습, AI 피드백을 통한 강화학습, 검증 가능한 보상을 통한 강화학습을 사용합니다. 사후 훈련에 사용하는 연산량은 실제로 사전 훈련보다 더 많습니다. 그리고 이는 어느 정도 합리적인데, 강화학습을 사용하는 동안 엄청난 양의 합성 데이터나 합성적으로 생성된 토큰을 만들어낼 수 있기 때문입니다. AI 모델들은 기본적으로 AI 모델들을 훈련시키기 위한 토큰을 생성하고 있습니다.</td></tr>
<tr><td>And that's post training. And the third part, this is the part that you mentioned is test time compute or reasoning, long thinking, inference scaling. They're all basically the same ideas. And there you have a chain of thought, you've search. The amount of tokens generated the amount of inference compute needed is already 100x more than the one-shot examples and the one-shot capabilities of large language models in the beginning. And that's just the beginning. This is just the beginning.</td><td>그리고 이는 훈련 후의 이야기입니다. 그리고 세 번째 부분, 이것이 바로 귀하께서 언급하신 테스트 타임 컴퓨트(test time compute) 또는 추론, 긴 사고, 추론 스케일링입니다. 이들은 모두 기본적으로 동일한 개념입니다. 그리고 여기서는 사고의 연쇄(chain of thought)와 검색이 있습니다. 생성되는 토큰의 양, 필요한 추론 컴퓨트의 양은 이미 초기 대형 언어 모델의 원샷 예시와 원샷 역량보다 100배 더 많습니다. 그리고 이것은 단지 시작일 뿐입니다. 이것은 정말 시작에 불과합니다.</td></tr>
<tr><td>The idea that the next generation could have thousands times and even hopefully, extremely thoughtful and simulation-based and search-based models that could be hundreds of thousands, millions of times more compute than today is in our future. And so the question is how do you design such an architecture? Some of the models are auto regressive. Some of the models are diffusion based. Some of the times you want your data center to have disaggregated inference. Sometimes it is compacted. And so it's hard to figure out what is the best configuration of a data center, which is the reason why NVIDIA's architecture is so popular. We run every model. We are great at training.</td><td>다음 세대에서는 수천 배, 그리고 바라건대 매우 사려 깊고 시뮬레이션 기반 및 검색 기반 모델들이 오늘날보다 수십만 배, 수백만 배 더 많은 컴퓨팅 파워를 가질 수 있다는 아이디어가 우리의 미래에 있습니다. 그래서 문제는 그러한 아키텍처를 어떻게 설계하느냐는 것입니다. 일부 모델들은 자기회귀적입니다. 일부 모델들은 확산 기반입니다. 때로는 데이터센터가 분산된 추론을 갖기를 원하고, 때로는 압축된 형태를 원합니다. 그래서 데이터센터의 최적 구성이 무엇인지 파악하기가 어렵습니다. 이것이 바로 NVIDIA의 아키텍처가 그토록 인기 있는 이유입니다. 우리는 모든 모델을 실행합니다. 우리는 훈련에 뛰어납니다.</td></tr>
<tr><td>The vast majority of our compute today is actually inference and Blackwell takes all of that to a new level. We designed Blackwell with the idea of reasoning models in mind. And when you look at training, it's many times more performing. But what's really amazing is for long thinking test time scaling, reasoning AI models were tens of times faster, 25 times higher throughput. And so Blackwell is going to be incredible across the board. And when you have a data center that allows you to configure and use your data center based on are you doing more pretraining now, post training now or scaling out your inference, our architecture is fungible and easy to use in all of those different ways.</td><td>현재 저희 컴퓨팅의 대부분은 실제로 추론(inference)이며, Blackwell은 이 모든 것을 새로운 차원으로 끌어올립니다. 저희는 추론 모델을 염두에 두고 Blackwell을 설계했습니다. 훈련을 살펴보면, 성능이 몇 배나 향상되었습니다. 하지만 정말 놀라운 것은 긴 사고 테스트 시간 스케일링(long thinking test time scaling)에서 추론 AI 모델이 25배 높은 처리량으로 수십 배 더 빨라졌다는 점입니다. 따라서 Blackwell은 전반적으로 놀라운 성과를 보일 것입니다. 그리고 현재 더 많은 사전 훈련을 하고 있는지, 사후 훈련을 하고 있는지, 아니면 추론을 확장하고 있는지에 따라 데이터센터를 구성하고 사용할 수 있는 데이터센터가 있다면, 저희 아키텍처는 이 모든 다양한 방식에서 대체 가능하고 사용하기 쉽습니다.</td></tr>
<tr><td>And so we're seeing, in fact, much, much more concentration of a unified architecture than ever before. Operator<br><br>Your next question comes from the line of Joe Moore with JPMorgan. Please go ahead. Joe Moore<br><br>Good morning, Morgan Stanley, actually. I wonder if you could talk about GB200 at CES, you sort of talked about the complexity of the rack level systems and the challenges you have. And then as you said in the prepared remarks, we've seen a lot of general availability -- where are you in terms of that ramp? Are there still bottlenecks to consider at a systems level above and beyond the chip level? And just have you maintained your enthusiasm for the NVL72 platforms?</td><td>그래서 실제로 우리는 그 어느 때보다도 통합 아키텍처의 집중도가 훨씬, 훨씬 더 높아지는 것을 보고 있습니다.<br><br>오퍼레이터<br><br>다음 질문은 JPMorgan의 Joe Moore님으로부터 받았습니다. 말씀해 주세요.<br><br>Joe Moore<br><br>안녕하세요, 사실은 Morgan Stanley입니다. CES에서의 GB200에 대해 말씀해 주실 수 있는지요. 랙 레벨 시스템의 복잡성과 직면하고 있는 과제들에 대해 언급하셨는데요. 그리고 준비된 발언에서 말씀하신 바와 같이, 많은 일반 공급 가능성을 보았습니다 -- 그 램프 측면에서 현재 어느 단계에 있으신지요? 칩 레벨을 넘어서 시스템 레벨에서 여전히 고려해야 할 병목 현상이 있나요? 그리고 NVL72 플랫폼에 대한 열정을 여전히 유지하고 계신가요?</td></tr>
<tr><td>Jensen Huang<br><br>Well, I'm more enthusiastic today than I was at CES. And the reason for that is because we've shipped a lot more since CES. We have some 350 plants manufacturing the 1.5 million components that go into each one of the Blackwell racks, Grace Blackwell racks. Yes, it's extremely complicated. And we successfully and incredibly ramped up Grace Blackwell, delivering some $11 billion of revenues last quarter. We're going to have to continue to scale as demand is quite high, and customers are anxious and impatient to get their Blackwell systems. You've probably seen on the web, a fair number of celebrations about Grace Blackwell systems coming online and we have them, of course.</td><td>젠슨 황<br><br>저는 CES 때보다 오늘 더욱 열정적입니다. 그 이유는 CES 이후 훨씬 더 많은 제품을 출하했기 때문입니다. 각각의 Blackwell 랙, Grace Blackwell 랙에 들어가는 150만 개의 부품을 제조하는 약 350개의 공장을 보유하고 있습니다. 네, 매우 복잡합니다. 그리고 우리는 Grace Blackwell을 성공적이고 놀랍도록 증산하여 지난 분기에 약 110억 달러의 매출을 달성했습니다. 수요가 상당히 높고 고객들이 Blackwell 시스템을 받기 위해 조급해하고 있어서 계속해서 규모를 확대해야 할 것입니다. 웹에서 Grace Blackwell 시스템이 온라인으로 가동되는 것에 대한 상당한 축하 소식들을 보셨을 텐데, 물론 저희가 그런 시스템들을 보유하고 있습니다.</td></tr>
<tr><td>We have a fairly large installation of Grace Blackwell goes for our own engineering and our own design teams and software teams. CoreWeave has now been quite public about the successful bring up of theirs. Microsoft has, of course, open AI has, and you're starting to see many come online. And so I think the answer to your question is nothing is easy about what we're doing, but we're doing great, and all of our partners are doing great. Operator<br><br>Your next question comes from the line of Vivek Arya with Bank of America Securities. Please go ahead. Vivek Arya<br><br>Thank you for taking my questions. Colette if you wouldn't mind confirming if Q1 is the bottom for gross margins?</td><td>저희는 자체 엔지니어링팀과 설계팀, 소프트웨어팀을 위해 Grace Blackwell을 상당히 대규모로 설치하여 운영하고 있습니다. CoreWeave는 이제 자사의 성공적인 구축에 대해 공개적으로 발표했습니다. Microsoft도 마찬가지이고, 물론 OpenAI도 그렇습니다. 그리고 많은 기업들이 온라인으로 서비스를 시작하는 것을 보고 있습니다. 따라서 귀하의 질문에 대한 답변은, 저희가 하고 있는 일이 쉬운 것은 없지만, 저희는 훌륭하게 해내고 있으며 모든 파트너들도 훌륭하게 해내고 있다는 것입니다.<br><br>오퍼레이터<br><br>다음 질문은 Bank of America Securities의 Vivek Arya 애널리스트로부터 받았습니다. 말씀해 주세요.<br><br>Vivek Arya<br><br>질문을 받아주셔서 감사합니다. Colette, 1분기가 매출총이익률의 바닥인지 확인해 주실 수 있을까요?</td></tr>
<tr><td>And then Jensen, my question is for you. What is on your dashboard to give you the confidence that the strong demand can sustain into next year? And has DeepSeek and whatever innovations they came up with, has that changed that view in any way? Thank you. Colette Kress<br><br>Let me first take the first part of the question there regarding the gross margin. During our Blackwell ramp, our gross margins will be in the low 70s. At this point, we are focusing on expediting our manufacturing, expediting our manufacturing to make sure that we can provide to customers as soon as possible. Our Blackwell is fully round.</td><td>그리고 젠슨, 제 질문은 당신을 위한 것입니다. 강력한 수요가 내년까지 지속될 수 있다는 확신을 주는 대시보드상의 지표는 무엇인가요? 그리고 DeepSeek와 그들이 내놓은 혁신들이 그러한 견해를 어떤 식으로든 바꾸었나요? 감사합니다.<br><br>**콜레트 크레스**<br><br>먼저 매출총이익률에 관한 질문의 첫 번째 부분부터 답변드리겠습니다. Blackwell 램프업 기간 동안 저희 매출총이익률은 70% 초반대가 될 것입니다. 현재 저희는 고객들에게 가능한 한 빨리 제품을 공급할 수 있도록 제조 공정을 가속화하는 데 집중하고 있습니다. 저희 Blackwell은 완전히 준비되었습니다.</td></tr>
<tr><td>And once it does -- I'm sorry, once our Blackwell fully rounds, we can improve our cost and our gross margin. So we expect to probably be in the mid-70s later this year. Walking through what you heard Jensen speak about the systems and their complexity, they are customizable in some cases. They've got multiple networking options. They have liquid cool and water cooled. So we know there is an opportunity for us to improve these gross margins going forward. But right now, we are going to focus on getting the manufacturing complete and to our customers as soon as possible.</td><td>그리고 일단 -- 죄송합니다, 일단 저희 Blackwell이 완전히 안정화되면, 비용을 개선하고 매출총이익률을 향상시킬 수 있습니다. 따라서 올해 후반에는 아마도 70% 중반대에 도달할 것으로 예상합니다. 젠슨이 말씀하신 시스템들과 그 복잡성을 살펴보면, 경우에 따라 맞춤형으로 제작 가능하고, 다양한 네트워킹 옵션을 갖추고 있으며, 액체 냉각과 수냉 방식을 지원합니다. 따라서 앞으로 이러한 매출총이익률을 개선할 기회가 있다는 것을 알고 있습니다. 하지만 지금 당장은 제조를 완료하고 가능한 한 빨리 고객들에게 제품을 공급하는 데 집중할 것입니다.</td></tr>
<tr><td>Jensen Huang<br><br>We know several things Vivek, we have a fairly good line of sight of the amount of capital investment that data centers are building out towards. We know that going forward, the vast majority of software is going to be based on machine learning. And so accelerated computing and generative AI, reasoning AI are going to be the type of architecture you want in your data center. We have, of course, forecast and plans from our top partners. And we also know that there are many innovative, really exciting start-ups that are still coming online as new opportunities for developing the next breakthroughs in AI, whether it's Agentic AIs, reasoning AI or physical AIs.</td><td>젠슨 황<br><br>비벡, 우리는 몇 가지 사실을 알고 있습니다. 데이터센터들이 구축하고 있는 자본 투자 규모에 대해 상당히 명확한 전망을 가지고 있습니다. 앞으로 소프트웨어의 대부분이 머신러닝을 기반으로 할 것이라는 점도 알고 있습니다. 따라서 가속 컴퓨팅과 생성형 AI, 추론 AI가 데이터센터에서 원하게 될 아키텍처 유형이 될 것입니다. 물론 우리의 주요 파트너들로부터 예측과 계획도 확보하고 있습니다. 또한 에이전틱 AI든, 추론 AI든, 물리적 AI든 AI의 차세대 혁신을 개발할 새로운 기회로서 여전히 온라인으로 등장하고 있는 혁신적이고 정말 흥미진진한 스타트업들이 많다는 것도 알고 있습니다.</td></tr>
<tr><td>The number of start-ups are still quite vibrant and each 1 of them need a fair amount of computing infrastructure. And so I think the -- whether it's the near term signals or the midterm signals, near-term signals, of course, are POs and forecasts and things like that. Midterm signals would be the level of infrastructure and CapEx scale-out compared to previous years. And then the long-term signals has to do with the fact that we know fundamentally software has changed from hand coding that runs on CPUs, to machine learning and AI-based software that runs on GPUs and accelerated computing systems. And so we have a fairly good sense that this is the future of software.</td><td>스타트업의 수는 여전히 매우 활발하며, 각각의 스타트업들은 상당한 양의 컴퓨팅 인프라를 필요로 합니다. 따라서 단기 신호든 중기 신호든 간에 - 단기 신호는 물론 PO(구매주문서)와 예측치 등과 같은 것들입니다. 중기 신호는 이전 연도 대비 인프라 및 자본지출 규모 확장 수준이 될 것입니다. 그리고 장기 신호는 소프트웨어가 근본적으로 CPU에서 실행되는 수작업 코딩에서 GPU와 가속 컴퓨팅 시스템에서 실행되는 머신러닝 및 AI 기반 소프트웨어로 변화했다는 사실과 관련이 있습니다. 따라서 우리는 이것이 소프트웨어의 미래라는 점에 대해 상당히 확신을 가지고 있습니다.</td></tr>
<tr><td>And then maybe as you roll it out, another way to think about that is we've really only tapped consumer AI and search and some amount of consumer generative AI, advertising, recommenders, kind of the early days of software. The next wave is coming, Agentic AI for enterprise, physical AI for robotics and sovereign AI as different regions build out their AI for their own ecosystems. And so each one of these are barely off the ground, and we can see them. We can see them because, obviously, we're in the center of much of this development and we can see great activity happening in all these different places and these will happen. So near term, midterm, long-term.</td><td>그리고 이를 확산시켜 나가면서, 다른 관점에서 생각해보면 우리는 실제로 소비자 AI와 검색, 그리고 어느 정도의 소비자 생성형 AI, 광고, 추천 시스템, 즉 소프트웨어의 초기 단계만을 활용해왔습니다. 다음 물결이 다가오고 있는데, 기업용 에이전틱 AI, 로보틱스를 위한 물리적 AI, 그리고 각 지역이 자체 생태계를 위한 AI를 구축하는 소버린 AI입니다. 이 각각의 영역들은 이제 막 시작 단계에 있으며, 우리는 이를 볼 수 있습니다. 우리가 이를 볼 수 있는 이유는 분명히 우리가 이러한 개발의 중심에 있기 때문이며, 이 모든 다양한 영역에서 활발한 활동이 일어나고 있는 것을 볼 수 있고, 이러한 일들이 실현될 것입니다. 따라서 단기, 중기, 장기적으로 말입니다.</td></tr>
<tr><td>Operator<br><br>Your next question comes from the line of Harlan Sur with JPMorgan. Please go ahead. Harlan Sur<br><br>Good afternoon. Thanks for taking my question. Your next-generation Blackwell Ultra is set to launch in the second half of this year, in line with the team's annual product cadence. Jensen, can you help us understand the demand dynamics for Ultra given that you'll still be ramping the current generation Blackwell solutions? How do your customers and the supply chain also manage the simultaneous ramps of these two products? And -- is the team still on track to execute Blackwell Ultra in the second half of this year? Jensen Huang<br><br>Yes. Blackwell Ultra is second half.</td><td>운영자<br><br>다음 질문은 JPMorgan의 Harlan Sur님으로부터 받았습니다. 말씀해 주세요.<br><br>Harlan Sur<br><br>안녕하세요. 질문을 받아주셔서 감사합니다. 차세대 Blackwell Ultra가 올해 하반기 출시 예정으로, 팀의 연간 제품 출시 주기에 맞춰 진행되고 있습니다. Jensen님, 현재 세대 Blackwell 솔루션을 여전히 증산하고 있는 상황에서 Ultra에 대한 수요 역학을 이해할 수 있도록 도와주실 수 있나요? 고객들과 공급망이 이 두 제품의 동시 증산을 어떻게 관리하고 있는지, 그리고 팀이 여전히 올해 하반기 Blackwell Ultra 실행 계획을 순조롭게 진행하고 있는지 궁금합니다.<br><br>Jensen Huang<br><br>네. Blackwell Ultra는 하반기입니다.</td></tr>
<tr><td>As you know, the first Blackwell was we had a hiccup that probably cost us a couple of months. We're fully recovered, of course. The team did an amazing job recovering and all of our supply chain partners and just so many people helped us recover at the speed of light. And so now we've successfully ramped production of Blackwell. But that doesn't stop the next train. The next train is on an annual rhythm and Blackwell Ultra with new networking, new memories and of course, new processors, and all of that is coming online. We've have been working with all of our partners and customers, laying this out.</td><td>아시다시피, 첫 번째 Blackwell에서 우리는 몇 달 정도의 손실을 가져온 차질을 겪었습니다. 물론 지금은 완전히 회복되었습니다. 팀이 회복 과정에서 놀라운 성과를 보였고, 모든 공급망 파트너들과 정말 많은 분들이 빛의 속도로 회복할 수 있도록 도와주셨습니다. 그래서 이제 우리는 Blackwell의 생산을 성공적으로 증산했습니다. 하지만 그렇다고 해서 다음 열차가 멈추는 것은 아닙니다. 다음 열차는 연간 주기로 운행되며, 새로운 네트워킹, 새로운 메모리, 그리고 물론 새로운 프로세서를 탑재한 Blackwell Ultra가 출시될 예정입니다. 이 모든 것들이 온라인으로 제공될 것입니다. 우리는 모든 파트너사와 고객사들과 함께 이를 계획해 왔습니다.</td></tr>
<tr><td>They have all of the necessary information, and we'll work with everybody to do the proper transition. This time between Blackwell and Blackwell Ultra, the system architecture is exactly the same. It's a lot harder going from Hopper to Blackwell because we went from an NVLink 8 system to a NVLink 72-based system. So the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change. This was quite a challenging transition. But the next transition will slot right in Blackwell Ultra will slot right in. We've also already revealed and been working very closely with all of our partners on the click after that.</td><td>그들은 모든 필요한 정보를 가지고 있으며, 저희는 모든 관계자들과 협력하여 적절한 전환을 진행할 것입니다. 이번 Blackwell과 Blackwell Ultra 사이의 전환에서는 시스템 아키텍처가 정확히 동일합니다. Hopper에서 Blackwell로 전환하는 것이 훨씬 더 어려웠는데, 이는 NVLink 8 시스템에서 NVLink 72 기반 시스템으로 변경되었기 때문입니다. 따라서 섀시, 시스템 아키텍처, 하드웨어, 전력 공급 등 모든 것이 변경되어야 했습니다. 이는 상당히 도전적인 전환이었습니다. 하지만 다음 전환은 바로 적용될 것입니다. Blackwell Ultra가 바로 슬롯에 들어갈 것입니다. 저희는 또한 그 다음 세대에 대해서도 이미 공개했으며 모든 파트너들과 매우 긴밀하게 협력하고 있습니다.</td></tr>
<tr><td>And the click after that is called Vera Rubin and all of our partners are getting up to speed on the transition of that and so preparing for that transition. And again, we're going to provide a big, huge step-up. And so come to GTC, and I'll talk to you about Blackwell Ultra, Vera Rubin and then show you what we place after that. Really exciting new products, so to come to GTC piece. Operator<br><br>Your next question comes from the line of Timothy Arcuri with UBS. Please go ahead. Timothy Arcuri<br><br>Thanks a lot. Jensen, we heard a lot about custom ASICs. Can you kind of speak to the balance between customer ASIC and merchant GPU.</td><td>그리고 그 다음 클릭은 Vera Rubin이라고 불리며, 저희의 모든 파트너들이 해당 전환에 대비하여 속도를 높이고 있고 그 전환을 준비하고 있습니다. 그리고 다시 말씀드리지만, 저희는 큰 폭의 성능 향상을 제공할 예정입니다. GTC에 오시면 Blackwell Ultra, Vera Rubin에 대해 말씀드리고, 그 다음에 저희가 출시할 제품들을 보여드리겠습니다. 정말 흥미진진한 신제품들이니 GTC에 꼭 오시기 바랍니다.<br><br>오퍼레이터<br><br>다음 질문은 UBS의 Timothy Arcuri님께서 주시겠습니다. 말씀해 주세요.<br><br>Timothy Arcuri<br><br>감사합니다. Jensen, 커스텀 ASIC에 대해 많이 들었는데요. 고객 맞춤형 ASIC과 범용 GPU 간의 균형에 대해 말씀해 주실 수 있나요?</td></tr>
<tr><td>We hear about some of these heterogeneous superclusters to use both GPU and ASIC? Is that something customers are planning on building? Or will these infrastructures remain fairly distinct. Thanks. Jensen Huang<br><br>Well, we built very different things than ASICs, in some ways, completely different in some areas we intercept. We're different in several ways. One, NVIDIA's architecture is general whether you're -- you've optimized for unaggressive models or diffusion-based models or vision-based models or multimodal models or text models. We're great in all of it.</td><td>이러한 이종 슈퍼클러스터들이 GPU와 ASIC을 모두 사용한다는 얘기를 듣고 있는데요? 고객들이 이런 것을 구축할 계획이 있는 건가요? 아니면 이러한 인프라들이 상당히 구별되어 남아있을 건가요? 감사합니다.<br><br>젠슨 황<br><br>저희는 ASIC과는 매우 다른 것들을 구축했습니다. 어떤 면에서는 완전히 다르고, 일부 영역에서는 교차점이 있습니다. 저희는 여러 면에서 다릅니다. 첫째, NVIDIA의 아키텍처는 범용적입니다. 어그레시브 모델에 최적화되어 있든, 디퓨전 기반 모델이든, 비전 기반 모델이든, 멀티모달 모델이든, 텍스트 모델이든 상관없이 저희는 모든 영역에서 뛰어납니다.</td></tr>
<tr><td>We're great on all of it because our software stack is so -- our architecture is sensible, our software stack ecosystem is so rich that were the initial target of most exciting innovations and algorithms. And so by definition, we're much, much more general than narrow. We're also really good from the end-to-end from data processing, the curation of the training data, to the training of the data, of course, to reinforcement learning used in post training, all the way to inference with tough time scaling. So we're general, we're end-to-end, and we're everywhere. And because we're not in just one cloud, we're in every cloud, we could be on-prem. We could be in a robot.</td><td>저희는 모든 영역에서 뛰어난 성과를 보이고 있습니다. 왜냐하면 저희의 소프트웨어 스택이 매우 - 저희의 아키텍처가 합리적이고, 소프트웨어 스택 생태계가 매우 풍부하기 때문에 가장 흥미진진한 혁신과 알고리즘들의 초기 타겟이 되고 있습니다. 따라서 정의상 저희는 협소한 것보다 훨씬, 훨씬 더 범용적입니다. 또한 저희는 데이터 처리부터 훈련 데이터의 큐레이션, 데이터 훈련, 물론 포스트 트레이닝에 사용되는 강화학습, 그리고 실시간 스케일링을 통한 추론까지 엔드투엔드로 정말 뛰어납니다. 따라서 저희는 범용적이고, 엔드투엔드이며, 어디에나 있습니다. 그리고 저희는 단 하나의 클라우드에만 있는 것이 아니라 모든 클라우드에 있고, 온프레미스에도 있을 수 있으며, 로봇에도 있을 수 있습니다.</td></tr>
<tr><td>Our architecture is much more accessible and a great target initial target for anybody who's starting up a new company. And so we're everywhere. And the third thing I would say is that our performance in our rhythm is so incredibly fast. Remember that these data centers are always fixed in size. They're fixed in size or they're fixing power. And if our performance per watt is anywhere from 2x to 4x to 8x, which is not unusual, it translates directly to revenues. And so if you have a 100-megawatt data center, if the performance or the throughput in that 100-megawatt or the gigawatt data center is 4 times or 8 times higher, your revenues for that gigawatt data center is 8 times higher.</td><td>우리의 아키텍처는 훨씬 더 접근하기 쉽고, 새로운 회사를 시작하는 누구에게나 훌륭한 초기 타겟이 됩니다. 그래서 우리는 어디에나 있습니다. 세 번째로 말씀드리고 싶은 것은 우리의 성능과 리듬이 믿을 수 없을 정도로 빠르다는 것입니다. 이러한 데이터센터들은 항상 크기가 고정되어 있다는 점을 기억하세요. 크기가 고정되어 있거나 전력이 고정되어 있습니다. 그리고 우리의 와트당 성능이 2배에서 4배, 8배에 이르는데, 이는 드문 일이 아니며, 이것이 직접적으로 매출로 이어집니다. 따라서 100메가와트 데이터센터가 있다면, 그 100메가와트 또는 기가와트 데이터센터의 성능이나 처리량이 4배 또는 8배 높다면, 그 기가와트 데이터센터의 매출은 8배 높아집니다.</td></tr>
<tr><td>And the reason that is so different than data centers of the past is because AI factories are directly monetizable through its tokens generated. And so the token throughput of our architecture being so incredibly fast is just incredibly valuable to all of the companies that are building these things for revenue generation reasons and capturing the fast ROI. And so I think the third reason is performance. And then the last thing that I would say is the software stack is incredibly hard. Building an ASIC is no different than what we do. We build a new architecture. And the ecosystem that sits on top of our architecture is 10x more complex today than it was 2 years ago.</td><td>그리고 이것이 과거의 데이터센터와 매우 다른 이유는 AI 팩토리가 생성되는 토큰을 통해 직접적으로 수익화가 가능하기 때문입니다. 따라서 우리 아키텍처의 토큰 처리량이 매우 빠르다는 것은 수익 창출 목적과 빠른 ROI 확보를 위해 이러한 시설을 구축하는 모든 기업들에게 엄청나게 가치 있는 것입니다. 그래서 세 번째 이유는 성능이라고 생각합니다. 그리고 마지막으로 말씀드리고 싶은 것은 소프트웨어 스택이 매우 어렵다는 점입니다. ASIC을 구축하는 것은 우리가 하는 일과 다르지 않습니다. 우리는 새로운 아키텍처를 구축합니다. 그리고 우리 아키텍처 위에 있는 생태계는 2년 전보다 오늘날 10배 더 복잡해졌습니다.</td></tr>
<tr><td>And that's fairly obvious because the amount of software that the world is building on top of architecture is growing exponentially and AI is advancing very quickly. So bringing that whole ecosystem on top of multiple chips is hard. And so I would say that those four reasons. And then finally, I will say this, just because the chip is designed doesn’t mean it gets deployed. And you’ve seen this over and over again. There are a lot of chips that gets built, but when the time comes, a business decision has to be made, and that business decision is about deploying a new engine, a new processor into a limited AI factory in size, in power and in fine.</td><td>그리고 이는 상당히 명백한데, 왜냐하면 전 세계가 아키텍처 위에 구축하고 있는 소프트웨어의 양이 기하급수적으로 증가하고 있고 AI가 매우 빠르게 발전하고 있기 때문입니다. 따라서 그 전체 생태계를 여러 칩 위에 구현하는 것은 어렵습니다. 그래서 저는 이러한 네 가지 이유를 말씀드리겠습니다. 그리고 마지막으로 이것을 말씀드리겠습니다. 칩이 설계되었다고 해서 배포되는 것은 아닙니다. 그리고 여러분은 이를 반복해서 보아왔습니다. 제작되는 칩은 많지만, 막상 그 시점이 되면 비즈니스 결정을 내려야 하고, 그 비즈니스 결정은 크기, 전력, 그리고 파인(fine) 측면에서 제한된 AI 팩토리에 새로운 엔진, 새로운 프로세서를 배포하는 것에 관한 것입니다.</td></tr>
<tr><td>And our technology is not only more advanced, more performance, it has much, much better software capability and very importantly, our ability to deploy is lightning fast. And so these things are enough for the faint of heart, as everybody knows now. And so there’s a lot of different reasons why we do well, why we win. Operator<br><br>Your next question comes from the line of Ben Reitzes with Melius Research. Please go ahead. Ben Reitzes<br><br>Yeah, I've been right to this here. Hey, thanks a lot for the question. Hi, Jensen, it's a geography-related question, you did a great job explaining some of the demand underlying factors here on the strength. But U.S. was up about $5 billion or so sequentially.</td><td>그리고 우리의 기술은 더 발전되고 성능이 뛰어날 뿐만 아니라, 훨씬 더 우수한 소프트웨어 역량을 갖추고 있으며, 매우 중요한 점은 우리의 배포 능력이 매우 빠르다는 것입니다. 그리고 이런 것들은 모든 사람이 이제 알고 있듯이 마음이 약한 사람들에게는 충분하지 않습니다. 그래서 우리가 잘하고 승리하는 이유는 여러 가지가 있습니다. <br><br>오퍼레이터<br><br>다음 질문은 멜리우스 리서치의 벤 라이체스님으로부터 받았습니다. 말씀해 주세요.<br><br>벤 라이체스<br><br>네, 여기서 바로 질문드리겠습니다. 질문 기회를 주셔서 감사합니다. 안녕하세요, 젠슨. 지역과 관련된 질문입니다. 여기서 강세의 근본적인 수요 요인들에 대해 훌륭하게 설명해 주셨는데요. 하지만 미국은 전분기 대비 약 50억 달러 정도 증가했습니다.</td></tr>
<tr><td>And I think there is a concern about whether U.S. can pick up the slack if there's regulations towards other geographies. And I was just wondering, as we go throughout the year, if this kind of surge in the U.S. continues and it's going to be -- whether that's okay. And if that underlies your growth rate, how can you keep growing so fast with this mix shift towards the U.S.? Your guidance looks like China is probably up sequentially. So just wondering if you could go through that dynamic and maybe collect can weigh in. . Jensen Huang<br><br>China is approximately the same percentage as Q4 and as previous quarters. It's about half of what it was before the export control.</td><td>그리고 다른 지역에 대한 규제가 있을 경우 미국이 그 공백을 메울 수 있을지에 대한 우려가 있다고 생각합니다. 올해 내내 미국에서 이런 급증세가 계속되고, 그것이 괜찮을지 궁금했습니다. 그리고 만약 이것이 귀하의 성장률을 뒷받침한다면, 미국으로의 이런 믹스 변화로 어떻게 이렇게 빠른 성장을 계속 유지할 수 있을까요? 귀하의 가이던스를 보면 중국이 아마 순차적으로 증가할 것 같습니다. 그래서 이런 역학관계에 대해 설명해 주시고, 콜렛이 의견을 제시할 수 있는지 궁금합니다.<br><br>젠슨 황:<br><br>중국은 4분기 및 이전 분기들과 거의 동일한 비율입니다. 수출 통제 이전의 약 절반 수준입니다.</td></tr>
<tr><td>But it's approximately the same in percentage. With respect to geographies, the takeaway is that AI is software. It's modern software. It's incredible modern software, but it's modern software and AI has gone mainstream. AI is used in delivery services everywhere, shopping services everywhere. If you were to buy a quarter, from milk is delivered to you, AI was involved. And so almost everything that a consumer service provides AIs at the core of it. Every student will use AI as a tutor, health care services use AI, financial services use AI. No fintech company will not use AI. Every Fintech company will. Climate tech company use AI. Mineral discovery now uses AI.</td><td>하지만 비율로 보면 거의 동일합니다. 지역별로 보면, 핵심은 AI가 소프트웨어라는 것입니다. 현대적인 소프트웨어죠. 놀라울 정도로 현대적인 소프트웨어이지만, 결국 현대적인 소프트웨어이고 AI는 주류가 되었습니다. AI는 모든 곳의 배송 서비스, 모든 곳의 쇼핑 서비스에 사용됩니다. 우유 한 팩을 주문해서 배송받는다면, 그 과정에 AI가 관여되어 있습니다. 따라서 소비자 서비스가 제공하는 거의 모든 것의 핵심에 AI가 있습니다. 모든 학생이 AI를 튜터로 사용할 것이고, 의료 서비스는 AI를 사용하며, 금융 서비스도 AI를 사용합니다. AI를 사용하지 않는 핀테크 회사는 없을 것입니다. 모든 핀테크 회사가 사용할 것입니다. 기후 기술 회사들도 AI를 사용합니다. 광물 탐사에도 이제 AI가 사용됩니다.</td></tr>
<tr><td>The number of -- every higher education, every university uses AI and so I think it is fairly safe to say that AI has gone mainstream and that it's being integrated into every application. And -- and our hope is that, of course, the technology continues to advance safely and advance in a helpful way to society. And with that, we're -- I do believe that we're at the beginning of this new transition. And what I mean by that in the beginning is, remember, behind us has been decades of data centers and decades of computers that have been built. And they've been built for a world of hand coding and general purpose computing and CPUs and so on and so forth.</td><td>모든 고등교육기관, 모든 대학이 AI를 사용하고 있으며, 따라서 AI가 주류로 자리잡았고 모든 애플리케이션에 통합되고 있다고 말하는 것이 상당히 안전하다고 생각합니다. 그리고 물론 기술이 안전하게 계속 발전하고 사회에 도움이 되는 방향으로 발전하기를 희망합니다. 이와 함께, 우리는 이 새로운 전환의 시작점에 있다고 믿습니다. 시작점에 있다는 것의 의미는, 기억하시겠지만 우리 뒤에는 수십 년간의 데이터센터와 수십 년간 구축된 컴퓨터들이 있습니다. 그리고 이들은 수작업 코딩과 범용 컴퓨팅, CPU 등등의 세계를 위해 구축되었습니다.</td></tr>
<tr><td>And going forward, I think it's fairly safe to say that world is going to be almost all software to be infused with AI. All software and all services will be based on -- ultimately, based on machine learning, the data flywheel is going to be part of improving software and services and that the future computers will be accelerated, the future computers will be based on AI. And we're really two years into that journey. And in modernizing computers that have taken decades to build out. And so I'm fairly sure that we're in the beginning of this new era. And then lastly, no technology has ever had the opportunity to address a larger part of the world's GDP than AI. No software tool ever has.</td><td>앞으로 나아가면서, 전 세계 거의 모든 소프트웨어가 AI로 융합될 것이라고 말하는 것이 상당히 안전하다고 생각합니다. 모든 소프트웨어와 모든 서비스는 궁극적으로 머신러닝을 기반으로 할 것이며, 데이터 플라이휠이 소프트웨어와 서비스 개선의 일부가 될 것입니다. 그리고 미래의 컴퓨터는 가속화될 것이고, 미래의 컴퓨터는 AI를 기반으로 할 것입니다. 우리는 현재 그 여정의 2년차에 있습니다. 그리고 수십 년에 걸쳐 구축된 컴퓨터를 현대화하는 과정에 있습니다. 따라서 우리가 이 새로운 시대의 시작점에 있다고 확신합니다. 마지막으로, AI보다 세계 GDP의 더 큰 부분을 다룰 기회를 가진 기술은 지금까지 없었습니다. 그 어떤 소프트웨어 도구도 그렇지 못했습니다.</td></tr>
<tr><td>And so this is now a software tool that can address a much larger part of the world's GDP more than any time in history. And so the way we think about growth and the way we think about whether something is big or small, has to be in the context of that. And when you take a step back and look at it from that perspective, we're really just in the beginning. Operator<br><br>Your next question comes from the line of Aaron Rakers with Wells Fargo. Aaron your line is open. Your next question comes from Mark Lipacis with Evercore ISI. Please go ahead. Mark Lipacis<br><br>I had a clarification and a question. Colette up for the clarification.</td><td>그래서 이제 이것은 역사상 그 어느 때보다도 전 세계 GDP의 훨씬 더 큰 부분을 다룰 수 있는 소프트웨어 도구입니다. 따라서 우리가 성장에 대해 생각하는 방식과 어떤 것이 크거나 작다고 생각하는 방식은 그러한 맥락에서 이해되어야 합니다. 그리고 한 발 물러서서 그런 관점에서 바라보면, 우리는 정말로 시작 단계에 불과합니다.<br><br>오퍼레이터<br><br>다음 질문은 Wells Fargo의 Aaron Rakers님으로부터 받았습니다. Aaron님, 연결되었습니다. 다음 질문은 Evercore ISI의 Mark Lipacis님으로부터 받았습니다. 말씀해 주세요.<br><br>Mark Lipacis<br><br>명확히 하고 싶은 것과 질문이 하나씩 있습니다. 명확히 하고 싶은 것은 Colette에게 드리겠습니다.</td></tr>
<tr><td>Did you say that enterprise within the data center grew 2x year-on-year for the January quarter? And if so, does that -- would that make it the fast faster growing than the hyperscalers? And then, Jensen, for you, the question, hyperscalers are the biggest purchasers of your solutions, but they buy equipment for both internal and external workloads, external workflows being cloud services that enterprise is used. So the question is, can you give us a sense of how that hyperscaler spend splits between that external workload and internal? And as these new AI workflows and applications come up, would you expect enterprises to become a larger part of that consumption mix?</td><td>1월 분기에 데이터센터 내 엔터프라이즈 부문이 전년 동기 대비 2배 성장했다고 말씀하신 건가요? 만약 그렇다면, 이것이 하이퍼스케일러보다 더 빠른 성장률을 의미하는 건가요? <br><br>그리고 젠슨에게 드리는 질문인데요, 하이퍼스케일러들이 귀하의 솔루션의 최대 구매자이지만, 이들은 내부 워크로드와 외부 워크로드 모두를 위한 장비를 구매합니다. 외부 워크플로우는 기업이 사용하는 클라우드 서비스를 의미하죠. 그래서 질문은, 하이퍼스케일러의 지출이 외부 워크로드와 내부 워크로드 사이에서 어떻게 분할되는지에 대한 감을 주실 수 있나요? 그리고 이러한 새로운 AI 워크플로우와 애플리케이션들이 등장함에 따라, 기업들이 그 소비 믹스에서 더 큰 부분을 차지하게 될 것으로 예상하시나요?</td></tr>
<tr><td>And does that impact how you develop your service, your ecosystem. Colette Kress<br><br>Sure. Thanks for the question regarding our Enterprise business. Yes, it grew to X and very similar to what we were seeing with our large CSPs. Keep in mind, these are both important areas to understand working with the CSPs and be working on large language models, can you working on inference in their own work. But keep in mind, that is also where the enterprises are servicing. Your enterprises are both with your CSPs as well as in terms of building on their own. They're both are growing quite well. Jensen Huang<br><br>The CSPs are about half of our business.</td><td>그리고 그것이 귀하의 서비스, 생태계 개발에 어떤 영향을 미치는지요?<br><br>콜레트 크레스<br><br>네. 저희 엔터프라이즈 사업에 대한 질문 감사합니다. 네, X까지 성장했으며 대형 CSP들에서 보고 있는 것과 매우 유사합니다. 이 두 영역 모두 중요한 부분이라는 점을 염두에 두시기 바랍니다. CSP들과 협력하고 대규모 언어 모델을 작업하며, 자체적인 추론 작업을 수행하는 것 말입니다. 하지만 기업들이 서비스를 제공하는 곳이기도 하다는 점을 기억해 주세요. 기업들은 CSP들과 협력하는 동시에 자체적으로 구축하는 작업도 하고 있습니다. 두 영역 모두 상당히 잘 성장하고 있습니다.<br><br>젠슨 황<br><br>CSP들은 저희 사업의 약 절반을 차지합니다.</td></tr>
<tr><td>And the CSPs have internal consumption and external consumption, as you say. And we are using -- of course, used for internal consumption. We work very closely with all of them to optimize workloads that are internal to them, because they have a large infrastructure of NVIDIA gear that they could take advantage of. And the fact that we could be used for AI on the one hand, video processing on the other hand, data processing like Spark, we're fungible. And so the useful life of our infrastructure is much better. If the useful life is much longer, then the TCO is also lower. And so -- the second part is how do we see the growth of enterprise or not CSPs, if you will, going forward?</td><td>그리고 CSP들은 말씀하신 대로 내부 소비와 외부 소비를 모두 가지고 있습니다. 저희는 물론 내부 소비에 사용되고 있습니다. 저희는 모든 CSP들과 매우 긴밀하게 협력하여 그들의 내부 워크로드를 최적화하고 있습니다. 왜냐하면 그들은 활용할 수 있는 대규모 NVIDIA 장비 인프라를 보유하고 있기 때문입니다. 그리고 저희가 한편으로는 AI에, 다른 한편으로는 비디오 처리에, 그리고 Spark와 같은 데이터 처리에 사용될 수 있다는 사실 때문에 저희는 대체 가능합니다. 따라서 저희 인프라의 유용한 수명이 훨씬 더 좋습니다. 유용한 수명이 훨씬 길다면 TCO도 더 낮아집니다. 그리고 두 번째 부분은 앞으로 기업 또는 CSP가 아닌 부문의 성장을 어떻게 보고 있는지에 관한 것입니다.</td></tr>
<tr><td>And the answer is, I believe, long-term, it is by far larger and the reason for that is because if you look at the computer industry today and what is not served by the computer industry is largely industrial. So let me give you an example. When we say enterprise, and let's use the car company as an example because they make both soft things and hard things. And so in the case of a car company, the employees will be what we call enterprise and Agentic AI and software planning systems and tools, and we have some really exciting things to share with you guys at GTC, build Agentic systems are for employees to make employees more productive to design to market plan to operate their company.</td><td>그리고 제가 생각하는 답은 장기적으로 볼 때 산업용이 훨씬 더 크다는 것입니다. 그 이유는 오늘날 컴퓨터 산업을 살펴보면 컴퓨터 산업이 서비스하지 못하는 영역이 주로 산업 분야이기 때문입니다. 예를 들어 설명해드리겠습니다. 우리가 기업용이라고 할 때, 자동차 회사를 예로 들어보겠습니다. 왜냐하면 그들은 소프트웨어와 하드웨어 모두를 만들기 때문입니다. 자동차 회사의 경우, 직원들은 우리가 기업용이라고 부르는 것에 해당하며, 에이전틱 AI와 소프트웨어 계획 시스템 및 도구들이 있습니다. 그리고 GTC에서 여러분들과 공유할 정말 흥미진진한 것들이 있는데, 에이전틱 시스템을 구축하는 것은 직원들이 더 생산적으로 일할 수 있도록, 설계하고 마케팅하고 계획하고 회사를 운영할 수 있도록 하기 위한 것입니다.</td></tr>
<tr><td>That's Agentic AI. On the other hand, the cars that they manufacture also need AI. They need an AI system that trains the cars, treats this entire giant fleet of cars. And today, there's 1 billion cars on the road. Someday, there will be 1 billion cars on the road, and every single one of those cars will be robotic cars, and they'll all be collecting data, and we'll be improving them using an AI factory. Whereas they have a car factory today in the future they'll have a car factory and an AI factory. And then inside the car itself is a robotic system. And so as you can see, there are three computers involved and there's the computer that helps the people.</td><td>그것이 바로 에이전틱 AI입니다. 한편으로는 그들이 제조하는 자동차들도 AI가 필요합니다. 자동차들을 훈련시키고, 이 거대한 자동차 플릿 전체를 관리하는 AI 시스템이 필요한 것입니다. 현재 도로에는 10억 대의 자동차가 있습니다. 언젠가는 도로에 10억 대의 자동차가 있을 것이고, 그 모든 자동차들이 로봇 자동차가 될 것이며, 모두 데이터를 수집하고 있을 것입니다. 그리고 우리는 AI 팩토리를 사용해서 이들을 개선해 나갈 것입니다. 현재 그들이 자동차 공장을 가지고 있는 반면, 미래에는 자동차 공장과 AI 팩토리를 모두 갖게 될 것입니다. 그리고 자동차 내부에는 로봇 시스템이 있습니다. 보시다시피, 여기에는 세 개의 컴퓨터가 관련되어 있고, 사람들을 돕는 컴퓨터가 있습니다.</td></tr>
<tr><td>There's the computer that build the AI for the machineries that could be, of course, could be a tractor, it could be a lawn mower. It could be a human or robot that's being developed today. It could be a building, it could be a warehouse. These physical systems require new type of AI we call physical AI. They can't just understand the meaning of words and languages, but they have to understand the meaning of the world, friction and inertia, object permanence and cause and effect. And all of those type of things that are common sense to you and I, but AIs have to go learn those physical effects. So we call that physical AI.</td><td>기계를 위한 AI를 구축하는 컴퓨터가 있습니다. 물론 이는 트랙터가 될 수도 있고, 잔디깎이가 될 수도 있습니다. 오늘날 개발되고 있는 휴머노이드나 로봇이 될 수도 있고요. 건물이나 창고가 될 수도 있습니다. 이러한 물리적 시스템들은 우리가 피지컬 AI라고 부르는 새로운 유형의 AI를 필요로 합니다. 이들은 단순히 단어와 언어의 의미를 이해하는 것이 아니라, 세상의 의미, 마찰과 관성, 객체 영속성, 그리고 인과관계를 이해해야 합니다. 여러분과 저에게는 상식적인 이 모든 것들을 AI는 학습해야 하는 물리적 효과들입니다. 그래서 우리는 이를 피지컬 AI라고 부릅니다.</td></tr>
<tr><td>That whole part of using Agentic AI to revolutionize the way we work inside companies, that's just starting. This is now the beginning of the agent AI era, and you hear a lot of people talking about it and we got some really great things going on. And then there's the physical AI after that, and then there are robotic systems after that. And so these 3 computers are all brand new. And my sense is that long term, this will be by far the larger of a mall, which kind of makes sense. The world’s GDP is representing – represented by either heavy industries or industrials and companies that are providing for those. Operator<br><br>Your next question comes from the line of Aaron Rakers with Wells Fargo.</td><td>기업 내부에서 일하는 방식을 혁신하기 위해 에이전틱 AI를 활용하는 전체적인 부분은 이제 막 시작되고 있습니다. 지금이 바로 에이전트 AI 시대의 시작이며, 많은 사람들이 이에 대해 이야기하고 있고 저희도 정말 훌륭한 일들을 진행하고 있습니다. 그 다음에는 물리적 AI가 있고, 그 이후에는 로봇 시스템들이 있습니다. 이 3가지 컴퓨터는 모두 완전히 새로운 것들입니다. 그리고 제 생각에는 장기적으로 이것이 전체 중에서 단연코 가장 큰 부분이 될 것이라고 봅니다. 이는 어느 정도 타당합니다. 세계 GDP는 중공업이나 산업 분야, 그리고 이들을 지원하는 기업들로 구성되어 있기 때문입니다.<br><br>오퍼레이터<br><br>다음 질문은 Wells Fargo의 Aaron Rakers님으로부터 받겠습니다.</td></tr>
<tr><td>Please go ahead. Aaron Rakers<br><br>Thanks for let me back in. Jensen, I'm curious as we now approach the 2-year anniversary of really the Hopper inflection that you saw in 2023 in GenAI in general. And when we think about the road map you have in front of us, how do you think about the infrastructure that's been deployed from a replacement cycle perspective? And whether if it's GB300 or if it's the Rubin cycle where we start to see maybe some refresh opportunity. I'm just curious how you look at that. Jensen Huang<br><br>I appreciate it. First of all, people are still using Voltus and Pascal and amperes.</td><td>계속해 주세요.<br><br>**Aaron Rakers**<br><br>다시 질문할 기회를 주셔서 감사합니다. 젠슨, 2023년 GenAI 전반과 정말로 Hopper 변곡점을 보았던 때로부터 이제 2주년이 다가오고 있는 시점에서 궁금한 점이 있습니다. 우리 앞에 있는 로드맵을 생각해볼 때, 교체 주기 관점에서 배포된 인프라에 대해 어떻게 생각하시는지요? 그리고 GB300이든 아니면 일부 리프레시 기회를 보기 시작할 수 있는 Rubin 사이클이든, 이에 대해 어떻게 보고 계신지 궁금합니다.<br><br>**Jensen Huang**<br><br>감사합니다. 우선, 사람들은 여전히 Volta와 Pascal, 그리고 Ampere를 사용하고 있습니다.</td></tr>
<tr><td>And the reason for that is because there are always things that because CUDA is so programmable you could use it Blackwell, one of the major use cases right now is data processing and data curation. You find a circumstance that an AI model is not very good at. You present that circumstance to a vision language model, let's say, it is a car. You present that circumstance to a vision language model. The vision language model actually looks in the circumstances, said, this is what happened and I was very good at it. You then take that response to the prompt and you go and prompt an AI model to go find in your whole lake of data other circumstances like that, whatever that circumstance was.</td><td>그 이유는 CUDA가 매우 프로그래밍 가능하기 때문에 항상 활용할 수 있는 것들이 있기 때문입니다. Blackwell의 경우, 현재 주요 사용 사례 중 하나가 데이터 처리와 데이터 큐레이션입니다. AI 모델이 잘 처리하지 못하는 상황을 발견하게 됩니다. 예를 들어 자동차와 관련된 상황을 비전 언어 모델에 제시한다고 가정해보겠습니다. 그 상황을 비전 언어 모델에 제시하면, 비전 언어 모델이 실제로 그 상황들을 살펴보고 "이런 일이 일어났고 나는 이것을 매우 잘 처리했다"고 말합니다. 그런 다음 그 프롬프트에 대한 응답을 가져와서 AI 모델에 프롬프트를 주어 전체 데이터 레이크에서 그 상황과 유사한 다른 상황들을 찾아내도록 합니다.</td></tr>
<tr><td>And then you use an AI to do domain randomization and generate a whole bunch of other examples. And then from that, you can go train the bottle. And so you could use amperes to go and do data processing and data curation and machine learning-based search. And then you create the training data set, which you then present to your Hopper systems for training. And so each one of these architectures are completely -- they're all CUDA-compatible and so everything wants on everything. But if you have infrastructure in place, then you can put the less intensive workloads onto the installed base of the past. All of our [GBUs are] (ph) very well employed.</td><td>그리고 AI를 사용해서 도메인 랜덤화를 수행하고 다른 많은 예시들을 생성합니다. 그런 다음 이를 통해 모델을 훈련시킬 수 있습니다. 따라서 Ampere를 사용해서 데이터 처리와 데이터 큐레이션, 그리고 머신러닝 기반 검색을 수행할 수 있습니다. 그런 다음 훈련 데이터셋을 생성하여 Hopper 시스템에 제공해서 훈련을 진행합니다. 이러한 각각의 아키텍처들은 완전히 -- 모두 CUDA 호환이 가능하므로 모든 것이 모든 것에서 작동합니다. 하지만 인프라가 구축되어 있다면, 상대적으로 집약도가 낮은 워크로드들을 기존에 설치된 과거 제품들에 배치할 수 있습니다. 저희의 모든 GPU들이 매우 잘 활용되고 있습니다.</td></tr>
<tr><td>Operator<br><br>We have time for one more question, and that question comes from Atif Malik with Citi. Pleaese go ahead. Atif Malik<br><br>I have a follow-up question on gross margins for Colette. So I understand there are many moving parts the Blackwell yields, NVLink 72 and Ethernet mix. And you kind of tipped to the earlier question, the April quarter is the bottom,; but second half would have to ramp like 200 basis points per quarter to get to the mid-70s range that you're giving for the end of the fiscal year. And we still don't know much about tariff impact to broader semiconductor. So what kind of gives you the confidence in that trajectory in the back half of this year? Colette Kress<br><br>Yes.</td><td>운영자<br><br>한 가지 질문을 더 받을 시간이 있습니다. 시티의 아티프 말릭(Atif Malik)님의 질문입니다. 말씀해 주세요.<br><br>아티프 말릭<br><br>콜레트님께 매출총이익률에 대한 후속 질문이 있습니다. 블랙웰 수율, NVLink 72, 이더넷 믹스 등 많은 변수들이 있다는 것을 이해합니다. 그리고 앞선 질문에 대한 답변에서 4월 분기가 바닥이라고 언급하셨는데, 회계연도 말에 제시하신 70% 중반대 범위에 도달하려면 하반기에 분기당 200베이시스 포인트씩 상승해야 할 것 같습니다. 그리고 아직 반도체 전반에 대한 관세 영향에 대해서는 잘 모르는 상황입니다. 그렇다면 올해 하반기 이러한 궤도에 대해 어떤 부분이 확신을 주는지요?<br><br>콜레트 크레스<br><br>네.</td></tr>
<tr><td>Thanks for the question. Our Gross margins, there are quite complex in terms of the material and everything that we put together in a Blackwell system, a tremendous amount of opportunity to look at a lot of different pieces of that on how we can better improve our gross margins over time. Remember, we have many different configurations as well on Blackwell that will be able to help us do that. So together, working after we get some of these really strong ramping completed for our customers, we can begin a lot of that work. If not, we are going to probably start as soon as possible if we can. If we can improve it in the short term, we will also do that.</td><td>질문해 주셔서 감사합니다. 저희 매출총이익률은 Blackwell 시스템에 들어가는 소재와 모든 구성 요소들 측면에서 상당히 복잡합니다. 시간이 지나면서 매출총이익률을 더 잘 개선할 수 있는 방법에 대해 다양한 부분들을 살펴볼 수 있는 엄청난 기회가 있습니다. Blackwell에는 이를 도울 수 있는 다양한 구성도 많이 있다는 점을 기억해 주시기 바랍니다. 따라서 고객들을 위한 이러한 강력한 램핑을 완료한 후 함께 협력하여 그런 작업들을 많이 시작할 수 있을 것입니다. 그렇지 않더라도, 가능하다면 최대한 빨리 시작할 예정입니다. 단기적으로 개선할 수 있다면 그것도 할 것입니다.</td></tr>
<tr><td>Tariff at this point, it's a little bit of an unknown it's an unknown until we understand further what the U.S. government's plan is, both its timing, it's where and how much. So at this time, we are awaiting, but again, we would, of course, always follow export controls and/or tariffs in that manner. Operator<br><br>Ladies and gentlemen, that does conclude our question-and-answer session. I'm sorry. Jensen Huang<br><br>Thank you. Colette Kress<br><br>We are going to open up to Jensen and I believe he has a couple things. Jensen Huang<br><br>I just wanted to thank you. Thank you, Colette. The demand for Blackwell is extraordinary. AI is evolving beyond perception and generative AI into reasoning.</td><td>관세의 경우, 현재로서는 다소 불확실한 상황입니다. 미국 정부의 계획에 대해 더 자세히 파악하기 전까지는 불확실합니다. 시기, 적용 대상, 그리고 규모 등을 말이죠. 따라서 현재로서는 지켜보고 있는 상황이지만, 물론 저희는 항상 수출 통제 및/또는 관세를 그런 방식으로 준수할 것입니다.<br><br>오퍼레이터<br><br>신사 숙녀 여러분, 질의응답 세션을 마치겠습니다. 죄송합니다.<br><br>젠슨 황<br><br>감사합니다.<br><br>콜레트 크레스<br><br>젠슨에게 마이크를 넘기겠습니다. 몇 가지 말씀하실 것이 있다고 생각합니다.<br><br>젠슨 황<br><br>감사 인사를 드리고 싶습니다. 콜레트, 감사합니다. 블랙웰에 대한 수요는 정말 놀랍습니다. AI는 인식과 생성형 AI를 넘어 추론으로 진화하고 있습니다.</td></tr>
<tr><td>With resenting AI, we're observing another scaling law, inference time or test time scaling, more computation. The more the model thinks the smarter the answer. Models like OpenAI, Grok-3, DeepSeek-R1 are reasoning models that apply inference time scaling. Reasoning models can consume 100x more compute. Future reasoning models can consume much more compute. DeepSeek-R1 has ignited global enthusiast -- it's an excellent innovation. But even more importantly, it has open source a world-class reasoning AI model. Nearly every AI developer is applying R1 or chain of thought and reinforcement learning techniques like R1 to scale their model's performance.</td><td>최신 AI를 통해 우리는 또 다른 스케일링 법칙, 즉 추론 시간 또는 테스트 타임 스케일링을 관찰하고 있습니다. 더 많은 연산을 통해 모델이 더 많이 생각할수록 더 스마트한 답변을 얻을 수 있습니다. OpenAI, Grok-3, DeepSeek-R1과 같은 모델들은 추론 시간 스케일링을 적용하는 추론 모델들입니다. 추론 모델들은 100배 더 많은 연산을 소비할 수 있습니다. 미래의 추론 모델들은 훨씬 더 많은 연산을 소비할 수 있을 것입니다. DeepSeek-R1은 전 세계적인 열광을 불러일으켰습니다 -- 이는 탁월한 혁신입니다. 하지만 더욱 중요한 것은 세계적 수준의 추론 AI 모델을 오픈소스로 공개했다는 점입니다. 거의 모든 AI 개발자들이 R1이나 사고의 연쇄(chain of thought) 및 R1과 같은 강화학습 기법들을 적용하여 자신들의 모델 성능을 확장하고 있습니다.</td></tr>
<tr><td>We now have three scaling loss, as I mentioned earlier, driving the demand for AI computing. The traditional scaling loss of AI remains intact. Foundation models are being enhanced with multimodality, and pretraining is still growing. But it's no longer enough. We have two additional scaling dimensions. Post-training skilling, where reinforcement learning, fine-tuning, model distillation require orders of magnitude more compute than pretraining alone. Inference time scaling and reasoning where a single query and demand 100x more compute. We defined Blackwell for this moment, a single platform that can easily transition from pre-trading, post training and test time scaling.</td><td>앞서 말씀드린 바와 같이, 현재 AI 컴퓨팅 수요를 견인하는 세 가지 스케일링 법칙이 있습니다. AI의 전통적인 스케일링 법칙은 여전히 유효합니다. 파운데이션 모델들이 멀티모달리티로 향상되고 있으며, 사전 훈련(pretraining)도 계속 성장하고 있습니다. 하지만 이것만으로는 더 이상 충분하지 않습니다. <br><br>두 가지 추가적인 스케일링 차원이 있습니다. 첫째는 사후 훈련 스케일링으로, 강화학습, 파인튜닝, 모델 증류가 사전 훈련만으로는 필요한 컴퓨팅 파워보다 몇 배 더 많은 연산을 요구합니다. 둘째는 추론 시간 스케일링과 추론 과정으로, 단일 쿼리가 100배 더 많은 컴퓨팅을 요구할 수 있습니다. <br><br>우리는 바로 이 순간을 위해 블랙웰을 정의했습니다. 사전 거래, 사후 훈련, 그리고 테스트 시간 스케일링 간에 쉽게 전환할 수 있는 단일 플랫폼입니다.</td></tr>
<tr><td>Blackwell's FP4 transformer engine and NVLink 72 scale-up fabric and new software technologies led Blackwell process reasoning AI models, 25x faster than Hopper. Blackwell in all of this configuration is in full production. Each Grace Blackwell NVLink 72 rack is an engineering marvel. 1.5 million components produced across 350 manufacturing sites by nearly 100,000 factory operators. AI is advancing at life speed. We are at the beginning of reasoning AI and inference time scaling. But we are just at the start of the age of AI, multimodal AIs, enterprise AI sovereign AI and physical AI are right around the corner. We will grow strongly in 2025.</td><td>블랙웰의 FP4 트랜스포머 엔진과 NVLink 72 스케일업 패브릭, 그리고 새로운 소프트웨어 기술들은 블랙웰 프로세스 추론 AI 모델을 호퍼 대비 25배 더 빠르게 구동합니다. 이 모든 구성의 블랙웰은 현재 본격적인 양산 중입니다. 각각의 Grace Blackwell NVLink 72 랙은 엔지니어링의 경이로움입니다. 350개 제조 사이트에서 거의 10만 명의 공장 운영자들이 150만 개의 부품을 생산하고 있습니다. AI는 생활 속도로 발전하고 있습니다. 우리는 추론 AI와 추론 시간 스케일링의 시작점에 있습니다. 하지만 우리는 여전히 AI 시대의 출발점에 불과합니다. 멀티모달 AI, 엔터프라이즈 AI, 소버린 AI, 그리고 피지컬 AI가 바로 코앞에 다가와 있습니다. 우리는 2025년에 강력한 성장을 이룰 것입니다.</td></tr>
<tr><td>Going forward, data centers will dedicate most of CapEx to accelerated computing and AI. Data centers will increasingly become AI factories and every company will have either renting or self-operated. I want to thank all of you for joining us today. I’m joining us at GTC in a couple of weeks. We’re going to be talking about Blackwell Ultra, Rubin and other new computing, networking, reasoning AI, physical AI products. And a whole bunch more. Thank you. Operator<br><br>This concludes today's conference call. You may now disconnect.</td><td>앞으로 데이터센터들은 대부분의 자본지출을 가속 컴퓨팅과 AI에 집중할 것입니다. 데이터센터들은 점점 더 AI 팩토리가 될 것이며, 모든 기업이 임대하거나 자체 운영하게 될 것입니다. 오늘 참석해 주신 모든 분들께 감사드립니다. 몇 주 후 GTC에서 뵙겠습니다. 그곳에서 Blackwell Ultra, Rubin 그리고 기타 새로운 컴퓨팅, 네트워킹, 추론 AI, 물리적 AI 제품들에 대해 이야기할 예정입니다. 그리고 훨씬 더 많은 것들을 다룰 것입니다. 감사합니다. <br><br>교환원<br><br>오늘 컨퍼런스 콜을 마치겠습니다. 이제 연결을 끊으셔도 됩니다.</td></tr>
    </table>
    <h3>📌 요약</h3>
    <p style="background:#f0f0f0; padding:15px; border-left: 5px solid #333;">Here's a summary of the key points from the earnings call transcript in Korean:<br><br>• 실적 및 전망:<br>- 블랙웰(Blackwell) 제품의 수요가 매우 강력하며, 생산이 본격화됨<br>- 2025년에도 강한 성장세가 예상됨<br>- 총이익률은 현재 70% 초반이나 연말까지 중반대로 개선 전망<br><br>• AI 기술 트렌드:<br>- AI가 인식/생성 단계를 넘어 추론 단계로 진화 중<br>- 3가지 주요 확장 요인: 사전학습, 사후학습, 추론시간 확장<br>- 추론 AI 모델은 기존 대비 100배 이상의 컴퓨팅 파워 필요<br><br>• 사업 전략:<br>- 블랙웰 플랫폼은 모든 AI 워크로드에 최적화된 통합 아키텍처 제공<br>- 엔터프라이즈 AI, 국가별 AI, 물리 AI 등 새로운 시장 기회 확대<br>- 데이터센터가 AI 팩토리로 진화하는 추세에 맞춘 전략 추진<br><br>이 내용은 투자자들에게 NVIDIA의 현재 실적과 미래 성장 잠재력을 객관적으로 전달하고 있습니다.</p>
    <hr style="margin:50px 0;">
    
</body></html>